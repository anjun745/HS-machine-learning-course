{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"8a6c62","input":"#insert 2","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"ed29a7","input":"from itertools import product\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import naive_bayes\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import load_digits","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"04412f","input":"print(classification_report(y, model.predict(X)))","output":{"0":{"name":"stdout","output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.96      0.93      0.94       212\n           1       0.96      0.97      0.97       357\n\n   micro avg       0.96      0.96      0.96       569\n   macro avg       0.96      0.95      0.95       569\nweighted avg       0.96      0.96      0.96       569\n\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"20b1a9","input":"cancer = load_breast_cancer()\nprint(cancer.DESCR)","output":{"0":{"name":"stdout","output_type":"stream","text":".. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (\"coastline approximation\" - 1)\n\n        The mean, standard error, and \"worst\" or largest (mean of the three\n        largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n        13 is Radius SE, field 23 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171.\n"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"21875f","input":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)\n\nmodel = LogisticRegression(multi_class = \"auto\", solver = 'lbfgs', max_iter=10000)\nmodel.fit(X_train, y_train)\n\n# accuracy for test & train:    \ny_proba_LR = model.predict_proba(X_test)[:, 1]\n\n#calculate AUC\nprint('accuracy:', model.score(X_test,y_test))\nprint('ROC-AUC:', roc_auc_score(y_test, y_proba_GB))","output":{"0":{"name":"stdout","output_type":"stream","text":"accuracy: 0.9440559440559441\nROC-AUC: 0.9762396694214875\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"0c057b","input":"def plot_roc(ytrue, yproba, model, title='some ROC curve'):\n    auc = roc_auc_score(ytrue, yproba)\n    fpr, tpr, thr = roc_curve(ytrue, yproba)\n    plt.plot([0, 1], [0, 1], color='k', linestyle='--', linewidth=.4)\n    plt.plot(fpr, tpr, label='{} auc={:.2f}%'.format(model, auc*100))\n    plt.axis('equal')\n    plt.xlim([-.02, 1.02])\n    plt.ylim([-.02, 1.02])\n    plt.ylabel('tpr')\n    plt.xlabel('fpr')\n    plt.title(title)\n    plt.legend(loc='best')\n    plt.grid(True)\n    \nplot_roc(y_test, y_proba_LR, \"Log Reg\")","output":{"0":{"data":{"image/png":"0d7286ceb38df20d6086ebbc5eba26abdb64437b","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"2c3459","input":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n\n# accuracy for test & train:    \ny_proba_GB = model.predict_proba(X_test)[:, 1]\n\n#calculate AUC\nprint('accuracy:', model.score(X_test,y_test))\nprint('ROC-AUC:', roc_auc_score(y_test, y_proba_GB))","output":{"0":{"name":"stdout","output_type":"stream","text":"accuracy: 0.9300699300699301\nROC-AUC: 0.9791322314049588\n"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"c3a64e","input":"model = naive_bayes.GaussianNB()\nmodel.fit(X_train, y_train)\n   \ny_proba_NB = model.predict_proba(X_test)[:, 1]\n\n\nplot_roc(y_test, y_proba_NB, \"Naive Bayes\")\nplot_roc(y_test, y_proba_GB, \"Gradient Boosting\")\nplot_roc(y_test, y_proba_LR, \"Log Regression\")","output":{"0":{"data":{"image/png":"e81e31247bca25daf9156b2556626a043f1ccfb8","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"8c2016","input":"X = pd.DataFrame(cancer.data,\n                 columns = cancer.feature_names)\nX.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>","text/plain":"   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0        17.99         10.38          122.80     1001.0          0.11840   \n1        20.57         17.77          132.90     1326.0          0.08474   \n2        19.69         21.25          130.00     1203.0          0.10960   \n3        11.42         20.38           77.58      386.1          0.14250   \n4        20.29         14.34          135.10     1297.0          0.10030   \n\n   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0           0.27760          0.3001              0.14710         0.2419   \n1           0.07864          0.0869              0.07017         0.1812   \n2           0.15990          0.1974              0.12790         0.2069   \n3           0.28390          0.2414              0.10520         0.2597   \n4           0.13280          0.1980              0.10430         0.1809   \n\n   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n0                 0.07871  ...         25.38          17.33           184.60   \n1                 0.05667  ...         24.99          23.41           158.80   \n2                 0.05999  ...         23.57          25.53           152.50   \n3                 0.09744  ...         14.91          26.50            98.87   \n4                 0.05883  ...         22.54          16.67           152.20   \n\n   worst area  worst smoothness  worst compactness  worst concavity  \\\n0      2019.0            0.1622             0.6656           0.7119   \n1      1956.0            0.1238             0.1866           0.2416   \n2      1709.0            0.1444             0.4245           0.4504   \n3       567.7            0.2098             0.8663           0.6869   \n4      1575.0            0.1374             0.2050           0.4000   \n\n   worst concave points  worst symmetry  worst fractal dimension  \n0                0.2654          0.4601                  0.11890  \n1                0.1860          0.2750                  0.08902  \n2                0.2430          0.3613                  0.08758  \n3                0.2575          0.6638                  0.17300  \n4                0.1625          0.2364                  0.07678  \n\n[5 rows x 30 columns]"},"exec_count":4,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"a9e82e","input":"y = cancer.target\n\nmodel = LogisticRegression(multi_class = \"auto\", solver = 'lbfgs', max_iter=10000)\nmodel.fit(X, y)\nprint(model.score(X,y))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.9578207381370826\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"7caa1c","input":"model.fit(X, y)\nconfusion_matrix(y, model.predict(X))","output":{"0":{"data":{"text/plain":"array([[197,  15],\n       [  9, 348]])"},"exec_count":6,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","id":"147f7a","input":"#insert here","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"1e80a9","input":"We can compare the Naive Bayes, logistic regression, and gradient boosting ROC curves:","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"2f3cbc","input":"Going back to our breast cancer example, let's do a test/train split and calculate the AUC score:\n    ","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"371458","input":"Our accuracy was 96%. To see exactly which ones were mislabeled, we can view a confusion matrix. We can see that fourteen benign tumors were incorrectly mislabeled as malignant and nine malignant tumors were incorrectly mislabeled as a benign.","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"5d8d6b","input":"####  Cost Benefit Example: \n\nWe can also optimize our models based on specific costs associated with our classification errors; here we will use specific dollar amounts as weights.\n\nFor this example let's assume that a true breast cancer positive early detection would \nsave the company 100000 dollars in later treatment and legal fees, a false negative would cost the insurance company 50,000 in later treatment, a false positive would cost 500 dollars in more screening, and a true negative would cost the company 100 in screening fees.  \n\nThen given the confusion matrix $[[198,  14], [  9, 348]]$ above, the expected value of offering the breast cancer screening would be:\n\n$\\text{Expected_Value} = (100000)\\#TPs + (-50000)\\#FNs+(-500)\\#FPs+(-100)\\#TNs$","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"627957","input":"The best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing 100% sensitivity (no false negatives) and 100% specificity (no false positives). The (0,1) point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the left bottom to the top right corners (regardless of the positive and negative base rates). An intuitive example of random guessing is a decision by flipping coins. As the size of the sample increases, a random classifier's ROC point tends towards the diagonal line. In the case of a balanced coin, it will tend to the point (0.5, 0.5). One can achieve any level of performance on this line by flipping a weighted coin to decide between two categories.\n\nThe diagonal divides the ROC space. Points above the diagonal represent good classification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\n\nLet us look into four prediction results from 100 positive and 100 negative instances:\n\n<img src=\"images/tables.png\" width=\"600\">\n\nPlots of the four results above in the ROC space are given in the figure. The result of method A clearly shows the best predictive power among A, B, and C. The result of B lies on the random guess line (the diagonal line), and it can be seen in the table that the accuracy of B is 50%. However, when C is mirrored across the center point (0.5,0.5), the resulting method C′(top left, the ' is really hard to see) is even better than A. This mirrored method simply reverses the predictions of whatever method or test produced the C contingency table. Although the original C method has negative predictive power, simply reversing its decisions leads to a new predictive method C′ which has positive predictive power. When the C method predicts p or n, the C′ method would predict n or p, respectively. In this manner, the C′ test would perform the best. ","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"628779","input":"We can also view the ROC plot:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"6b43ff","input":"### Breast Cancer Example\nFor our next example, we'll load a breast cancer dataset that classifies tumors as benign (0) or malignant(1).","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"71fd6f","input":"Let's load the dataset:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"7576ac","input":"### Homework - #1 Digits\n\nRead in the digit example from yesterday. Do a 70/30 train/test split and then print a classification report for your logistic regression algorithm. Then, in words, write what the precision and recall scores that you get mean for the digits 1 and 8.","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"79a924","input":"Run the following cell to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"79abd4","input":"And perform the regression:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"84a65d","input":"Luckily, there is a scikit-learn function that gives us all of these measures:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"b6a74d","input":"### AUC-ROC Curves\n\nIf we wanted to we could interpret recall as the True Positive Rate (TPR). Recall that recall is given by:\n\n$\\text{Recall/TPR} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n\nGoing back to the breast cancer example, recall answers the question \"Out of all the (few) positive cases, how many did I actually find?\"\n\nAnother measure of error will be specificity. **Specificity** is defined as:\n\n$\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}$\n\nSuppose in a product survey most people liked ice cream. Specificity answers the question \"Out of all the (few) negative cases that didn't like ice cream, how many did I actually find?\"\n\nThe False Positive Rate (FPR) is given by:\n\n$\\text{FPR} = 1- \\text{Specificity} = \\frac{\\text{False Positives}}{\\text{True Negatives + False Positives}}$\n\nWe can plot a model in ROC (Receiver Operating Characteristic) space by plotting the False Positive Rate against the True Positive Rate:\n\n<img src=\"images/roc2.png\" width=\"500\">","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"bd20dd","input":"Let's interpret this. \n- With a precision score of 96%, out of all cases predicted as benign, we were correct 96% of the time. With a recall score of 93%, out of all benign tumors, we found 93% of them.\n- With a precision score of 96%, out of all cases predicted as malignant, we were correct 96% of the time. With a recall score of 97%, out of all malignant tumors, we found 97% of them.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"c68dce","input":"\n### Accuracy, Recall, Precision, F Score\nLet's get into a bit more specifics about what types of errors we can make.\n\n![title](images/errors.png)\n\n**Accuracy** is the one we are most used to. It is defined as:\n\n$\\text{Accuracy} = \\frac{\\text{True Positive + True Negative}}{\\text{True Positive + True Negative + False Positive + False Negative}}$\n\nOr more simply, as:\n\n$\\text{Accuracy} = \\frac{\\text{Correct samples}}{\\text{All samples}}$\n\nIn our iris example, 96% of our classifications were correct and 4% were incorrect.\n\nMany times, though, accuracy may not be the best measure. For example, suppose that 1% of a sample has breast cancer and 99% doesn't. Let's say your model simply spits out that no one has breast cancer. Then your model is 99% accurate. Sounds pretty good, right? We should probably aim to do better.\n\nLet's now define another measure of error, **recall** (also known as **sensitivity**).\n\n$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n\nin other words:\n\n$\\text{Recall} = \\frac{\\text{True Positive Predictions}}{\\text{All Positive samples}}$\n\nGoing back to the breast cancer example, recall answers the question \"Out of all the (few) positive cases, how many did I actually find?\" A model with high recall has a small number of false negatives.\n\nAnother measure of error is called **precision**.\n\n$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n\nin other words:\n\n$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{All samples predicted to be positive}}$\n\nGoing back to our breast cancer example, precision answers the question \"Out of all cases I predicted as positive, how many times was I right?\" A model with high precision has a small number of false positives.\n\nSuppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n\n\nA model with high recall and low precision will have most positive examples correctly recognized (low false negative rate) but there will be a lot of false positives (probably scaring a lot of women unncessarily if we're talking about the breast cancer example).\n\nA model with low recall and high precision will miss a lot of positive examples (high false negative rate) but those which we predict as positive are indeed positive (low false positive rate). This type of model would not scare women unnecessarily but would miss a lot of detections in women that did indeed have breast cancer.\n\nWe do have a third measure, called the **F1 score**, that balances precision and recall; it is the harmonic mean of the two:\n\n$F_1 = 2* \\frac{\\text{precision}*\\text{recall}}{\\text{precision}+\\text{recall}}$\n\n\nThe **support** is the number of samples of the true response that lie in that class.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"d03003","input":"## Gradient Boosting Classifier\n\nA classifying technique that sometimes provides greater accuracy than logistic regression is the gradient boosting classifer. Let's try it out on the breast cancer dataset:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"fbfe61","input":"### HW #2 - Hyperthyroid\n\nThere is an excellent hyperthyroid example listed [here](http://gim.unmc.edu/dxtests/ROC1.htm) and [here](http://gim.unmc.edu/dxtests/roc2.htm).  List the accuracy, recall/sensitivity/true positive rate, precision, support, sensitivity, and false negative rate for each of the three threshold values. Also make sure you understand how the ROC curve is obtained. There will be a quiz on exactly this question with different numbers next week.\n\n<img src=\"images/hyper2.png\" width=\"300\">\n<img src=\"images/hyper1.png\" width=\"300\">\n<img src=\"images/hyper3.png\" width=\"300\">\n<img src=\"images/hyper5.png\" width=\"300\">\n<img src=\"images/hyper4.png\" width=\"300\">","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"fda3e2","input":"The logistic regression model we have been using set a 50% threshold for determining classifications. In the graphic below, if the probability was above 50%, then the object was classified as an apple:\n\n<img src=\"images/apple.png\" width=\"500\">\n\nIf you wanted to reduce the false positive rate, you could move the threshold up (so that an object got classified as an apple only if the probability was above 70%, for example.) If you wanted to reduce the false negative rate, you could move the threshold down (so that an object got classified as an apple if the probability was above 30%, for example.) Each threshold is a different model. If you move that threshold from 0 to 1 then at each threshold value, you will have a certain TPR and a certain FPR. Plotting each coordinate gives you a model's ROC curve:\n\n<img src=\"images/roc.png\" width=\"300\">\n\nAUC is defined as the area under the ROC curve. A perfect model has AUC of the 1 which means it has good measure of separability. \n\n<img src=\"images/roc3.png\" width=\"600\">\n\nIn reality, though, most of our ROC curves will look more like this:\n\n<img src=\"images/roc4.png\" width=\"600\">\n\nWe can compare models and choose the one with the highest AUC.\n\n(source: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)","pos":17,"type":"cell"}
{"id":0,"time":1608092616007,"type":"user"}
{"last_load":1608092616219,"type":"file"}