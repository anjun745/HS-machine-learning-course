{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-1fc7451f-ddf7-40e1-b267-f865837f0c9d.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1619117437813,"exec_count":1,"id":"ceb161","input":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.utils import shuffle\n# from sklearn.datasets import fetch_mldata\nfrom sklearn.cluster import KMeans","kernel":"python3","pos":2,"start":1619117434161,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117442039,"exec_count":2,"id":"9d29dc","input":"iris = load_iris()\nX = iris.data\ny = iris.target\ny","kernel":"python3","output":{"0":{"data":{"text/plain":"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"},"exec_count":2}},"pos":5,"start":1619117442027,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117443647,"exec_count":3,"id":"150b5a","input":"sc_X = StandardScaler()\nX = sc_X.fit_transform(X)","kernel":"python3","pos":7,"start":1619117443642,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117446029,"exec_count":4,"id":"c46e2c","input":"#make the three random centroids to start be the same every time for plotting purposes\nnp.random.seed(5)\n\n#run the model\nmodel = KMeans(n_clusters=3)\nmodel.fit(X) \n\n#plot our clustering results\nfig = plt.figure(1, figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\nlabels = model.labels_\nax.scatter(X[:, 3], X[:, 0], X[:, 2],\n           c=labels.astype(np.float), edgecolor='k')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nax.set_title('Predicted Clusters (without knowing labels)')\nax.dist = 12\n\n# Plot the true iris classifications\nfig = plt.figure(2, figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nfor name, label in [('Setosa', 0),\n                    ('Versicolour', 1),\n                    ('Virginica', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean(),\n              X[y == label, 2].mean() + 2, name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny_true = np.choose(y, [1,2,0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y_true, edgecolor='k')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nax.set_title('True Classifications with Labels')\nax.dist = 12\n","kernel":"python3","output":{"0":{"name":"stderr","text":"<ipython-input-4-d4207b42d8b7>:10: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n  ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n<ipython-input-4-d4207b42d8b7>:26: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n  ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n"},"1":{"data":{"image/png":"162f172d6cb8fe69880a5b85bca5fb2af1e7fbf4","text/plain":"<Figure size 288x216 with 1 Axes>"},"metadata":{"image/png":{"height":245,"width":274},"needs_background":"light"}},"2":{"data":{"image/png":"c3457e91c34b3f82bc1dc29ed8c0bd3f610e7d20","text/plain":"<Figure size 288x216 with 1 Axes>"},"metadata":{"image/png":{"height":245,"width":250},"needs_background":"light"}}},"pos":9,"start":1619117445058,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117450175,"exec_count":5,"id":"5de873","input":"#make the three random centroids to start be the same every time for plotting purposes\nnp.random.seed(5)\n\n#run the model\nmodel = KMeans(n_clusters=8)\nmodel.fit(X) \n\n#plot our clustering results\nfig = plt.figure(1, figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\nlabels = model.labels_\nax.scatter(X[:, 3], X[:, 0], X[:, 2],\n           c=labels.astype(np.float), edgecolor='k')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nax.set_title('Predicted Clusters (without knowing labels)')\nax.dist = 12","kernel":"python3","output":{"0":{"name":"stderr","text":"<ipython-input-5-c05b4c2f4ba6>:10: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n  ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n"},"1":{"data":{"image/png":"55bd84449ba0ae141ccc8bbea80dfe0b4526c315","text/plain":"<Figure size 288x216 with 1 Axes>"},"metadata":{"image/png":{"height":245,"width":274},"needs_background":"light"}}},"pos":11,"start":1619117449697,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117457196,"exec_count":6,"id":"ebdc21","input":"# we're skipping the import of this because it's too big\n# here it is if you want to use full set on your own machine\n# X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n\n# we'll instead use a subset of the data that is pickledd\nimport pickle\nwith open('data/X.pickle', 'rb') as f:\n    X = pickle.load(f)\n    \nwith open('data/y.pickle','rb') as f:\n    y = pickle.load(f)","kernel":"python3","pos":14,"start":1619117457177,"state":"done","type":"cell"}
{"cell_type":"code","end":1619117463798,"exec_count":7,"id":"6b5411","input":"# X and y pickled version conain only 3000 images of the full 70,000 images in the original\nplt.rc(\"image\", cmap=\"binary\") # use black/white palette for plotting\nminimum = 2000\nfor i in range(minimum, minimum+10):\n    plt.subplot(2,5,i+1-minimum)\n    plt.imshow(X[i].reshape(28,28))\n    plt.xticks(())\n    plt.yticks(())\nplt.tight_layout()","kernel":"python3","output":{"0":{"data":{"image/png":"6451330f5354a008b46291dd0e0c78c351af5faf","text/plain":"<Figure size 432x288 with 10 Axes>"},"metadata":{"image/png":{"height":207,"width":424}}}},"pos":17,"start":1619117463258,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118551404,"exec_count":9,"id":"cfa5ee","input":"model = KMeans(n_clusters=10)\nmodel.fit(X)\n\nmu_digits = model.cluster_centers_\n\nplt.figure(figsize=(8, 3))\nfor i in range(2*(mu_digits.shape[0]//2)): # loop over all means\n    plt.subplot(2,mu_digits.shape[0]//2,i+1)\n    plt.imshow(mu_digits[i].reshape(28,28))\n    plt.xticks(())\n    plt.yticks(())\nplt.tight_layout()","kernel":"python3","output":{"0":{"data":{"image/png":"6161baedb928579add0bc232471979bbc5df0232","text/plain":"<Figure size 576x216 with 10 Axes>"},"metadata":{"image/png":{"height":208,"width":557}}}},"pos":21,"start":1619118544254,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118797318,"exec_count":14,"id":"96fb75","input":"model = KMeans(n_clusters=8)\nmodel.fit(X)\n\nmu_digits = model.cluster_centers_\n\nplt.figure(figsize=(8, 3))\nfor i in range(2*(mu_digits.shape[0]//2)): # loop over all means\n    plt.subplot(2,mu_digits.shape[0]//2,i+1)\n    plt.imshow(mu_digits[i].reshape(28,28))\n    plt.xticks(())\n    plt.yticks(())\nplt.tight_layout()","kernel":"python3","output":{"0":{"data":{"image/png":"1812524ec6e16001f5a85a5f074797e9cedd4669","text/plain":"<Figure size 576x216 with 8 Axes>"},"metadata":{"image/png":{"height":208,"width":515}}}},"pos":23,"start":1619118792079,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118804034,"exec_count":15,"id":"092809","input":"model = KMeans(n_clusters=12)\nmodel.fit(X)\n\nmu_digits = model.cluster_centers_\n\nplt.figure(figsize=(8, 3))\nfor i in range(2*(mu_digits.shape[0]//2)): # loop over all means\n    plt.subplot(2,mu_digits.shape[0]//2,i+1)\n    plt.imshow(mu_digits[i].reshape(28,28))\n    plt.xticks(())\n    plt.yticks(())\nplt.tight_layout()","kernel":"python3","output":{"0":{"data":{"image/png":"0f94db0e945fab9bf7251ddc134d85a977bf143f","text/plain":"<Figure size 576x216 with 12 Axes>"},"metadata":{"image/png":{"height":196,"width":568}}}},"pos":25,"start":1619118797326,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118857267,"exec_count":16,"id":"b33104","input":"df = pd.read_csv('data/fleet.csv', sep='\\t')\ndf","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Driver_ID</th>\n      <th>Distance_Feature</th>\n      <th>Speeding_Feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3423311935</td>\n      <td>71.24</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3423313212</td>\n      <td>52.53</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3423313724</td>\n      <td>64.54</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3423311373</td>\n      <td>55.69</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3423310999</td>\n      <td>54.58</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>3423310685</td>\n      <td>160.04</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>3423312600</td>\n      <td>176.17</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>3423312921</td>\n      <td>170.91</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>3423313630</td>\n      <td>176.14</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>3423311533</td>\n      <td>168.03</td>\n      <td>9.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 3 columns</p>\n</div>","text/plain":"       Driver_ID  Distance_Feature  Speeding_Feature\n0     3423311935             71.24              28.0\n1     3423313212             52.53              25.0\n2     3423313724             64.54              27.0\n3     3423311373             55.69              22.0\n4     3423310999             54.58              25.0\n...          ...               ...               ...\n3995  3423310685            160.04              10.0\n3996  3423312600            176.17               5.0\n3997  3423312921            170.91              12.0\n3998  3423313630            176.14               5.0\n3999  3423311533            168.03               9.0\n\n[4000 rows x 3 columns]"},"exec_count":16}},"pos":27,"start":1619118857211,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118861828,"exec_count":17,"id":"9c45ef","input":"plt.plot(df['Distance_Feature'], df['Speeding_Feature'], '.')\nplt.xlabel('Mean Distance Driven Per Day')\nplt.ylabel('Percentage of the time speeding')","kernel":"python3","output":{"0":{"data":{"text/plain":"Text(0, 0.5, 'Percentage of the time speeding')"},"exec_count":17},"1":{"data":{"image/png":"54c53d75247e527d27cdcf53ff26ee4df9270856","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"image/png":{"height":261,"width":390},"needs_background":"light"}}},"pos":29,"start":1619118861623,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118883149,"exec_count":18,"id":"c1a6e8","input":"X=df.drop('Driver_ID', axis=1)\n\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n\nmodel = KMeans(n_clusters=2)\nmodel.fit(X)","kernel":"python3","output":{"0":{"data":{"text/plain":"KMeans(n_clusters=2)"},"exec_count":18}},"pos":31,"start":1619118883082,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118899327,"exec_count":19,"id":"c2e3f1","input":"model.cluster_centers_","kernel":"python3","output":{"0":{"data":{"text/plain":"array([[-0.48581565, -0.13816168],\n       [ 1.94630276,  0.55351131]])"},"exec_count":19}},"pos":33,"start":1619118899320,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118921135,"exec_count":20,"id":"cc0d45","input":"X = sc_X.inverse_transform(X)\n\nmodel.cluster_centers_ = sc_X.inverse_transform(model.cluster_centers_)\nx_coord = [point[0] for point in X]\ny_coord = [point[1] for point in X]\n\nplt.scatter(x_coord, y_coord, c=model.labels_.astype(np.float), edgecolor='k')\nplt.xlabel('Distance Feature')\nplt.ylabel('Speeding Feature')\n\nfor cluster in model.cluster_centers_:\n    plt.plot(cluster[0], cluster[1], 'r*', markersize=12)\n   ","kernel":"python3","output":{"0":{"data":{"image/png":"7248385ba1489c1b408b10fcb46600dd5107163a","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"image/png":{"height":261,"width":390},"needs_background":"light"}}},"pos":35,"start":1619118920811,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118939794,"exec_count":21,"id":"c6c05e","input":"X=df.drop('Driver_ID', axis=1)\n\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n\nmodel = KMeans(n_clusters=4)\nmodel.fit(X)\n\nX = sc_X.inverse_transform(X)\n\nmodel.cluster_centers_ = sc_X.inverse_transform(model.cluster_centers_)\n\nx_coord = [point[0] for point in X]\ny_coord = [point[1] for point in X]\n\nplt.scatter(x_coord, y_coord, c=model.labels_.astype(np.float), edgecolor='k')\nplt.xlabel('Distance Feature')\nplt.ylabel('Speeding Feature')\n\nfor cluster in model.cluster_centers_:\n    plt.plot(cluster[0], cluster[1], 'r*', markersize=12)","kernel":"python3","output":{"0":{"data":{"image/png":"692e722960843cd06c5a5d26ec641892432ba200","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"image/png":{"height":261,"width":390},"needs_background":"light"}}},"pos":37,"start":1619118939402,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118950238,"exec_count":22,"id":"c7dead","input":"print(model.inertia_)","kernel":"python3","output":{"0":{"name":"stdout","text":"739.1534508645556\n"}},"pos":40,"start":1619118950227,"state":"done","type":"cell"}
{"cell_type":"code","end":1619118958751,"exec_count":23,"id":"79e201","input":"X=df.drop('Driver_ID', axis=1)\n\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)\n\n# fitting multiple k-means algorithms and storing the values in an empty list\nSSE = []\nfor cluster in range(1,20):\n    model = KMeans(n_clusters = cluster, init='k-means++')\n    model.fit(X)\n    SSE.append(model.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.xticks(np.arange(20));","kernel":"python3","output":{"0":{"data":{"image/png":"cbb071d5901abd0ebc54da646a2179374e88ba21","text/plain":"<Figure size 864x432 with 1 Axes>"},"metadata":{"image/png":{"height":370,"width":730},"needs_background":"light"}}},"pos":42,"start":1619118956104,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":67,"id":"c5ac48","input":"# don't have to do this if you are using the pickled X and y\n# X, y = shuffle(X, y,random_state = 77) \n# X = X[-3000:]\n# y = y[-3000:]\n# print(len(X))\n# print(len(y))","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"053c78","input":"### Unsupervised Learnin with K-Means Clustering","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"0793c9","input":"Let's view a scatterplot:","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"086791","input":"What if we choose 12 clusters instead of 10? We seem to cluster different variations of 0's and 1's separately:","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"192115","input":"Let's scale the data and apply the algorithm:","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"289dd8","input":"Run the cell below to import the required packages:","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"323857","input":"We can view these clusters on a graph, along with their centroids. We'll need to transform our data back first:","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"42cc2d","input":"The two cluster centroids are located at:","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"460da6","input":"The clustering doesn't quite resemble all of the digits, but remember that we only used 3000 training examples, and that the K-Means algorithm wasn't given any information about the numbers except the fact that they should be sorted into ten different categories! Pretty impressive.\n\nJust out of curiosity, what happens if we had used 8 clusters instead of 10? Sometimes the 4 and 5 get lost but this might vary over different simulations:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"59b404","input":"NOTE THAT WE WILL NOT GIVE THE K-MEANS ALGORITHM THE ACTUAL IRIS LABELS (y) BELOW; WE ONLY TELL IT TO CLUSTER ITEMS INTO THREE CATEGORIES.\n\nNotice how closely the three cluster resembles the true iris categories. Don't pay too much attention to the code below, just the pictures:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"6099b0","input":" We see that four distinct groups have been identified by the algorithm; now speeding drivers have been separated from those who follow speed limits, in addition to the rural vs. urban divide. The threshold for speeding is lower with the urban driver group than for the rural drivers, likely due to urban drivers spending more time in intersections and stop-and-go traffic.","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"65b5ff","input":"We will first fit multiple k-means models and in each successive model, we will increase the number of clusters. We will store the inertia value of each model and then plot it to visualize the result:","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"6fff59","input":"### Example 2: Digits","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"73c890","input":"### Example 1: Iris\n\nLet's return to our Iris example. \n\nLet's initialize our algorithm to give us three clusters, knowing that there are three types of irises.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"808452","input":"Let's take in an even larger sample of digits, the famous MNIST dataset. It's pretty big! Now, there are 70,000 images, each with a size of 28x28 pixels, so 784 attributes for each picture. It will take about a minute to load:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"99f484","input":"Sources:\n\nhttps://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n\nhttps://www.geeksforgeeks.org/k-means-clustering-introduction/\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html\n\nhttps://www.datascience.com/learn-data-science/tutorials/introduction-to-k-means-clustering-algorithm-data-science\n\nhttps://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\n\n### Unsupervised Learning\nUp until now, we have been exploring supervised learning algorithms, in which we already have labels on our testing data. Some common tasks within unsupervised learning are clustering and dimensionality reduction. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels. Unsupervised learning is very useful in exploratory analysis because it can automatically identify structure in data. For example, if an analyst were trying to segment consumers, unsupervised clustering methods would be a great starting point for their analysis. In situations where it is either impossible or impractical for a human to propose trends in the data, unsupervised learning can provide initial insights that can then be used to test individual hypotheses. \n\nAs an example of clustering, using three clusters, we may break the characters below into the following three categories, even though we never explicitly enter into the algorithm the labels of duck, mouse, rabbit:\n\n<img src=\"images/sup2.png\" width=500>\n\nThe technique we will use to cluster data is called K-Means clustering.\n\n### K-Means Clustering\n\nK-Means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. Choosing K is the trickier part.\n\n\n### Choosing k\nThe objective of K-means is simple: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.\n\nYou’ll define a target number k, which refers to the number of centroids you need in the dataset. A **centroid** is the location representing the center of the cluster.\n\nThe K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster.\n\nThe ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.\n\n### The Algorithm\n\nThe algorithm works as follows:\n\n1. First we initialize k points, called means, randomly.\n2. Next we categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far.\n3. We repeat the process for a given number of iterations or until the centroids have stabilized, meaning that there is no change in their values because the clustering has been successful. At the end, we have our clusters.\n\nStudy this visual carefully to understand the algorithm:\n\n<img src=\"images/kmeans1.png\" width=800>\n\n### Scaling is important\nAs always, remember that when you use distance-based algorithms you should first normalize the data.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"9e80be","input":"### How to choose the number of clusters\n\nOne of the most common doubts everyone has while working with K-Means is selecting the right number of clusters.\n\nSo, let’s look at a technique that will help us choose the right value of clusters for the K-Means algorithm. \n\n**Inertia** tells us how far the points within a cluster are. So, inertia is calculated as the sum of squared distance for each point to it's closest centroid.\n\nHow can we decide the optimum number of clusters? One thing we can do is plot a graph, also known as an elbow curve, where the x-axis will represent the number of clusters and the y-axis will be an evaluation metric. Let’s say inertia for now.\n\nWhen we change the cluster value from 2 to 4, the inertia value reduced very sharply. This decrease in the inertia value reduces and eventually becomes constant as we increase the number of clusters further.\n\n<img src=\"images/kmeans2.png\" width=500>\n\nHere, we can choose any number of clusters between 6 and 10. We can have 7, 8, or even 9 clusters. You must also look at the computation cost while deciding the number of clusters. If we increase the number of clusters, the computation cost will also increase. So, if you do not have high computational resources, you may need to choose a lesser number of clusters.\n\nGoing back to the truck driving example with four clusters, we find that the inertia measure is built in to the knn algorithm:","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"a2cc50","input":"We can clearly see that the elbow is located at k=4. Thus, our intuition to use four clusters seems appropriate.","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"bd30eb","input":"### Example 3 - Fleets\n\nIn the above examples, it was kind of silly to use unsupervised learning, since we knew there were exactly three iris types and exactly 10 digits. Unsupervised learning tends to be more useful when we don't know the structure ahead of time.\n\nOne last example is a business example using delivery fleet driver data. For the sake of simplicity, we'll only be looking at two driver features: mean distance driven per day and the mean percentage of time a driver was >5 mph over the speed limit. In general, this algorithm can be used for any number of features, so long as the number of data samples is much greater than the number of features.","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"c06b53","input":"Let's use 10 clusters (since we know there are 10 digits). We'll store the means of the clusters (the centroids) in a variable called `mu_digits` via a call to `cluster_centers_`. What does the \"average\" digit in each cluster look like?","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"c15adc","input":"Using domain knowledge of the dataset, we can infer that Group 1 is urban drivers and Group 2 is rural drivers in the clusters above.\n\nNow, let's try using four clusters.","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"d68a02","input":"K-Means is such a computationally intensive algorithm that let's shuffle the data and only take the last 3000 (instead of the full 70,000 digits) in order to reduce our runtime:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"dd7cf0","input":"I won't scale the data for this example so that the images appear darker with more contrast, but remember that typically we would scale.","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"edb6e7","input":"Let's scale the data before we apply our algorithm. Also notice that I am scaling my entire dataset. We don't have a test/train split because we don't have a testing set - we are exploring whether the data might be separated into clusters but we do not have labels ahead of time. We will never be using the target data, y.","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"f25898","input":"Of course, for this example we knew that we wanted three clusters. What if we were trying to find structure in a dataset that we knew knowing about? For example, suppose that we didn't know that the iris dataset had three classifications and so we happened to try eight clusters:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"f8cf8f","input":"Here are 10 examples in the dataset:","pos":16,"type":"cell"}
{"end":1619118790308,"exec_count":13,"id":"1837b4","input":"X","kernel":"python3","output":{"0":{"data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"exec_count":13}},"pos":21.5,"start":1619118790298,"state":"done","type":"cell"}
{"id":0,"time":1619117423762,"type":"user"}
{"last_load":1619117380075,"type":"file"}