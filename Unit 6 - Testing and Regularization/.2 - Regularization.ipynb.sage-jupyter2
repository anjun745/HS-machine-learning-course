{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-d3ff2a26-d3ba-4233-9627-78b67d0e0b1e.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"c86d59","input":"","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"a99757","input":"print(model.steps[1][1].intercept_)\nprint(model.steps[1][1].coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.7416613325491779\n[ 0.         -0.805065   -0.85323434 -0.57000594 -0.27233909 -0.03230409\n  0.14592968  0.27349385  0.36267917  0.42363743]\n"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"575f67","input":"alpha = 0.0\nmodel = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\nmodel.fit(X_train, y_train)\nprint(model.steps[1][1].intercept_)\nprint(model.steps[1][1].coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"-0.5565420393217826\n[ 0.00000000e+00  1.94573477e+01 -1.67820001e+02  1.71398814e+03\n -1.14015674e+04  4.17181937e+04 -8.63803847e+04  1.01154264e+05\n -6.24093670e+04  1.57568607e+04]\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"1841ef","input":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"67e967","input":"# We start by seeding the random number generator so that everyone will have the same \"random\" results\nnp.random.seed(9)\n\n# Function that returns the sin(2*pi*x)\ndef f(x):\n    return np.sin(2 * np.pi * x)\n\n# This returns 100 evenly spaced numbers from 0 to 1\n\nX_plot = np.linspace(0, 1, 100)\n\n#applying model.predict(X) needs the X to be a matrix so the line below adds another dimension\nX = X_plot[:, np.newaxis]\n\n# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\ny = f(X_plot) + np.random.normal(0,.3,100)\n\n# Plot the training data against what we know to be the ground truth sin function\nfig,ax = plt.subplots(1,1)\nax.plot(X_plot, f(X_plot), label='ground truth', color='green')\nax.scatter(X_plot, y, label='data')\nax.set_ylabel('y')\nax.set_xlabel('x')\nax.legend()\n","output":{"0":{"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f3204fdc250>"},"exec_count":4,"output_type":"execute_result"},"1":{"data":{"image/png":"c32a1e15cdfaf8fb5d5cceee1ac7368ba82f8219","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":261,"width":394},"needs_background":"light"},"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"14b4df","input":"# Bringing in our helper functions\n\ndef f(x):\n    return np.sin(2 * np.pi * x)\n\nx_plot = np.linspace(0, 1, 100)\n\n#plots the left hand column plots below\n\ndef plot_approximation(model, ax, label=None):\n    \"\"\"Plot the approximation of ``model`` on axis ``ax``. \"\"\"\n    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n    ax.scatter(X, y, s=100)\n    ax.plot(x_plot, model.predict(x_plot[:, np.newaxis]), color='red', label=label)\n    ax.set_ylim((-2, 2))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='upper right',frameon=True)\n\n# A helper function to plot the absolute value of the coefficients on the right-hand column plot\ndef plot_coefficients(model, ax, label=None):\n    coef = model.steps[-1][1].coef_.ravel()\n    ax.semilogy(np.abs(coef), marker='o', label=label)\n    ax.set_ylim((1e-1, 1e8))\n    ax.set_xlim((1, 9))\n    ax.set_ylabel('abs(coefficient)')\n    ax.set_xlabel('coefficients')","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"80eae2","input":"#helper function to plot things\ndef plot_approximation(model, ax, x_plot, y, label=None):\n    \"\"\"Plot the approximation of ``model`` on axis ``ax``. \"\"\"\n    ax.plot(x_plot, f(x_plot), 'g', label=\"ground truth\")\n    ax.scatter(x_plot, y, s=10)\n    x = x_plot[:, np.newaxis]\n    ax.plot(x_plot, model.predict(x), color='red', label=label)\n    ax.set_ylim((-2, 2))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='upper right')\n\n#split the data into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.7)\n\n#set up four subplots\nfig, axes = plt.subplots(2, 2, figsize=(8, 5))\n\n#fit different polynomials and plot approximations\nfor ax, degree in zip(axes.ravel(), [0, 1, 3, 9]):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    plot_approximation(model, ax, X_plot, y, label='degree=%d' % degree)","output":{"0":{"data":{"image/png":"bdf86df5a3c4295676a8651907c9a192273a3cf6","text/plain":"<Figure size 576x360 with 4 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":319,"width":504},"needs_background":"light"},"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"93d022","input":"degrees = 10\n\ntrain_error = []\ntest_error = []\n\nfor degree in range(degrees):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    train_error.append(mean_squared_error(y_train, model.predict(X_train)))\n    test_error.append(mean_squared_error(y_test, model.predict(X_test)))\n\n# Plot the training and test errors against degree\nplt.figure(figsize=(8,6))\nplt.plot(np.arange(degrees), train_error, color='green', label='train')\nplt.plot(np.arange(degrees), test_error, color='red', label='test')\nplt.ylim((0.0, 1e0))\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')","output":{"0":{"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f3204ec3eb0>"},"exec_count":6,"output_type":"execute_result"},"1":{"data":{"image/png":"dc92782b943ce9b5104eeace53122c80b1452333","text/plain":"<Figure size 576x432 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":374,"width":497},"needs_background":"light"},"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"f1652c","input":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set up a figure and axes for 8 plots, 2 per row for 4 rows\nfig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n\ndegree = 9\nalphas = [0.0, 1e-5, 1e-1, 1]\n\nfor alpha, ax_row in zip(alphas, ax_rows):\n    ax_left, ax_right = ax_row\n    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n    model.fit(X_train, y_train)\n    \n    print(\"alpha: \", alpha, \"    R^2:\", model.score(X_test, y_test))\n    plot_approximation(model, ax_left, label='alpha=%r' % alpha)\n    plot_coefficients(model, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n\nplt.tight_layout()","output":{"0":{"name":"stdout","output_type":"stream","text":"alpha:  0.0     R^2: 0.5048513990724568\nalpha:  1e-05     R^2: 0.7989419726360487\nalpha:  0.1     R^2: 0.6372330256140104\nalpha:  1     R^2: 0.5081199085581769\n"},"1":{"data":{"image/png":"519c362b9b3ec30bf748ae2d19737cd7eaf58930","text/plain":"<Figure size 1080x1440 with 8 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":1431,"width":1072},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"5bb8c5","input":"y_train","output":{"0":{"data":{"text/plain":"array([ 0.4968002 , -1.16522564,  0.34625658,  0.79548561, -1.11480845,\n        0.22114639, -0.67653319,  0.27915903, -1.07389337, -0.39497956,\n        0.37012409, -1.2848539 , -0.8708035 ,  0.9202245 , -0.82359477,\n        0.89330229,  1.05948788, -0.20822744,  0.981399  ,  0.94684281,\n        0.78367525, -0.58383548,  0.87655645,  0.59204162, -0.41355938,\n        0.82933502, -0.35932482,  0.11291054, -0.73005796, -1.50649431])"},"exec_count":9,"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"markdown","id":"0583e9","input":"Trying out 4 different values of the RidgeRegression parameter alpha, we see how the resulting models change.\nWith higher values of alpha, more complex (more wiggly) models will be more punished and thus less likely to score highly. We also notice in this case that alpha = 1e-05 has the highest $R^2$ score:","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"0fac42","input":"And we concluded that a polynomial between degree 3 and 6 would minimize the error:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"16141d","input":"What does mean that our model was? We will ignore the first coefficient (0) because it is actually contained in the separate intercept_ parameter. Thus:\n\n$y_{\\text{predicted}} = 0.74166 + -0.805065 x + -0.85323434 x^2 + ... + 0.42363743 x^9$","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"196d69","input":"$y_{\\text{predicted}} = -0.55654 + 19.457 x - 167.8 x^2 + ... + 15756.8 x^9$","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"7df4f2","input":"We fit various polynomials to the data:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"994639","input":"### Ridge Regression\n\n#### Hand picking polynomials is hard work, and data scientists are lazy so....\n...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the **regularization** term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n\nDon't let \"L2 norm\" scare you...It is just the distance formula that you know and love: $|(x_1,x_2,...x_n)|_2 = \\sqrt{x_1^2+x_2^2+...+x_n^2}$.\n\nA popular regularized linear regression model is **Ridge Regression**. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n\n  $J(\\boldsymbol\\theta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\theta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\theta\\|_2$\n\nwhere $\\boldsymbol\\theta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature vector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n\nMore specifically, if we are trying to come up with optimal parameters for the regression equation $f(x_1,x_2,...)=\\theta_0+\\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1 x_2 +\\theta_4 x_2 ^2 ...$, then Ridge Regression minimizes our cost function   $J(\\boldsymbol\\theta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\theta^T \\mathbf{x}_i')^2 + \\alpha \\sqrt{\\theta_0^2+\\theta_1^2+\\theta_2^2...+\\theta_n^2}=\\frac{1}{n}\\sum_{i=0}^n (y_i - (\\theta_0+\\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1 x_2 +\\theta_4 x_2 ^2 ...))^2 + \\alpha \\sqrt{\\theta_0^2+\\theta_1^2+\\theta_2^2...+\\theta_n^2}$\n\nBelow you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. First, read in some helper functions:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"accdab","input":"Notice in the last model corresponding to alpha=1, we can acquire our intercept and coefficients from the pipeline:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"bd974f","input":"Import the following packages:","pos":0,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"d758af","input":"How does this model compare to the first picture in which alpha was 0? The coefficients were wayyyy bigger:","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"eab0e7","input":"### Homework: \nRead about the differences between Ridge, Lasso, and Elastic Net regularization and post a summary to the discussion board.\n\nChallenge: Can you try to edit your previous Stochastic Gradient Descent method using the Ridge regression cost function? ","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"edf8ac","input":"### Regularization\n\nRecall our $f(x)=\\sin(2 \\pi x)$ function from yesterday:","pos":2,"type":"cell"}
{"id":0,"time":1615912500674,"type":"user"}
{"last_load":1615933440290,"type":"file"}