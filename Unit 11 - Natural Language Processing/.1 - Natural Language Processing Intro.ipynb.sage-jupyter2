{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-b37d0428-2804-4775-ad9c-6d00b2c6e2a3.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"f3710b","input":"","pos":44,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"4de955","input":"import pandas as pd\nimport numpy as np\n\nimport re\nimport nltk\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\n\nfrom nltk.corpus import stopwords","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"5af36b","input":"cv = CountVectorizer()\n\nX = cv.fit_transform(corpus)\n\npd.DataFrame(X.toarray(), columns=cv.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car</th>\n      <th>driven</th>\n      <th>highway</th>\n      <th>is</th>\n      <th>on</th>\n      <th>road</th>\n      <th>the</th>\n      <th>truck</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   car  driven  highway  is  on  road  the  truck\n0    1       1        0   1   1     1    2      0\n1    0       1        1   1   1     0    2      1"},"exec_count":10,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"e081df","input":"data = [(0,0),(0.043,0),(0,0.043),(0,0),(0,0),(0,0),(0,0),(0.043,0),(0,0.043)]\n(pd.DataFrame(data, columns=['A','B'], index=['the', 'car', 'truck', 'is', 'driven', 'on', 'the', 'road', 'highway'])).T","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>the</th>\n      <th>car</th>\n      <th>truck</th>\n      <th>is</th>\n      <th>driven</th>\n      <th>on</th>\n      <th>the</th>\n      <th>road</th>\n      <th>highway</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A</th>\n      <td>0.0</td>\n      <td>0.043</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.043</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>B</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.043</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.043</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   the    car  truck   is  driven   on  the   road  highway\nA  0.0  0.043  0.000  0.0     0.0  0.0  0.0  0.043    0.000\nB  0.0  0.000  0.043  0.0     0.0  0.0  0.0  0.000    0.043"},"exec_count":11,"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"aa5c31","input":"tf = TfidfVectorizer(lowercase=True, \n                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n                     stop_words=stopwords.words('english'),\n                     min_df=1)\n\nX = tf.fit_transform(corpus)\n\npd.DataFrame(X.toarray(), columns=tf.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car</th>\n      <th>driven</th>\n      <th>highway</th>\n      <th>road</th>\n      <th>truck</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.631667</td>\n      <td>0.449436</td>\n      <td>0.000000</td>\n      <td>0.631667</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.449436</td>\n      <td>0.631667</td>\n      <td>0.000000</td>\n      <td>0.631667</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"        car    driven   highway      road     truck\n0  0.631667  0.449436  0.000000  0.631667  0.000000\n1  0.000000  0.449436  0.631667  0.000000  0.631667"},"exec_count":12,"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"5cc25b","input":"example = ['Football baseball basketball',\n            'baseball giants cubs redsox',\n            'football broncos cowboys',\n            'baseball redsox tigers',\n            'pop stars hendrix prince',\n            'hendrix prince jagger rock',\n            'joplin pearl jam tupac rock',\n          ]\n\nvectorizer = TfidfVectorizer(lowercase=True, \n                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n                     stop_words=stopwords.words('english'),\n                     min_df=1)\n\nX = vectorizer.fit_transform(example)\n\npd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseball</th>\n      <th>basketball</th>\n      <th>broncos</th>\n      <th>cowboys</th>\n      <th>cubs</th>\n      <th>football</th>\n      <th>giants</th>\n      <th>hendrix</th>\n      <th>jagger</th>\n      <th>jam</th>\n      <th>joplin</th>\n      <th>pearl</th>\n      <th>pop</th>\n      <th>prince</th>\n      <th>redsox</th>\n      <th>rock</th>\n      <th>stars</th>\n      <th>tigers</th>\n      <th>tupac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.479185</td>\n      <td>0.675356</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.560603</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.397106</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.559675</td>\n      <td>0.000000</td>\n      <td>0.559675</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.464579</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.609819</td>\n      <td>0.609819</td>\n      <td>0.000000</td>\n      <td>0.506202</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.479185</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.560603</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.675356</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.451635</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.544082</td>\n      <td>0.451635</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.544082</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.570997</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.000000</td>\n      <td>0.473977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.461804</td>\n      <td>0.461804</td>\n      <td>0.461804</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.383337</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.461804</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   baseball  basketball   broncos   cowboys      cubs  football    giants  \\\n0  0.479185    0.675356  0.000000  0.000000  0.000000  0.560603  0.000000   \n1  0.397106    0.000000  0.000000  0.000000  0.559675  0.000000  0.559675   \n2  0.000000    0.000000  0.609819  0.609819  0.000000  0.506202  0.000000   \n3  0.479185    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n4  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n5  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n6  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n    hendrix    jagger       jam    joplin     pearl       pop    prince  \\\n0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n4  0.451635  0.000000  0.000000  0.000000  0.000000  0.544082  0.451635   \n5  0.473977  0.570997  0.000000  0.000000  0.000000  0.000000  0.473977   \n6  0.000000  0.000000  0.461804  0.461804  0.461804  0.000000  0.000000   \n\n     redsox      rock     stars    tigers     tupac  \n0  0.000000  0.000000  0.000000  0.000000  0.000000  \n1  0.464579  0.000000  0.000000  0.000000  0.000000  \n2  0.000000  0.000000  0.000000  0.000000  0.000000  \n3  0.560603  0.000000  0.000000  0.675356  0.000000  \n4  0.000000  0.000000  0.544082  0.000000  0.000000  \n5  0.000000  0.473977  0.000000  0.000000  0.000000  \n6  0.000000  0.383337  0.000000  0.000000  0.461804  "},"exec_count":13,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"fd6688","input":"vectorizer.get_feature_names()","output":{"0":{"data":{"text/plain":"['baseball',\n 'basketball',\n 'broncos',\n 'cowboys',\n 'cubs',\n 'football',\n 'giants',\n 'hendrix',\n 'jagger',\n 'jam',\n 'joplin',\n 'pearl',\n 'pop',\n 'prince',\n 'redsox',\n 'rock',\n 'stars',\n 'tigers',\n 'tupac']"},"exec_count":14,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"b6775f","input":"svd = TruncatedSVD(2)\nX_svd = svd.fit_transform(X)\n\npd.DataFrame(svd.components_.round(5),\n             index = [\"component_1\",\"component_2\"],\n             columns = vectorizer.get_feature_names())","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseball</th>\n      <th>basketball</th>\n      <th>broncos</th>\n      <th>cowboys</th>\n      <th>cubs</th>\n      <th>football</th>\n      <th>giants</th>\n      <th>hendrix</th>\n      <th>jagger</th>\n      <th>jam</th>\n      <th>joplin</th>\n      <th>pearl</th>\n      <th>pop</th>\n      <th>prince</th>\n      <th>redsox</th>\n      <th>rock</th>\n      <th>stars</th>\n      <th>tigers</th>\n      <th>tupac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>component_1</th>\n      <td>0.59434</td>\n      <td>0.26389</td>\n      <td>0.10775</td>\n      <td>0.10775</td>\n      <td>0.25565</td>\n      <td>0.30849</td>\n      <td>0.25565</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.47627</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.31811</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>component_2</th>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>-0.00000</td>\n      <td>-0.00000</td>\n      <td>0.00000</td>\n      <td>-0.00000</td>\n      <td>0.00000</td>\n      <td>0.51977</td>\n      <td>0.33357</td>\n      <td>0.10539</td>\n      <td>0.10539</td>\n      <td>0.10539</td>\n      <td>0.29259</td>\n      <td>0.51977</td>\n      <td>0.00000</td>\n      <td>0.36438</td>\n      <td>0.29259</td>\n      <td>0.00000</td>\n      <td>0.10539</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"             baseball  basketball  broncos  cowboys     cubs  football  \\\ncomponent_1   0.59434     0.26389  0.10775  0.10775  0.25565   0.30849   \ncomponent_2   0.00000     0.00000 -0.00000 -0.00000  0.00000  -0.00000   \n\n              giants  hendrix   jagger      jam   joplin    pearl      pop  \\\ncomponent_1  0.25565 -0.00000 -0.00000  0.00000  0.00000  0.00000 -0.00000   \ncomponent_2  0.00000  0.51977  0.33357  0.10539  0.10539  0.10539  0.29259   \n\n              prince   redsox     rock    stars   tigers    tupac  \ncomponent_1 -0.00000  0.47627 -0.00000 -0.00000  0.31811  0.00000  \ncomponent_2  0.51977  0.00000  0.36438  0.29259  0.00000  0.10539  "},"exec_count":15,"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"7ab15c","input":"svd.explained_variance_ratio_","output":{"0":{"data":{"text/plain":"array([0.14219813, 0.16486574])"},"exec_count":16,"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"42670a","input":"dtm_svd = Normalizer(copy=False).fit_transform(X_svd)","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"141e5a","input":"pd.DataFrame(dtm_svd.round(5),\n             index=example, \n             columns=[\"component_1\",\"component_2\"])","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>component_1</th>\n      <th>component_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Football baseball basketball</th>\n      <td>1.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>baseball giants cubs redsox</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>football broncos cowboys</th>\n      <td>1.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>baseball redsox tigers</th>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>pop stars hendrix prince</th>\n      <td>-0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>hendrix prince jagger rock</th>\n      <td>-0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>joplin pearl jam tupac rock</th>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                              component_1  component_2\nFootball baseball basketball          1.0         -0.0\nbaseball giants cubs redsox           1.0          0.0\nfootball broncos cowboys              1.0         -0.0\nbaseball redsox tigers                1.0          0.0\npop stars hendrix prince             -0.0          1.0\nhendrix prince jagger rock           -0.0          1.0\njoplin pearl jam tupac rock           0.0          1.0"},"exec_count":18,"output_type":"execute_result"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"8fa697","input":"new_article = [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # corresponds to \"baseball basketball broncos\" in the matrix above\nprint(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\nprint(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))","output":{"0":{"name":"stdout","output_type":"stream","text":"Dot product with component 1:  0.9659800000000001\nDot product with component 2:  0.0\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"2c6a43","input":"corpus = 'I like football. Football is one of my favorite sports. I play Fantasy Football with my friends. I am in several different fantasy football leagues.'","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"c03e5a","input":"new_article = [0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0] # corresponds to \"jagger jam joplin\" in the matrix above\nprint(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\nprint(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))","output":{"0":{"name":"stdout","output_type":"stream","text":"Dot product with component 1:  0.0\nDot product with component 2:  0.54435\n"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"c282bc","input":"new_article = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0] # corresponds to \"redsox hendrix\" in the matrix above\nprint(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\nprint(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))","output":{"0":{"name":"stdout","output_type":"stream","text":"Dot product with component 1:  0.47627\nDot product with component 2:  0.51977\n"}},"pos":43,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"041be3","input":"corpus = corpus.lower()\nprint(corpus)","output":{"0":{"name":"stdout","output_type":"stream","text":"i like football. football is one of my favorite sports. i play fantasy football with my friends. i am in several different fantasy football leagues.\n"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"9d818d","input":"string.punctuation","output":{"0":{"data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"exec_count":4,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"609be5","input":"punc_re = re.compile('[%s]' % re.escape(string.punctuation))\ncorpus = map(lambda x: punc_re.sub(' ', x), corpus)\ncorpus = ''.join(list(corpus))\nprint(corpus)","output":{"0":{"name":"stdout","output_type":"stream","text":"i like football  football is one of my favorite sports  i play fantasy football with my friends  i am in several different fantasy football leagues \n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"ed1078","input":"from nltk.corpus import stopwords\n\nstopwords.words('english')[:10]","output":{"0":{"data":{"text/plain":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"},"exec_count":6,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"e2f699","input":"resultwords  = [word for word in corpus.split() if word not in stopwords.words('english')]\ncorpus = ' '.join(resultwords)\nprint(corpus)","output":{"0":{"name":"stdout","output_type":"stream","text":"like football football one favorite sports play fantasy football friends several different fantasy football leagues\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"7d22b6","input":"corpus.split()","output":{"0":{"data":{"text/plain":"['like',\n 'football',\n 'football',\n 'one',\n 'favorite',\n 'sports',\n 'play',\n 'fantasy',\n 'football',\n 'friends',\n 'several',\n 'different',\n 'fantasy',\n 'football',\n 'leagues']"},"exec_count":8,"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"2c6892","input":"corpus = ['The car is driven on the road.', \n          'The truck is driven on the highway.']","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"07e74b","input":"Before we apply our machine learning algorithms, we often preprocess the corpus in order to transform the raw data in a useful and efficient format. Here are some common types of preprocessing:\n\n- **Normalization**: Making all the text lower case is one of the simplest and most effective forms of text preprocessing.\n\nWe'll do that now:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"0d7e7c","input":"Not surprisingly, the document was much more similar to component 1. What about a new music-related document like \"jagger jam joplin\"?","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"13600e","input":"### Text Data Vectorization\n\nOnce the data is preprocessed, we can numerically represent text data. Here are some ways to do so.\n\n\n- **Bag of Words** we can think of as creating a table where columns are the set of unique words in the corpus and rows correspond to each sentence(document). We set the value as 1 if the word is present in the sentence else we set it to 0. Consider the list below as two documents:","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"1dfad9","input":"We'll also want to scale our results using Normalizer. This ensures that each vector has a norm of 1. Vectors with a norm of 1 are easy to work with for calculating similarity.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"26789d","input":"- **Punctuation removal**: Punctuation can also be removed. The string library contains a list of (most) punctuation characters:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"26b280","input":"Some other types of preprocessing that we won't do now but are helpful to know about are (and you may use if you NLP for your final project):\n    \n- **Stemming** is the process of reducing a word to its stem/root word. It reduces inflection in words (e.g. ‘help’, ’helping’, ’helped’, ’helpful’) to their root form (e.g. ‘help’). It removes the morphological affixes from words, leaving only the word stem. The stem word may or may not be a valid word in the language. For example ‘movi’ is the root word for ‘movie’, ‘emot’ is the root word for ‘emotion’.\n\n- **Lemmatization** does the same thing as stemming, converting a word to its root form but with one difference i.e., the root word in this case belongs to a valid word in the language. For example the word caring would map to ‘care’ and not ‘car’ as the in case of stemming.\n\n- **Ngrams** are the combination of multiple words used together. N-grams can be used when we want to preserve sequence information in the document, like what word is likely to follow the given one. For example, if we were making a Donald Trump chatbot, we might want the word \"news\" to always follow the word \"fake\".","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"2d7abc","input":"We can view the words that are contained in the columns here:","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"36e891","input":"- **Stop words** are common words that do not contribute much of the information in a text document. Words like ‘the’, ‘is’, ‘a’ have less value and add noise to the text data. Here are the first ten contained in the nltk stopwords list:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"3bb4b0","input":"We will let Python create the TF-IDF matrix for us instead.\n\nNicely enough, the sklearn TF-IDF vectorizer can make the corpus lowercase and remove punctuation and stop words. We remove punctuation using the argument stop_words by setting it equal to a regular expression that indicates we only want alphabetic characters and not punctuation. We can also add a min_df argument, which ignores terms in the document that have a document frequency strictly lower than the given threshold.\n\nDon't be alarmed that sklearn's matrix looks quite a bit different than yours. The values differ slightly because sklearn uses a smoothed version idf and various other little optimizations. For example, sklearn uses $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})+1$ instead of just $ \\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})$ to calculate the IDF score. This ensures that the words with an IDF score of zero (i.e., words that occur in every document) don’t get suppressed entirely.","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"3edeae","input":"We'll remove the punctuation using regular expressions. Regular expressions are a topic unto themselves and you can google tutorials on them if you'd like. We'll import the regular expression package (called re) in order to use them. Don't worry about the code in the following cell for now, just run it to remove the punctuation:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"4e16f7","input":"Each document is a linear combination of the PCA components. We notice that the first sports-related documents are composed entirely of the first component. The music-related documents are composed entirely of the second component:","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"61725c","input":"### LSA\n\nLatent Semantic Analysis is just SVD applied to a word/document matrix\n\n- D1 = \"I like databases\"\n- D2 = \"I hate databases\"\n\nthen the document-term matrix would be:\n\n$$ \\begin{matrix} \\text{I} & \\text{like} & \\text{hate} & \\text{database} \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 0 & 1 & 1  \\end{matrix} $$\n\nWith each row being a different document\nand each column being a new word.\n\nIn this case our decomposition has a new interpretation:\n- $\\Sigma$ are the importances of each of our topics\n- $U$ is a transform from a word vector to the topics that word is most used in\n- $V$ is a transform from each document to the topics it is about","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"72d5e7","input":"What about an article that is a combination of the two, such as \"redsox hendrix\"?","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"9275af","input":"In summary, we have reduced a 19 dimensional space corresponding to 19 unique words down to 2 dimensions. Similar docs point in similar directions. Dissimilar docs have perpendicular (orthogonal) vectors.\n\nSuppose we have a new sports-related document that is not already in our corpus, such as \"baseball basketball broncos\" and we want to see if it is more similar to component_1 or component_2. We will calculate the dot product of the new vector with each of the components and see which dot product is larger:","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"94f6d9","input":"- **TF-IDF** stands for Term Frequency - Inverse Document Frequency. It takes into account that we should weight rare words more highly than common words.\n\n**Term Frequency** defines the probability of finding a word in the document. Let’s say we want to find what is the probability of finding $\\text{word}_i$ in $\\text{document}_j$:\n\n$\\text{ TermFrequency(word_i,document_j}) = \\frac{\\text{Number of times word_i occurs in document_j}}{\\text{Total number of words in document_j}}$\n\n**Inverse Document Frequency**:The intuition behind IDF is that a word is not of much use if it is appearing in all the documents. It defines how unique the word is in the total corpus:\n\n$\\text{ InverseDocumentFrequency(word_i,All Documents in Corpus}) = \\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})$\n\nIf word_i is more frequent in the corpus then IDF value decreases.\n\nIf word_i is not frequent which means ni decreases and hence IDF value increases.\n\nAnd finally, we obtain the formula for TF-IDF:\n\n$\\text{TF-IDF = TF(word_i, document_j) * IDF(word_i, All documents in corpus)}$\n\nWe can calculate the TF-IDF matrix in the above example:\n\n<img src=\"images/td.png\" width=500>\n\nFrom the above table, we can see that TF-IDF of common words was zero, which shows they are not significant. On the other hand, the TF-IDF of “car” , “truck”, “road”, and “highway” are non-zero. These words have more significance.\n\nYou will see the matrix written in this form:\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"b4d7fb","input":"Let's remove them from our corpus now:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"b85c2a","input":"Sources:\n\nhttps://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863\n\nhttps://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/\n\n\n### Natural Language Processing\n\nNLP is a branch of artificial intelligence that deals with analyzing, understanding and generating the languages that humans use naturally in order to interface with computers in both written and spoken contexts using natural human languages instead of computer languages.\n\n### Applications of NLP\n\n- Machine translation(Google Translate)\n- Natural language generation\n- Web Search\n- Spam filters\n- Sentiment Analysis (positive or negative tone)\n- Chatbots\n\n… and many more\n\n### Preprocessing of data\n\nA **text corpus** is a large and structured set of texts. \n\nHere's an example of a corpus. This example is a document containing three sentences:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"be0e41","input":"Let's perform LSA on a list of six documents below. We'll first create a TF-IDF matrix:","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"cd31ad","input":"We notice that the first component accounted for 14% of the explained variance and the second component accounted from 16%. It may be confusing that these are not in descending order. It is due to a rather obscure fact that we didn't apply the standard scaler to our data first. However, we'll keep our data unscaled in order to more easily interpret later results:","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"dad280","input":"We'll apply SVD using 2 components:","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"e5eec3","input":"### Natural Language Processing\n\nRun the cell below to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"efdbae","input":"- **Tokenization** is the process of breaking up a text document into individual words called tokens. Let's do that now:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"f78d41","input":"We can use the NLTK count vectorizer to create a bag of words, where each row corresponds to a different document:","pos":19,"type":"cell"}
{"id":0,"time":1621964122650,"type":"user"}
{"last_load":1621969816200,"type":"file"}