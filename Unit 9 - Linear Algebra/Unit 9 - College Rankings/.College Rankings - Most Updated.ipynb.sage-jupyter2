{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-966f4bab-1036-4e49-ac91-3d5e5f92bff3.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1620709881452,"exec_count":4,"id":"858e8b","input":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import linalg\n\n\nfrom sklearn.preprocessing import StandardScaler","kernel":"python3","pos":1,"start":1620709878717,"state":"done","type":"cell"}
{"cell_type":"code","end":1620709882478,"exec_count":5,"id":"2fd5d0","input":"A = np.matrix([[3,0],[8,-1]])\nb = np.matrix([[1],[2]])\nA*b","kernel":"python3","output":{"0":{"data":{"text/plain":"matrix([[3],\n        [6]])"},"exec_count":5}},"pos":4,"start":1620709882468,"state":"done","type":"cell"}
{"cell_type":"code","end":1620709883667,"exec_count":6,"id":"96c9c0","input":"print(A)\n#inverse:\nprint(linalg.inv(A))\nprint(A.I)\n\n\nprint()\n\n#transpose:\nprint(A.T)\n\nprint()\n\n#determinant:\nprint(linalg.det(A))","kernel":"python3","output":{"0":{"name":"stdout","text":"[[ 3  0]\n [ 8 -1]]\n[[ 0.33333333  0.        ]\n [ 2.66666667 -1.        ]]\n[[ 0.33333333  0.        ]\n [ 2.66666667 -1.        ]]\n\n[[ 3  8]\n [ 0 -1]]\n\n-3.0\n"}},"pos":6,"start":1620709883657,"state":"done","type":"cell"}
{"cell_type":"code","end":1620709900987,"exec_count":7,"id":"2d7918","input":"linalg.solve(A, b)","kernel":"python3","output":{"0":{"data":{"text/plain":"array([[0.33333333],\n       [0.66666667]])"},"exec_count":7}},"pos":8,"start":1620709900977,"state":"done","type":"cell"}
{"cell_type":"code","end":1620709902299,"exec_count":8,"id":"1899e4","input":"#least squares\nA=np.matrix([[1,1],[1,2],[1,3]])\nb=np.matrix([[1],[2],[2]])\n\nX=linalg.inv(A.T*A)*A.T*b\nprint(X) # solution\nprint()\nprint(A*X) # projection","kernel":"python3","output":{"0":{"name":"stdout","text":"[[0.66666667]\n [0.5       ]]\n\n[[1.16666667]\n [1.66666667]\n [2.16666667]]\n"}},"pos":10,"start":1620709902289,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710769000,"exec_count":13,"id":"8548bd","input":"A=np.matrix([[3,0],[8,-1]])\n\n#eigenvalues \nprint(linalg.eigvals(A))\n\n#eigenvalues and eigenvectors:\nprint(linalg.eig(A))","kernel":"python3","output":{"0":{"name":"stdout","text":"[-1.+0.j  3.+0.j]\n(array([-1.+0.j,  3.+0.j]), array([[0.        , 0.4472136 ],\n       [1.        , 0.89442719]]))\n"}},"pos":12,"start":1620710768993,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710771993,"exec_count":14,"id":"4b093d","input":"df = pd.read_csv('data/collegedata.csv')\ndf","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>College</th>\n      <th>Score</th>\n      <th>Academic Reputation</th>\n      <th>Selectivity rank (lower = better)</th>\n      <th>SAT</th>\n      <th>Percent in Top 10 of HS</th>\n      <th>Acceptance Rate</th>\n      <th>Fac Resources Rank (lower = better)</th>\n      <th>Percent classes fewer than 20 students</th>\n      <th>Percentage classes greater than 50 students (lower = better)</th>\n      <th>Fac Student Ratio (lower = better)</th>\n      <th>Percent full time faculty</th>\n      <th>Graduation Retention rank (lower = better)</th>\n      <th>Freshmen Retention</th>\n      <th>Financial resources rank (lower = better)</th>\n      <th>Alumni Giving rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>williams</td>\n      <td>100</td>\n      <td>92</td>\n      <td>4</td>\n      <td>1420.0</td>\n      <td>91</td>\n      <td>17</td>\n      <td>3</td>\n      <td>71</td>\n      <td>4.0</td>\n      <td>7</td>\n      <td>93</td>\n      <td>1</td>\n      <td>97</td>\n      <td>6</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amherst</td>\n      <td>98</td>\n      <td>92</td>\n      <td>5</td>\n      <td>1425.0</td>\n      <td>84</td>\n      <td>13</td>\n      <td>7</td>\n      <td>70</td>\n      <td>2.0</td>\n      <td>9</td>\n      <td>94</td>\n      <td>1</td>\n      <td>98</td>\n      <td>10</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>swarthmore</td>\n      <td>96</td>\n      <td>91</td>\n      <td>6</td>\n      <td>1440.0</td>\n      <td>84</td>\n      <td>15</td>\n      <td>7</td>\n      <td>74</td>\n      <td>2.0</td>\n      <td>8</td>\n      <td>93</td>\n      <td>4</td>\n      <td>97</td>\n      <td>9</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>middlebury</td>\n      <td>94</td>\n      <td>87</td>\n      <td>6</td>\n      <td>1385.0</td>\n      <td>86</td>\n      <td>18</td>\n      <td>17</td>\n      <td>68</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>94</td>\n      <td>11</td>\n      <td>96</td>\n      <td>3</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pomona</td>\n      <td>94</td>\n      <td>87</td>\n      <td>2</td>\n      <td>1460.0</td>\n      <td>90</td>\n      <td>14</td>\n      <td>20</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>94</td>\n      <td>1</td>\n      <td>98</td>\n      <td>6</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bowdoin</td>\n      <td>93</td>\n      <td>87</td>\n      <td>8</td>\n      <td>1410.0</td>\n      <td>83</td>\n      <td>16</td>\n      <td>14</td>\n      <td>68</td>\n      <td>1.0</td>\n      <td>10</td>\n      <td>93</td>\n      <td>6</td>\n      <td>96</td>\n      <td>14</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>wellesley</td>\n      <td>93</td>\n      <td>89</td>\n      <td>12</td>\n      <td>1390.0</td>\n      <td>78</td>\n      <td>31</td>\n      <td>12</td>\n      <td>69</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>93</td>\n      <td>14</td>\n      <td>95</td>\n      <td>10</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>carlton</td>\n      <td>92</td>\n      <td>88</td>\n      <td>12</td>\n      <td>1415.0</td>\n      <td>78</td>\n      <td>31</td>\n      <td>16</td>\n      <td>65</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>97</td>\n      <td>4</td>\n      <td>97</td>\n      <td>27</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>haverford</td>\n      <td>91</td>\n      <td>83</td>\n      <td>2</td>\n      <td>1400.0</td>\n      <td>94</td>\n      <td>25</td>\n      <td>5</td>\n      <td>79</td>\n      <td>1.0</td>\n      <td>8</td>\n      <td>94</td>\n      <td>6</td>\n      <td>96</td>\n      <td>15</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>claremont_mckenna</td>\n      <td>90</td>\n      <td>85</td>\n      <td>14</td>\n      <td>1390.0</td>\n      <td>71</td>\n      <td>14</td>\n      <td>4</td>\n      <td>86</td>\n      <td>2.0</td>\n      <td>9</td>\n      <td>94</td>\n      <td>11</td>\n      <td>96</td>\n      <td>21</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>vassar</td>\n      <td>90</td>\n      <td>88</td>\n      <td>14</td>\n      <td>1395.0</td>\n      <td>74</td>\n      <td>23</td>\n      <td>20</td>\n      <td>68</td>\n      <td>0.3</td>\n      <td>8</td>\n      <td>95</td>\n      <td>6</td>\n      <td>97</td>\n      <td>13</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>davidson</td>\n      <td>89</td>\n      <td>83</td>\n      <td>10</td>\n      <td>1360.0</td>\n      <td>82</td>\n      <td>28</td>\n      <td>15</td>\n      <td>69</td>\n      <td>0.0</td>\n      <td>11</td>\n      <td>99</td>\n      <td>6</td>\n      <td>96</td>\n      <td>37</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>harvey_mudd</td>\n      <td>89</td>\n      <td>89</td>\n      <td>1</td>\n      <td>1500.0</td>\n      <td>95</td>\n      <td>22</td>\n      <td>18</td>\n      <td>67</td>\n      <td>2.0</td>\n      <td>8</td>\n      <td>97</td>\n      <td>21</td>\n      <td>98</td>\n      <td>18</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>us_naval_academy</td>\n      <td>88</td>\n      <td>88</td>\n      <td>46</td>\n      <td>1270.0</td>\n      <td>53</td>\n      <td>7</td>\n      <td>24</td>\n      <td>61</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>94</td>\n      <td>25</td>\n      <td>97</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>washington_and_lee</td>\n      <td>88</td>\n      <td>78</td>\n      <td>9</td>\n      <td>1395.0</td>\n      <td>81</td>\n      <td>18</td>\n      <td>2</td>\n      <td>74</td>\n      <td>0.2</td>\n      <td>9</td>\n      <td>91</td>\n      <td>14</td>\n      <td>94</td>\n      <td>25</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>hamilton</td>\n      <td>87</td>\n      <td>81</td>\n      <td>17</td>\n      <td>1390.0</td>\n      <td>74</td>\n      <td>27</td>\n      <td>6</td>\n      <td>74</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>94</td>\n      <td>21</td>\n      <td>95</td>\n      <td>23</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>wesleyan</td>\n      <td>86</td>\n      <td>85</td>\n      <td>19</td>\n      <td>1390.0</td>\n      <td>66</td>\n      <td>24</td>\n      <td>48</td>\n      <td>68</td>\n      <td>5.0</td>\n      <td>9</td>\n      <td>97</td>\n      <td>6</td>\n      <td>96</td>\n      <td>29</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>colby</td>\n      <td>84</td>\n      <td>81</td>\n      <td>25</td>\n      <td>1335.0</td>\n      <td>61</td>\n      <td>29</td>\n      <td>20</td>\n      <td>69</td>\n      <td>2.0</td>\n      <td>10</td>\n      <td>93</td>\n      <td>14</td>\n      <td>95</td>\n      <td>29</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>colgate</td>\n      <td>84</td>\n      <td>83</td>\n      <td>19</td>\n      <td>1350.0</td>\n      <td>67</td>\n      <td>29</td>\n      <td>29</td>\n      <td>64</td>\n      <td>2.0</td>\n      <td>9</td>\n      <td>95</td>\n      <td>14</td>\n      <td>94</td>\n      <td>32</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>smith</td>\n      <td>84</td>\n      <td>85</td>\n      <td>35</td>\n      <td>1320.0</td>\n      <td>61</td>\n      <td>45</td>\n      <td>20</td>\n      <td>66</td>\n      <td>5.0</td>\n      <td>9</td>\n      <td>97</td>\n      <td>35</td>\n      <td>92</td>\n      <td>21</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>us_military_academy</td>\n      <td>83</td>\n      <td>83</td>\n      <td>28</td>\n      <td>1260.0</td>\n      <td>58</td>\n      <td>27</td>\n      <td>43</td>\n      <td>67</td>\n      <td>3.0</td>\n      <td>10</td>\n      <td>95</td>\n      <td>14</td>\n      <td>94</td>\n      <td>35</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>bates</td>\n      <td>83</td>\n      <td>83</td>\n      <td>28</td>\n      <td>1340.0</td>\n      <td>58</td>\n      <td>27</td>\n      <td>43</td>\n      <td>67</td>\n      <td>3.0</td>\n      <td>10</td>\n      <td>95</td>\n      <td>14</td>\n      <td>94</td>\n      <td>35</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>grinnell</td>\n      <td>83</td>\n      <td>86</td>\n      <td>32</td>\n      <td>NaN</td>\n      <td>62</td>\n      <td>51</td>\n      <td>29</td>\n      <td>62</td>\n      <td>0.3</td>\n      <td>9</td>\n      <td>91</td>\n      <td>28</td>\n      <td>94</td>\n      <td>27</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>macalaster</td>\n      <td>82</td>\n      <td>83</td>\n      <td>19</td>\n      <td>1340.0</td>\n      <td>70</td>\n      <td>35</td>\n      <td>35</td>\n      <td>70</td>\n      <td>1.0</td>\n      <td>10</td>\n      <td>89</td>\n      <td>28</td>\n      <td>94</td>\n      <td>41</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bryn_mawr</td>\n      <td>81</td>\n      <td>83</td>\n      <td>39</td>\n      <td>1315.0</td>\n      <td>60</td>\n      <td>46</td>\n      <td>27</td>\n      <td>74</td>\n      <td>3.0</td>\n      <td>8</td>\n      <td>90</td>\n      <td>47</td>\n      <td>92</td>\n      <td>23</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>oberlin</td>\n      <td>81</td>\n      <td>82</td>\n      <td>19</td>\n      <td>1370.0</td>\n      <td>68</td>\n      <td>30</td>\n      <td>43</td>\n      <td>70</td>\n      <td>3.0</td>\n      <td>9</td>\n      <td>96</td>\n      <td>32</td>\n      <td>94</td>\n      <td>37</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"                College  Score  Academic Reputation  \\\n0              williams    100                   92   \n1               amherst     98                   92   \n2            swarthmore     96                   91   \n3            middlebury     94                   87   \n4                pomona     94                   87   \n5               bowdoin     93                   87   \n6             wellesley     93                   89   \n7               carlton     92                   88   \n8             haverford     91                   83   \n9     claremont_mckenna     90                   85   \n10               vassar     90                   88   \n11             davidson     89                   83   \n12          harvey_mudd     89                   89   \n13     us_naval_academy     88                   88   \n14   washington_and_lee     88                   78   \n15             hamilton     87                   81   \n16             wesleyan     86                   85   \n17                colby     84                   81   \n18              colgate     84                   83   \n19                smith     84                   85   \n20  us_military_academy     83                   83   \n21                bates     83                   83   \n22             grinnell     83                   86   \n23           macalaster     82                   83   \n24            bryn_mawr     81                   83   \n25              oberlin     81                   82   \n\n    Selectivity rank (lower = better)     SAT  Percent in Top 10 of HS  \\\n0                                   4  1420.0                       91   \n1                                   5  1425.0                       84   \n2                                   6  1440.0                       84   \n3                                   6  1385.0                       86   \n4                                   2  1460.0                       90   \n5                                   8  1410.0                       83   \n6                                  12  1390.0                       78   \n7                                  12  1415.0                       78   \n8                                   2  1400.0                       94   \n9                                  14  1390.0                       71   \n10                                 14  1395.0                       74   \n11                                 10  1360.0                       82   \n12                                  1  1500.0                       95   \n13                                 46  1270.0                       53   \n14                                  9  1395.0                       81   \n15                                 17  1390.0                       74   \n16                                 19  1390.0                       66   \n17                                 25  1335.0                       61   \n18                                 19  1350.0                       67   \n19                                 35  1320.0                       61   \n20                                 28  1260.0                       58   \n21                                 28  1340.0                       58   \n22                                 32     NaN                       62   \n23                                 19  1340.0                       70   \n24                                 39  1315.0                       60   \n25                                 19  1370.0                       68   \n\n    Acceptance Rate  Fac Resources Rank (lower = better)  \\\n0                17                                    3   \n1                13                                    7   \n2                15                                    7   \n3                18                                   17   \n4                14                                   20   \n5                16                                   14   \n6                31                                   12   \n7                31                                   16   \n8                25                                    5   \n9                14                                    4   \n10               23                                   20   \n11               28                                   15   \n12               22                                   18   \n13                7                                   24   \n14               18                                    2   \n15               27                                    6   \n16               24                                   48   \n17               29                                   20   \n18               29                                   29   \n19               45                                   20   \n20               27                                   43   \n21               27                                   43   \n22               51                                   29   \n23               35                                   35   \n24               46                                   27   \n25               30                                   43   \n\n    Percent classes fewer than 20 students  \\\n0                                       71   \n1                                       70   \n2                                       74   \n3                                       68   \n4                                       70   \n5                                       68   \n6                                       69   \n7                                       65   \n8                                       79   \n9                                       86   \n10                                      68   \n11                                      69   \n12                                      67   \n13                                      61   \n14                                      74   \n15                                      74   \n16                                      68   \n17                                      69   \n18                                      64   \n19                                      66   \n20                                      67   \n21                                      67   \n22                                      62   \n23                                      70   \n24                                      74   \n25                                      70   \n\n    Percentage classes greater than 50 students (lower = better)  \\\n0                                                 4.0              \n1                                                 2.0              \n2                                                 2.0              \n3                                                 1.0              \n4                                                 1.0              \n5                                                 1.0              \n6                                                 1.0              \n7                                                 1.0              \n8                                                 1.0              \n9                                                 2.0              \n10                                                0.3              \n11                                                0.0              \n12                                                2.0              \n13                                                0.0              \n14                                                0.2              \n15                                                1.0              \n16                                                5.0              \n17                                                2.0              \n18                                                2.0              \n19                                                5.0              \n20                                                3.0              \n21                                                3.0              \n22                                                0.3              \n23                                                1.0              \n24                                                3.0              \n25                                                3.0              \n\n    Fac Student Ratio (lower = better)  Percent full time faculty  \\\n0                                    7                         93   \n1                                    9                         94   \n2                                    8                         93   \n3                                    9                         94   \n4                                    8                         94   \n5                                   10                         93   \n6                                    8                         93   \n7                                    9                         97   \n8                                    8                         94   \n9                                    9                         94   \n10                                   8                         95   \n11                                  11                         99   \n12                                   8                         97   \n13                                   9                         94   \n14                                   9                         91   \n15                                   9                         94   \n16                                   9                         97   \n17                                  10                         93   \n18                                   9                         95   \n19                                   9                         97   \n20                                  10                         95   \n21                                  10                         95   \n22                                   9                         91   \n23                                  10                         89   \n24                                   8                         90   \n25                                   9                         96   \n\n    Graduation Retention rank (lower = better)  Freshmen Retention  \\\n0                                            1                  97   \n1                                            1                  98   \n2                                            4                  97   \n3                                           11                  96   \n4                                            1                  98   \n5                                            6                  96   \n6                                           14                  95   \n7                                            4                  97   \n8                                            6                  96   \n9                                           11                  96   \n10                                           6                  97   \n11                                           6                  96   \n12                                          21                  98   \n13                                          25                  97   \n14                                          14                  94   \n15                                          21                  95   \n16                                           6                  96   \n17                                          14                  95   \n18                                          14                  94   \n19                                          35                  92   \n20                                          14                  94   \n21                                          14                  94   \n22                                          28                  94   \n23                                          28                  94   \n24                                          47                  92   \n25                                          32                  94   \n\n    Financial resources rank (lower = better)  Alumni Giving rank  \n0                                           6                  58  \n1                                          10                  57  \n2                                           9                  46  \n3                                           3                  55  \n4                                           6                  43  \n5                                          14                  50  \n6                                          10                  46  \n7                                          27                  58  \n8                                          15                  44  \n9                                          21                  43  \n10                                         13                  33  \n11                                         37                  53  \n12                                         18                  33  \n13                                          1                  21  \n14                                         25                  46  \n15                                         23                  47  \n16                                         29                  49  \n17                                         29                  41  \n18                                         32                  40  \n19                                         21                  36  \n20                                         35                  46  \n21                                         35                  46  \n22                                         27                  40  \n23                                         41                  39  \n24                                         23                  40  \n25                                         37                  38  "},"exec_count":14}},"pos":16,"start":1620710771807,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710773255,"exec_count":15,"id":"5f1502","input":"df = df[df['College'] != 'grinnell']","kernel":"python3","pos":18,"start":1620710773248,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710773267,"exec_count":16,"id":"a83ee5","input":"b = df['Score']\nA_dataframe = df.drop(columns=['College', 'Score'])","kernel":"python3","pos":20,"start":1620710773263,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710774711,"exec_count":17,"id":"99e7d4","input":"A=np.matrix(A_dataframe)\nb=np.matrix(b)\nb = b.T\nprint(A.shape, b.shape)","kernel":"python3","output":{"0":{"name":"stdout","text":"(25, 14) (25, 1)\n"}},"pos":22,"start":1620710774705,"state":"done","type":"cell"}
{"cell_type":"code","end":1620710774730,"exec_count":18,"id":"8b0f22","input":"# the solving part\nX=linalg.inv(A.T*A)*(A.T)*b","kernel":"python3","pos":24,"start":1620710774715,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":12,"id":"37bee3","input":"categories = A_dataframe.columns                   # column names\n\ntuples = []                                        # create tuples containing the category weights and names\nfor i in range(len(categories)):\n    tuples.append((X[i][0,0], categories[i]))\n    \ntuples.sort(reverse = True)                        # sort in decending order\n\nfor i in range(len(categories)):                   # print the output\n    print(tuples[i])","output":{"0":{"name":"stdout","output_type":"stream","text":"(0.4748330614781506, 'Freshmen Retention')\n(0.4440284991052175, 'Academic Reputation')\n(0.15681036395747983, 'Alumni Giving rank')\n(0.08345613809772363, 'Percent in Top 10 of HS')\n(0.06145716039747648, 'Fac Student Ratio (lower = better)')\n(0.045818800863871495, 'Selectivity rank (lower = better)')\n(0.02441425042844362, 'Percent classes fewer than 20 students')\n(0.014115335306772399, 'Percentage classes greater than 50 students (lower = better)')\n(-0.0022145837411212195, 'SAT')\n(-0.022757846448288585, 'Percent full time faculty')\n(-0.029479732409194645, 'Acceptance Rate')\n(-0.06828671150818781, 'Graduation Retention rank (lower = better)')\n(-0.08629494142266136, 'Fac Resources Rank (lower = better)')\n(-0.10011751785227262, 'Financial resources rank (lower = better)')\n"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"248e47","input":"#get colby's info\ncolby = df[df['College'] == 'colby']\n\n#apply the least squares projection to colby's info\ncolby = colby.drop(columns = ['College', 'Score'])\ncolby = np.matrix(colby)\ncolby\n\nprojection = colby*X\n\n#print the predicted and actual ranking\nprint('predicted ranking:', projection[0,0])\nprint('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)","output":{"0":{"name":"stdout","output_type":"stream","text":"predicted ranking: 84.5551721286744\ncolby actual ranking: [84]\n"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"57d686","input":"error = 0\nfor i in range(len(A)):\n    projection = A[i][:]*X\n    newerror = abs(projection[0,0] - b[i,0])\n    error = error + newerror\n    \nprint('Average Absolute Prediction Error:', error/len(A))","output":{"0":{"name":"stdout","output_type":"stream","text":"Average Absolute Prediction Error: 0.4953938650829491\n"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":36,"id":"f12cfe","input":"#insert here\nI think that I understand the way of which the weights are ranked, with freshmen retention being the most important, however I am wondering why SAT is of such low importance, has society finally evolve?","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":39,"id":"1d2000","input":"scaler = StandardScaler()\n\nscaler.fit(A_dataframe)\n\nA = np.matrix(scaler.transform(A_dataframe))\n\nprint(linalg.det(linalg.inv(A.T*A)))","output":{"0":{"name":"stdout","output_type":"stream","text":"5.22950749562984e-14\n"},"1":{"name":"stderr","output_type":"stream","text":"/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\n/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n  \"\"\"\n"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":40,"id":"28e2a1","input":"scaler = MinMaxScaler()\n\nscaler.fit(A_dataframe)\n\nA = np.matrix(scaler.transform(A_dataframe))\n\nprint(linalg.det(linalg.inv(A.T*A)))","output":{"0":{"name":"stdout","output_type":"stream","text":"8.073530888128822\n"},"1":{"name":"stderr","output_type":"stream","text":"/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n  return self.partial_fit(X, y)\n"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":41,"id":"66582f","input":"X=linalg.inv(A.T*A)*(A.T)*b\n\ncategories = A_dataframe.columns                   # column names\n\ntuples = []                                        # create tuples containing the category weights and names\nfor i in range(len(categories)):\n    tuples.append((X[i][0,0], categories[i]))\n    \ntuples.sort(reverse = True)                        # sort in decending order\n\nfor i in range(len(categories)):                   # print the output\n    print(tuples[i])\n    \n    \n#get colby's info\ncolby = df[df['College'] == 'colby']\n\n#apply the least squares projection to colby's info\ncolby = colby.drop(columns = ['College', 'Score'])\n\n\n\ncolby = scaler.transform(colby)   # don't scale because it gives worse results\ncolby = np.matrix(colby)\ncolby\n\nprojection = colby*X\n\n#print the predicted and actual ranking\nprint('predicted ranking:', projection[0,0])\nprint('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)","output":{"0":{"name":"stdout","output_type":"stream","text":"(90.08714553229467, 'Selectivity rank (lower = better)')\n(62.90344262517712, 'Percent in Top 10 of HS')\n(28.033586279227876, 'SAT')\n(16.834509195267742, 'Academic Reputation')\n(16.412458903052826, 'Percent classes fewer than 20 students')\n(9.908130978435764, 'Alumni Giving rank')\n(8.702879857101745, 'Fac Resources Rank (lower = better)')\n(7.902273414080135, 'Fac Student Ratio (lower = better)')\n(6.572495288934505, 'Financial resources rank (lower = better)')\n(-0.3363738170115411, 'Percent full time faculty')\n(-6.436559188506777, 'Acceptance Rate')\n(-6.44794516704593, 'Percentage classes greater than 50 students (lower = better)')\n(-11.645287688328125, 'Freshmen Retention')\n(-23.80914101286759, 'Graduation Retention rank (lower = better)')\npredicted ranking: 78.04071482316772\ncolby actual ranking: [84]\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":42,"id":"3636a2","input":"error = 0\nfor i in range(len(A)):\n    projection = A[i][:]*X\n    newerror = abs(projection[0,0] - b[i,0])\n    error = error + newerror\n    \nprint('Average Absolute Prediction Error:', error/len(A))","output":{"0":{"name":"stdout","output_type":"stream","text":"Average Absolute Prediction Error: 3.3892794524637186\n"}},"pos":41,"type":"cell"}
{"cell_type":"markdown","id":"0bbcd7","input":"5.We can also calculate the eigenvalues and eigenvectors of a matrix, which we'll get to more later:","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"173238","input":"The average absolute error is:","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"2a080a","input":"### Linear Algebra Python Intro","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"31a7c0","input":"In summary, when using Ordinary Least Squares to solve for the closed form solution, feature scaling will NOT be necessary; in fact, it may make our predictions worse.\n\nThe exception is when you apply regularization (L1/L2 Ridge, Lasso, etc.) Then you should use feature scaling. However, in the above examples, we have not applied any regularization.\n\nIn the past, we used feature scaling for gradient descent in order to help the solution converge in a shorter period of time.\n\nSpeaking of gradient descent...","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"323e77","input":"Note that the above output is read as $ \\lambda_1 = -1, \\lambda_2 = 3$ with eigenvectors $v_1 = [0,1],v_2=[0.447,0.894]$, which are the normalized versions of what you get from obtaining $v_1 = [0,1],v_2=[1,2]$ by hand.\n\nI must admit that the output you get from typing ```eigenvectors(([[3,0],[8,-1]])``` into wolfram alpha is nicer.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"428699","input":"The average error is also worse:","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"499c6c","input":"6.Print the weights in decending order:","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"4e6c69","input":"There are two ways to solve for an optimal regression solution. You can solve it via the analytical solution (OLS) or via an iterative algorithm such as gradient descent.\n\n1.**Similarities** between the two methods:\n\n- Both can be applied to linear regression models.\n- Both are minimizing the sum of the squared residuals $$\\sum_{i=1}^n(y^{(i)}_{\\text{predicted}}-y^{(i)}_{\\text{actual}})^2$$\n- Both work for multivariate problems.\n\n2.**Differences** between the two methods:\n\nOLS directly calculates the solution by solving for the system of equations generated when setting all partial derivatives = 0. The whole process is analytical and generates a closed form solution.\n\nIn contrast, gradient descent starts from guessing the local min and proceeds by taking small steps along the direction of the steepest descent. It's numerical and iterative and when the step size is small enough, one expects the updated guess to approach the real local min (converge to the least squared solution). In addition, gradient descent is able to tackle a wider array of problems that are analytically unsolvable.\n\nThe OLS closed-form solution may (should) be preferred for “smaller” datasets – if computing (a “costly”) matrix inverse is not a concern. For very large datasets, or datasets where the inverse of $X^T X$ may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the GD or SGD approaches are to be preferred. Here's a good article going into more detail...\n\nhttps://sebastianraschka.com/faq/docs/closed-form-vs-gd.html","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"5a81a1","input":"4.Convert the dataframes to numpy matrices and then tranpose vector b so that the shapes are 25x14 and 25x1.","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"6b5c6e","input":"We can try using the MinMax scaler instead, and this will no longer give us a zero determinat:","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"6d37b2","input":"3.Save the Score column as series b and save the rest of the dataframe (except for the college and score columns) as A_dataframe:","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"70e4ac","input":"1.Let's see what the very important numpy and linalg packages can help us with. We can create matrices and multiply them together:","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"746bc9","input":"SAT Scores are on a much different scale than the other variables, so perhaps we should scale first? We have several options. We do NOT want to use the Standard scaler in this case because it causes the square matrix  $A^T A$ to have a determinant very close to zero, making it a nearly non-invertible matrix, which is bad.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"7595bc","input":"8.Print Colby's actual score and predicted score:","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"7e4744","input":"1.Read in the US News & World Report 2013 College Rankings:","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"81462b","input":"### Ordinary Least Squares vs. Gradient Descent","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"856a03","input":"### A Discussion of Scaling","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"8bcb0d","input":"4.We can find the least squares solution and projection:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"8cbb23","input":"### College Rankings","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"b2c07b","input":"However, it still does not give us predictions as good as the non-scaled version. For example, the weightings are wackier and Colby's prediction is worse:","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"bc87a4","input":"3.We can solve the system $Ax=b$:","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"d30c6b","input":"2.We can calculate the inverse, tranpose, and the determinant of a matrix:","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"d4996b","input":"5.Apply the least squares transformation:","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"e23d18","input":"2.Drop Grinnell since it does not contain SAT info:","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"e48c9d","input":"7.What questions or observations do you have about the importance of these categories?","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"fb075e","input":"Read in the necessary imports:","pos":0,"type":"cell"}
{"id":0,"time":1620706326297,"type":"user"}
{"last_load":1620709785881,"type":"file"}