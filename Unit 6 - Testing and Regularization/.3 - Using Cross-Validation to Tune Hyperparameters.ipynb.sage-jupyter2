{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":32,"id":"0bee9e","input":"X.shape","output":{"0":{"data":{"text/plain":"(506, 13)"},"exec_count":32,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":59,"id":"5eb33a","input":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import RidgeCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":60,"id":"6b0576","input":"boston = load_boston()\nX = boston.data\ny = boston.target","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":61,"id":"501b1a","input":"# intermediate/test split (gives us test set)\nX_intermediate, X_test, y_intermediate, y_test = train_test_split(X, \n                                                                  y, \n                                                                  shuffle=True,\n                                                                  test_size=0.2, \n                                                                  random_state=15)\n\n# train/validation split (gives us train and validation sets)\nX_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n                                                                y_intermediate,\n                                                                shuffle=False,\n                                                                test_size=0.25,\n                                                                random_state=2018)\n# delete intermediate variables\ndel X_intermediate, y_intermediate\n\n# print proportions\nprint('train: {}% | validation: {}% | test {}%'.format(round(len(y_train)/len(y),2),\n                                                       round(len(y_validation)/len(y),2),\n                                                       round(len(y_test)/len(y),2)))","output":{"0":{"name":"stdout","output_type":"stream","text":"train: 0.6% | validation: 0.2% | test 0.2%\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":62,"id":"535a23","input":"alphas = [0.001, 0.01, 0.1, 1, 10]\n\nfor alpha in alphas:\n    # instantiate and fit model\n    model = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n    model.fit(X_train, y_train)\n    \n    # calculate errors\n    new_train_error = mean_squared_error(y_train, model.predict(X_train))\n    new_validation_error = mean_squared_error(y_validation, model.predict(X_validation))\n    new_test_error = mean_squared_error(y_test, model.predict(X_test))\n\n    #calculate R^2s\n    new_validation_r2 = model.score(X_validation, y_validation)\n    new_test_r2 = model.score(X_test, y_test)\n    \n    # print report\n    print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {:6} | val R^2: {} | test R^2: {}'.\n          format(alpha,\n                 round(new_train_error,3),\n                 round(new_validation_error,3),\n                 round(new_test_error,3),\n                 round(new_validation_r2,3),\n                 round(new_test_r2,3)))","output":{"0":{"name":"stdout","output_type":"stream","text":"alpha:   0.001 | train error: 22.924 | val error: 19.804 | test error: 23.958 | val R^2: 0.719 | test R^2: 0.69\nalpha:    0.01 | train error: 22.924 | val error: 19.801 | test error: 23.943 | val R^2: 0.72 | test R^2: 0.69\nalpha:     0.1 | train error: 22.938 | val error: 19.791 | test error:  23.82 | val R^2: 0.72 | test R^2: 0.692\nalpha:       1 | train error: 23.315 | val error: 20.158 | test error: 23.533 | val R^2: 0.714 | test R^2: 0.696\nalpha:      10 | train error: 24.199 | val error: 20.981 | test error: 23.369 | val R^2: 0.703 | test R^2: 0.698\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":63,"id":"91a3fd","input":"# train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    shuffle=True,\n                                                    test_size=0.2, \n                                                    random_state=15)\n\n# instantiate model\nmodel = Ridge(alpha=0.1, fit_intercept=True, random_state=99)\n\n#fit model\nmodel.fit(X_train, y_train)\n\n#evaluate model\nnew_train_error = mean_squared_error(y_train, model.predict(X_train))\nnew_test_error = mean_squared_error(y_test, model.predict(X_test))\nprint('MSE train error:', new_train_error, 'R2 train: ', model.score(X_train, y_train))\nprint('MSE test error:', new_test_error, 'R2 test: ', model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"MSE train error: 21.87892746431845 R2 train:  0.7454491108712407\nMSE test error: 23.679851355216798 R2 test:  0.6937869418612654\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":64,"id":"ef5620","input":"boston = load_boston()\ndf = pd.DataFrame(boston.data)\nX = boston.data\ndf['y'] = boston.target\n\ndf = df.sample(frac=1, random_state=99).reset_index(drop=True)\n\ny = df['y']\nX = df.drop(columns = 'y')","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":65,"id":"0c45bc","input":"alphas = [0.001, 0.01, 0.1, 1, 10]\nprint('Mean Squared Error')\nprint('-'*76)\nfor alpha in alphas:\n    # instantiate and fit model\n    model = Ridge(alpha=alpha, fit_intercept=True, random_state=0)\n    \n    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n\n    # print errors as report\n    print('alpha: {:6} | MSE scores: {} | Average MSE: {:.6}'.\n          format(alpha,\n                 -scores,\n                 np.mean(-scores)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Mean Squared Error\n----------------------------------------------------------------------------\nalpha:  0.001 | MSE scores: [30.96782513 22.11152292 23.22761861 25.21320707 17.54926187] | Average MSE: 23.8139\nalpha:   0.01 | MSE scores: [30.97909387 22.11197295 23.22541345 25.21882094 17.53020742] | Average MSE: 23.8131\nalpha:    0.1 | MSE scores: [31.08656646 22.12097468 23.21110735 25.27376024 17.36271556] | Average MSE: 23.811\nalpha:      1 | MSE scores: [31.79626369 22.32711642 23.34010436 25.63985018 16.72726737] | Average MSE: 23.9661\nalpha:     10 | MSE scores: [33.05123595 22.8678197  24.26513545 25.17216681 16.9264725 ] | Average MSE: 24.4566\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":66,"id":"ccdc00","input":"model = Ridge(alpha=0.1, fit_intercept=True, random_state=0)\n\nscores = cross_val_score(model, X, y, cv=5, scoring='r2')\n\nprint('R2 test error', np.mean(scores))","output":{"0":{"name":"stdout","output_type":"stream","text":"R2 test error 0.7169549585918291\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":67,"id":"b6fd4f","input":"model = Ridge(alpha=0.1, fit_intercept=True, random_state=0)\nmodel.fit(X,y)\nmodel.coef_","output":{"0":{"data":{"text/plain":"array([-1.07473720e-01,  4.65716366e-02,  1.59989982e-02,  2.67001859e+00,\n       -1.66846452e+01,  3.81823322e+00, -2.69060598e-04, -1.45962557e+00,\n        3.03515266e-01, -1.24205910e-02, -9.40758541e-01,  9.36807461e-03,\n       -5.25966203e-01])"},"exec_count":67,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":68,"id":"acc077","input":"model = Ridge(alpha=10, fit_intercept=True, random_state=0)\nmodel.fit(X,y)\nmodel.coef_","output":{"0":{"data":{"text/plain":"array([-0.10143535,  0.0495791 , -0.0429624 ,  1.95202082, -2.37161896,\n        3.70227207, -0.01070735, -1.24880821,  0.2795956 , -0.01399313,\n       -0.79794498,  0.01003684, -0.55936642])"},"exec_count":68,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":69,"id":"0a1523","input":"# We start by seeding the random number generator so that everyone will have the same \"random\" results\nnp.random.seed(9)\n\n# Function that returns the sin(2*pi*x)\ndef f(x):\n    return np.sin(2 * np.pi * x)\n\n# This returns 100 evenly spaced numbers from 0 to 1\nX_plot = np.linspace(0, 1, 100)\n\n#applying model.predict(X) needs the X to be a matrix so the line below adds another dimension\nX = X_plot[:, np.newaxis]\n\n# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\ny = f(X_plot) + np.random.normal(0,.3,100)\n\n#plots the left hand column plots below\n\ndef plot_approximation(model, ax, label=None):\n    \"\"\"Plot the approximation of ``model`` on axis ``ax``. \"\"\"\n    ax.plot(X_plot, f(X_plot), label='ground truth', color='green')\n    ax.scatter(X, y, s=100)\n    ax.plot(X_plot, model.predict(X_plot[:, np.newaxis]), color='red', label=label)\n    ax.set_ylim((-2, 2))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='upper right',frameon=True)\n\n# A helper function to plot the absolute value of the coefficients on the right-hand column plot\ndef plot_coefficients(model, ax, label=None):\n    coef = model.steps[-1][1].coef_.ravel()\n    ax.plot(np.arange(0,degree+1),np.abs(coef), marker='o', label=label)\n    ax.set_ylabel('abs(coefficient)')\n    ax.set_xlabel('coefficients')\n\n#split the data into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.7)\n\n# Set up a figure and axes for 8 plots, 2 per row for 4 rows\nfig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n\ndegree = 3\nalphas = [1e-8, 1e-1, 1, 10]\n\nfor alpha, ax_row in zip(alphas, ax_rows):\n    ax_left, ax_right = ax_row\n    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n    model.fit(X_train, y_train)\n    \n    print(\"alpha: \", alpha, \"    R^2:\", model.score(X_test, y_test))\n    plot_approximation(model, ax_left, label='alpha=%r' % alpha)\n    plot_coefficients(model, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n\nplt.tight_layout()","output":{"0":{"name":"stdout","output_type":"stream","text":"alpha:  1e-08     R^2: 0.7975694106142842\nalpha:  0.1     R^2: 0.41073128170897105\nalpha:  1     R^2: 0.37112276205445915\nalpha:  10     R^2: 0.28458540377371144\n"},"1":{"data":{"image/png":"ec67cf25d6c010cc4b4c60e167c45123037aeb41","text/plain":"<Figure size 1080x1440 with 8 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":70,"id":"053fa2","input":"df = pd.DataFrame(data={'X_plot':X_plot, 'y':y})\ndf = df.sample(frac=1).reset_index(drop=True)\n\ny = df['y']\nX_plot = df['X_plot']\nX = df.drop(columns = 'y')","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":71,"id":"7eed48","input":"rcv = RidgeCV(cv=10, alphas = [1e-8, 1e-1, 1, 10])\nmodel = make_pipeline(PolynomialFeatures(degree), rcv)\nmodel.fit(X, y)\nprint('R2', model.score(X, y))\nprint('alpha: ', model.steps[1][1].alpha_)","output":{"0":{"name":"stdout","output_type":"stream","text":"R2 0.846161803863472\nalpha:  1e-08\n"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":73,"id":"c91d79","input":"rcv = RidgeCV(alphas = [1e-8, 1e-1, 1, 10], store_cv_values = True)\nmodel = make_pipeline(PolynomialFeatures(degree), rcv)\nmodel.fit(X, y)\nprint(f'MSE for each of the alphas {alphas}: {np.mean(model.steps[1][1].cv_values_, axis=0)}')\nprint(f'MSE corresponding to best alpha {model.steps[1][1].alpha_}: {np.mean(model.steps[1][1].cv_values_, axis=0)[2]}')","output":{"0":{"name":"stdout","output_type":"stream","text":"MSE for each of the alphas [1e-08, 0.1, 1, 10]: [0.10647886 0.31318084 0.36364163 0.40929001]\nMSE corresponding to best alpha 1e-08: 0.3636416283450069\n"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":74,"id":"7778eb","input":"df=pd.read_pickle('data/cars2frame.pkl')\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bore</th>\n      <th>stroke</th>\n      <th>compression-ratio</th>\n      <th>horsepower</th>\n      <th>peak-rpm</th>\n      <th>city-mpg</th>\n      <th>highway-mpg</th>\n      <th>...</th>\n      <th>make[T.nissan]</th>\n      <th>make[T.peugot]</th>\n      <th>make[T.plymouth]</th>\n      <th>make[T.porsche]</th>\n      <th>make[T.renault]</th>\n      <th>make[T.saab]</th>\n      <th>make[T.subaru]</th>\n      <th>make[T.toyota]</th>\n      <th>make[T.volkswagen]</th>\n      <th>make[T.volvo]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>172.0</td>\n      <td>65.4</td>\n      <td>52.5</td>\n      <td>3.62</td>\n      <td>2.64</td>\n      <td>9.5</td>\n      <td>82.0</td>\n      <td>4800.0</td>\n      <td>32</td>\n      <td>37</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186.6</td>\n      <td>66.5</td>\n      <td>56.1</td>\n      <td>2.54</td>\n      <td>2.07</td>\n      <td>9.3</td>\n      <td>110.0</td>\n      <td>5250.0</td>\n      <td>21</td>\n      <td>28</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>166.3</td>\n      <td>64.4</td>\n      <td>53.0</td>\n      <td>3.27</td>\n      <td>3.35</td>\n      <td>22.5</td>\n      <td>56.0</td>\n      <td>4500.0</td>\n      <td>34</td>\n      <td>36</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>159.1</td>\n      <td>64.2</td>\n      <td>54.1</td>\n      <td>3.03</td>\n      <td>3.15</td>\n      <td>9.0</td>\n      <td>68.0</td>\n      <td>5000.0</td>\n      <td>31</td>\n      <td>38</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>186.7</td>\n      <td>68.4</td>\n      <td>56.7</td>\n      <td>3.70</td>\n      <td>3.52</td>\n      <td>21.0</td>\n      <td>95.0</td>\n      <td>4150.0</td>\n      <td>28</td>\n      <td>33</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 38 columns</p>\n</div>","text/plain":"   length  width  height  bore  stroke  compression-ratio  horsepower  \\\n0   172.0   65.4    52.5  3.62    2.64                9.5        82.0   \n1   186.6   66.5    56.1  2.54    2.07                9.3       110.0   \n2   166.3   64.4    53.0  3.27    3.35               22.5        56.0   \n3   159.1   64.2    54.1  3.03    3.15                9.0        68.0   \n4   186.7   68.4    56.7  3.70    3.52               21.0        95.0   \n\n   peak-rpm  city-mpg  highway-mpg  ...  make[T.nissan]  make[T.peugot]  \\\n0    4800.0        32           37  ...             0.0             0.0   \n1    5250.0        21           28  ...             0.0             0.0   \n2    4500.0        34           36  ...             0.0             0.0   \n3    5000.0        31           38  ...             0.0             0.0   \n4    4150.0        28           33  ...             0.0             1.0   \n\n   make[T.plymouth]  make[T.porsche]  make[T.renault]  make[T.saab]  \\\n0               0.0              0.0              0.0           0.0   \n1               0.0              0.0              0.0           1.0   \n2               0.0              0.0              0.0           0.0   \n3               0.0              0.0              0.0           0.0   \n4               0.0              0.0              0.0           0.0   \n\n   make[T.subaru]  make[T.toyota]  make[T.volkswagen]  make[T.volvo]  \n0             1.0             0.0                 0.0            0.0  \n1             0.0             0.0                 0.0            0.0  \n2             0.0             1.0                 0.0            0.0  \n3             0.0             0.0                 0.0            0.0  \n4             0.0             0.0                 0.0            0.0  \n\n[5 rows x 38 columns]"},"exec_count":74,"output_type":"execute_result"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":75,"id":"99eeb5","input":"df['price'].hist()","output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x1a1bbcb6a0>"},"exec_count":75,"output_type":"execute_result"},"1":{"data":{"image/png":"6de5bb8d6d496cfabada26392b130aeb57de43b4","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":76,"id":"c4d22f","input":"df['log_price'].hist()","output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x1056fff60>"},"exec_count":76,"output_type":"execute_result"},"1":{"data":{"image/png":"7d57f1c851108097e16f5252d800cef2cd3b37b7","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":78,"id":"9d4dea","input":"y=df['log_price']\nX=df.drop(['log_price','price'],1)\n\nmodel=LinearRegression()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3)\n\nmodel.fit(X_train, y_train)\n\nprint(f\"R^2: {model.score(X_test,y_test)}\")\nprint(f\"Adjusted R^2: {1 - (1-model.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"R^2: 0.9030391054494279\nAdjusted R^2: 0.7443758234575826\n"}},"pos":42,"type":"cell"}
{"cell_type":"code","exec_count":79,"id":"13cbd7","input":"model = RidgeCV(cv=10)\nmodel.fit(X, y)\n\nprint(f\"R^2: {model.score(X,y)}\")\nprint(f\"Adjusted R^2: {1 - (1-model.score(X,y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"R^2: 0.9546709353380736\nAdjusted R^2: 0.9443427940226979\n"}},"pos":44,"type":"cell"}
{"cell_type":"markdown","id":"01e7df","input":"Thus, we can conclude that 69.4% of the variation in home price in our test set can be attributed to variation in our explanatory variables.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"0304c5","input":"Note that once again, we find the optimal alpha to be around 1e-8 with an R2 of 85%. Notice how much nicer it was, though, to simply use RidgeCV to do the work for us.\n\n\nNote that the optimal value alpha found using RidgeCV may not always correspond to the optimal alpha found above using simply Ridge. This is because the manual Ridge approach is not doing any cross-validation and therefore train and test data are the same. Using RidgeCV, though, cross-validation is by default activated. The scoring process used to determine the best parameters is not using the same data for train and test.\n\nYou can print out the mean cv_values_ as you are using store_cv_values = True. This allows you to view the mean squared error of each alpha:","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"09a638","input":"### Using Validation Sets to Tune Hyperparameters: Cross Validation\n\nCross validation assigns a certain percentage of the dataset to test data, and then does this multiple times. Using `cross_val_score` is convenient. Be careful, though:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"0da247","input":"We notice that alpha = 0.1 gives us the lowest mean squared error, so we choose to use that one:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"12b1eb","input":"Recall that regularization is a form of constrained optimization that imposes limits on determining model parameters. It effectively allows us to add bias to a model that’s overfitting (overfitting occurs when your model is so specific to your training dataset that it does worse in predicting your test data). We can control the amount of bias with a hyperparameter called lambda (called ```alpha``` in Python since lambda is a reserved word) that defines regularization strength. Let's see below how we can test different values of alpha against our validation set in order to get the best hyperparameter:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"16aeea","input":"Recall that previously, we used a simple linear regression:","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"197c14","input":"Now, we can use our cross_val_score method:","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"1dc495","input":"### Homework\nCreate a pipeline that first applies the StandardScalar transformation and then applies the RidgeCV(cv=10) model to this cars dataset. ","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"32a8a9","input":"### Sine example\n\nNow, let's indeed return to an example in which we used higher degree polynomials. \nLet's return to our $f(x)=\\sin(2 \\pi x)$ example from the last notebook in which we decided on a degree of 3:","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"39e88f","input":"In addition, recall that the price was skewed right:","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"3dc20c","input":"What about Ridge CV? Typically better!","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"42dcf2","input":"### What is alpha doing here?\n\nWe should take a step back and make sure we understand what tuning alpha did in the above examples. We were not using higher order polynomials and so alpha definitely was not penalizing high degree polynomials. Instead, it was penalizing the complexity of all of the linear features in our model, since the Boston dataset has 13 predictor variables:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"5d6d1f","input":"Run the cell below to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"5e016d","input":"This tests default alpha values of alphas=(0.1, 1.0, 10.0). If we would like to specify our alpha values, we can do so. The alpha_ attribute gives us the optimal alpha value:","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"65d35a","input":"Let's read in the Boston dataset again and make a dataframe out of it:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"79d3f4","input":"We conclude that alpha = 1e-8 corresponding to $R^2$ around 80% is our best option.\n\nThat's a lot of work by hand to investigate the optimal alpha value so let's use some built-in sklearn methods instead:\n\n### Automatic Cross-Validation with sklearn\nSo far we've just been using train/test splits for cross-validation and we haven't let sklearn do the work for us.  We will now use **RidgeCV** to perform regression with automatic Regularization and Cross-Validation in just a few lines!\n\nCreate a `RidgeCV` object with the parameter `cv` set to 10.  \n\nWhat this will do is perform [10-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation).  The CV in RidgeCV means that is has this capability built in.  Because this is a Ridge Regressor, it also has regularization built in.  You can check out the [RidgeCV Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to see the default values it uses for `alpha` (they are configurable as a parameter of course).","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"7bace5","input":"Advantages of a **train/validation/test split:**\n\n- Runs K times faster than K-fold cross-validation\n- Simpler to examine the detailed results of the testing process\n\nAdvantages of **cross validation:**\n\n- More accurate estimate of out-of-sample accuracy\n- More \"efficient\" use of data (every observation is used for both training and testing)\n\nRecommendations for **cross validation:**\n\n- K can be any number, but K=4 or 5 is common\n\n- Each response class should be represented with equal proportions in each of the K folds\n\n- Scikit-learn's cross_val_score function does this by default\n\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"98223f","input":"Recall that we previously compared train/validation/test splits to cross-validation methods:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"9adeda","input":"### Using Validation Sets to Tune Hyperparameters: Test/Train/Split\nIn the Ridge examples below, we would like to tune alpha, so we will create a validation set.\n\nTo create a validation set, we'll need to use test/train/split twice, once to separate the test set and then again to separate the validation set. We'll separate the data into 60% training, 20% validation, and 20% testing:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"9ec761","input":"Therefore, we also created a log(price) column to reduce the skew:","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"c19793","input":"### Shuffle!\nIf we are splitting the data into training/testing samples, we definitely need the data shuffled first! Some methods like train_test_split above have shuffling options built into their methods, but others like cross_val_score and RidgeCV do not. Therefore, we'll first shuffle our data below. To do so, there is a built in pandas command that is helpful. We'll put our X and y data into a dataframe and then after shuffling, we can then update X and y to be the input and output information:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"cf233f","input":"Notice the coefficients for the model using alpha = 0.1:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"d65025","input":"These are much smaller than the coefficients using alpha = 10:","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"db23c5","input":"### Multiple Linear Regression Cars Example\nLet's return to the cars dataset. We'll read it back in a form in which the one-hot matrix has already been created for the car makes. Let's also shuffle it since we'll be using RidgeCV:","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"e225d2","input":"There are a few key takeaways here. First, notice the U-shaped behavior exhibited by the validation error above. It starts around 19.8, goes down for two steps and then back up. Also notice that validation error and test error tend to move together, but by no means is the relationship perfect. We see both errors decrease as alpha increases initially but then test error keeps going down while validation error rises again. \n\nThe U shape is typical. We prefer to be somewhere in the sweet spot between not overfitting (having too little error in the training data) and not underfitting (having too much error in the training data).\n\n\n<img src=\"images/train2.png\" width=\"400\">\n\nIn our case, we'll decide to use alpha=0.1 to now train our entire training dataset since this gave us the smallest validation error:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"f80de6","input":"Unfortunately, RidgeCV does not contain a shuffle argument so we will need to do that first:","pos":29,"type":"cell"}
{"id":0,"time":1608092222216,"type":"user"}
{"last_load":1608092101673,"type":"file"}