{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-a889d7f0-a118-4ab7-bb11-f3567cf5ded9.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1614833735051,"exec_count":2,"id":"57fe83","input":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","kernel":"python3","pos":1,"start":1614833732340,"state":"done","type":"cell"}
{"cell_type":"code","end":1614833825169,"exec_count":3,"id":"232fcd","input":"df = pd.read_csv('data/cars.csv', index_col = 0)\ndf.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wheel-base</th>\n      <th>length</th>\n      <th>width</th>\n      <th>height</th>\n      <th>curb-weight</th>\n      <th>engine-size</th>\n      <th>bore</th>\n      <th>stroke</th>\n      <th>compression-ratio</th>\n      <th>horsepower</th>\n      <th>peak-rpm</th>\n      <th>city-mpg</th>\n      <th>highway-mpg</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>88.6</td>\n      <td>168.8</td>\n      <td>64.1</td>\n      <td>48.8</td>\n      <td>2548</td>\n      <td>130</td>\n      <td>3.47</td>\n      <td>2.68</td>\n      <td>9.0</td>\n      <td>111.0</td>\n      <td>5000.0</td>\n      <td>21</td>\n      <td>27</td>\n      <td>13495.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>88.6</td>\n      <td>168.8</td>\n      <td>64.1</td>\n      <td>48.8</td>\n      <td>2548</td>\n      <td>130</td>\n      <td>3.47</td>\n      <td>2.68</td>\n      <td>9.0</td>\n      <td>111.0</td>\n      <td>5000.0</td>\n      <td>21</td>\n      <td>27</td>\n      <td>16500.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>94.5</td>\n      <td>171.2</td>\n      <td>65.5</td>\n      <td>52.4</td>\n      <td>2823</td>\n      <td>152</td>\n      <td>2.68</td>\n      <td>3.47</td>\n      <td>9.0</td>\n      <td>154.0</td>\n      <td>5000.0</td>\n      <td>19</td>\n      <td>26</td>\n      <td>16500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>99.8</td>\n      <td>176.6</td>\n      <td>66.2</td>\n      <td>54.3</td>\n      <td>2337</td>\n      <td>109</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>10.0</td>\n      <td>102.0</td>\n      <td>5500.0</td>\n      <td>24</td>\n      <td>30</td>\n      <td>13950.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>99.4</td>\n      <td>176.6</td>\n      <td>66.4</td>\n      <td>54.3</td>\n      <td>2824</td>\n      <td>136</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>8.0</td>\n      <td>115.0</td>\n      <td>5500.0</td>\n      <td>18</td>\n      <td>22</td>\n      <td>17450.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   wheel-base  length  width  height  curb-weight  engine-size  bore  stroke  \\\n0        88.6   168.8   64.1    48.8         2548          130  3.47    2.68   \n1        88.6   168.8   64.1    48.8         2548          130  3.47    2.68   \n2        94.5   171.2   65.5    52.4         2823          152  2.68    3.47   \n3        99.8   176.6   66.2    54.3         2337          109  3.19    3.40   \n4        99.4   176.6   66.4    54.3         2824          136  3.19    3.40   \n\n   compression-ratio  horsepower  peak-rpm  city-mpg  highway-mpg    price  \n0                9.0       111.0    5000.0        21           27  13495.0  \n1                9.0       111.0    5000.0        21           27  16500.0  \n2                9.0       154.0    5000.0        19           26  16500.0  \n3               10.0       102.0    5500.0        24           30  13950.0  \n4                8.0       115.0    5500.0        18           22  17450.0  "},"exec_count":3}},"pos":34,"start":1614833825141,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c69b1c","input":"","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"d582b7","input":"one_hot = pd.get_dummies(df['purchased'])\ndf = df.drop('purchased', axis = 1)\ndf = df.join(one_hot)\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>income</th>\n      <th>No</th>\n      <th>Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44</td>\n      <td>72000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>48000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>54000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38</td>\n      <td>61000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>30000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age  income  No  Yes\n0   44   72000   1    0\n1   27   48000   0    1\n2   30   54000   1    0\n3   38   61000   1    0\n4   40   30000   0    1"},"exec_count":10,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"60a88c","input":"standardScaler = StandardScaler()\nX = standardScaler.fit_transform(df)\npd.DataFrame(X)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.593507</td>\n      <td>0.499777</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.088096</td>\n      <td>-0.388716</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.791343</td>\n      <td>-0.166592</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.092551</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.197836</td>\n      <td>-1.055085</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.296753</td>\n      <td>-0.018510</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.989178</td>\n      <td>0.758921</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.187014</td>\n      <td>0.907003</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-1.978356</td>\n      <td>-2.165701</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.187014</td>\n      <td>1.536352</td>\n      <td>-1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          0         1    2    3\n0  0.593507  0.499777  1.0 -1.0\n1 -1.088096 -0.388716 -1.0  1.0\n2 -0.791343 -0.166592  1.0 -1.0\n3  0.000000  0.092551  1.0 -1.0\n4  0.197836 -1.055085 -1.0  1.0\n5 -0.296753 -0.018510 -1.0  1.0\n6  0.989178  0.758921 -1.0  1.0\n7  1.187014  0.907003  1.0 -1.0\n8 -1.978356 -2.165701  1.0 -1.0\n9  1.187014  1.536352 -1.0  1.0"},"exec_count":11,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"b0f2ff","input":"df = pd.DataFrame([[44,72000, 'No', 1000], \n                        [27,48000, 'Yes', 100],\n                        [30,54000,'No', 50],\n                       [38,61000,'No', 100],\n                       [40,30000,'Yes', 20],\n                       [35,58000, 'Yes', 1000],\n                       [48,79000,'Yes', 500],\n                       [50,83000,'No', 2],\n                      [18,0,'No', 50],\n                      [50, 100000, 'Yes', 1000]], columns = ['age', 'income', 'purchased', 'price_of_item'])\n\none_hot = pd.get_dummies(df['purchased'])\ndf = df.drop('purchased', axis = 1)\ndf = df.join(one_hot)\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>income</th>\n      <th>price_of_item</th>\n      <th>No</th>\n      <th>Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44</td>\n      <td>72000</td>\n      <td>1000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>48000</td>\n      <td>100</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>54000</td>\n      <td>50</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38</td>\n      <td>61000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>30000</td>\n      <td>20</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age  income  price_of_item  No  Yes\n0   44   72000           1000   1    0\n1   27   48000            100   0    1\n2   30   54000             50   1    0\n3   38   61000            100   1    0\n4   40   30000             20   0    1"},"exec_count":12,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"d57781","input":"y = df['price_of_item']\nX = df.drop(columns=['price_of_item'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)\n\nstandardScaler = StandardScaler()\nX_train = standardScaler.fit_transform(X_train)\npd.DataFrame(X_train)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.916261</td>\n      <td>-0.248082</td>\n      <td>-0.774597</td>\n      <td>0.774597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.723364</td>\n      <td>0.587562</td>\n      <td>1.290994</td>\n      <td>-1.290994</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.784298</td>\n      <td>-1.919370</td>\n      <td>1.290994</td>\n      <td>-1.290994</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.337570</td>\n      <td>-0.874815</td>\n      <td>-0.774597</td>\n      <td>0.774597</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.302056</td>\n      <td>1.562480</td>\n      <td>-0.774597</td>\n      <td>0.774597</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.144673</td>\n      <td>0.100103</td>\n      <td>-0.774597</td>\n      <td>0.774597</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.109158</td>\n      <td>0.831292</td>\n      <td>-0.774597</td>\n      <td>0.774597</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.626916</td>\n      <td>-0.039171</td>\n      <td>1.290994</td>\n      <td>-1.290994</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"          0         1         2         3\n0 -0.916261 -0.248082 -0.774597  0.774597\n1  0.723364  0.587562  1.290994 -1.290994\n2 -1.784298 -1.919370  1.290994 -1.290994\n3  0.337570 -0.874815 -0.774597  0.774597\n4  1.302056  1.562480 -0.774597  0.774597\n5 -0.144673  0.100103 -0.774597  0.774597\n6  1.109158  0.831292 -0.774597  0.774597\n7 -0.626916 -0.039171  1.290994 -1.290994"},"exec_count":13,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"c58b1b","input":"model = LinearRegression()\n\n#fit the model and give the training error\nmodel.fit(X_train, y_train)\nprint('Train R^2:', model.score(X_train, y_train))\n\n# Evaluate the model against the TRANFORMED testing data\nX_test = standardScaler.fit_transform(X_test)\nprint('Test R^2:', model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"Train R^2: 0.50157724313442\nTest R^2: -529.933820344465\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"36f4bc","input":"model = LinearRegression()\n\nscores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n\n# scores output is negative, which is because\n# Scikit-learn uses negative mean squared error so that \n# scores always improve with higher values \nprint(-scores)\nprint('Average MSE: ', np.mean(-scores))","output":{"0":{"name":"stdout","output_type":"stream","text":"[983615.94433939 185423.75792239 277565.66877086 325066.6933274\n 127597.53261061]\nAverage MSE:  379853.91939413117\n"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"04cd2b","input":"#insert work here\n# setting the x and y\ny = df['price']\nX = df.drop(columns=['price'])\n# training with test_size at 0.3\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3)\nstandardScaler = StandardScaler()\nX_train = standardScaler.fit_transform(X_train)\npd.DataFrame(X_train)\nmodel = LinearRegression()\n\n#fit the model and give the training error\nmodel.fit(X_train, y_train)\nprint('Train R^2:', model.score(X_train, y_train))\n\n# Evaluate the model against the TRANFORMED testing data\nX_test = standardScaler.fit_transform(X_test)\nprint('Test R^2:', model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"Train R^2: 0.854819470767963\nTest R^2: 0.8006775032495961\n"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"e9b0f5","input":"boston = load_boston()\nboston.data","output":{"0":{"data":{"text/plain":"array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n        4.9800e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n        9.1400e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n        4.0300e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        5.6400e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n        6.4800e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        7.8800e+00]])"},"exec_count":2,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"ef3a18","input":"model = LinearRegression()\n\nscores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\nprint(scores)","output":{"0":{"name":"stdout","output_type":"stream","text":"[-17485530.54933337 -15938866.61126756 -16131885.83884144\n -16594395.02172921  -9822838.24330085]\n"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"461f3a","input":"print(boston.DESCR)\nprint(load_boston.__doc__)","output":{"0":{"name":"stdout","output_type":"stream","text":".. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\nLoad and return the boston house-prices dataset (regression).\n\n    ==============   ==============\n    Samples total               506\n    Dimensionality               13\n    Features         real, positive\n    Targets           real 5. - 50.\n    ==============   ==============\n\n    Read more in the :ref:`User Guide <boston_dataset>`.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    Returns\n    -------\n    data : :class:`~sklearn.utils.Bunch`\n        Dictionary-like object, with the following attributes.\n\n        data : ndarray of shape (506, 13)\n            The data matrix.\n        target : ndarray of shape (506, )\n            The regression target.\n        filename : str\n            The physical location of boston csv dataset.\n\n            .. versionadded:: 0.20\n\n        DESCR : str\n            The full description of the dataset.\n        feature_names : ndarray\n            The names of features\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n\n    Notes\n    -----\n        .. versionchanged:: 0.20\n            Fixed a wrong data point at [445, 0].\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_boston\n    >>> X, y = load_boston(return_X_y=True)\n    >>> print(X.shape)\n    (506, 13)\n    \n"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"fedecb","input":"X = boston.data\ny = boston.target","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"3ad648","input":"model = LinearRegression()\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.3)\n\n\n# Fit the model against the training data\nmodel.fit(X_train, y_train)\n\n# Evaluate the model against the testing data\nprint('Train:', model.score(X_train, y_train))\nprint('Test:', model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"Train: 0.7024586593727125\nTest: 0.8032713871812368\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"2a4aff","input":"#create dataframe\ndf = pd.DataFrame(X)\ndf['price'] = y\n\n#now sort\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# now we can separate the predictor and target variables again\ny = df['price']\nX = df.drop(columns = 'price')","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"2433ab","input":"model = LinearRegression()\n\nscores = cross_val_score(model, X, y, cv=5, scoring='r2')\n\nprint(scores)\nprint('Average Test R2: ', np.mean(scores))","output":{"0":{"name":"stdout","output_type":"stream","text":"[0.7395564  0.67407191 0.78842333 0.67238546 0.69193864]\nAverage Test R2:  0.7132751477907916\n"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"42489e","input":"df = pd.DataFrame([[44,72000, 'No'], \n                        [27,48000, 'Yes'],\n                        [30,54000,'No'],\n                       [38,61000,'No'],\n                       [40,30000,'Yes'],\n                       [35,58000, 'Yes'],\n                       [48,79000,'Yes'],\n                       [50,83000,'No'],\n                      [18,0,'No'],\n                      [50, 100000, 'Yes']], columns = ['age', 'income', 'purchased'])\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>income</th>\n      <th>purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44</td>\n      <td>72000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>48000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>54000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38</td>\n      <td>61000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>40</td>\n      <td>30000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age  income purchased\n0   44   72000        No\n1   27   48000       Yes\n2   30   54000        No\n3   38   61000        No\n4   40   30000       Yes"},"exec_count":9,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"markdown","id":"05171d","input":"Run the cell below to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"174f13","input":"Run the above cell multiple times to notice the variation. Notice the R^2 value of the test (hold-out) set. Notice that model performance is usually a little lower on the test set. This is expected. In fact, this lower value is a much more accurate number to report as \"real world\" performance.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"38dbf9","input":"If we wanted to do an 80/20 train/test split, we would first split the data and then apply feature scaling to our training set only:","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"39e17f","input":"### Simple Test/Train sets\nLet's omit the validation set for now and focus on splitting into training and testing sets. You'll definitely want to **shuffle** the data first (What if your data happened to be sorted? That would really mess with your results, as you would be training on a dataset very different than your testing set.) Luckily, train_test_split has a sorting parameter built in.\n\nIf we save 30% for the testing set and run a simple linear regression, we get the following results:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"3bef1a","input":"### Feature Scaling\nSource: https://medium.com/@contactsunny/why-do-we-need-feature-scaling-in-machine-learning-and-how-to-do-it-using-scikit-learn-d8314206fe73\n\nWhen you’re working with a learning model, it is important to scale the features to a range which is centered around zero. This is done so that the variance of the features are in the same range. If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset, which is not something we want happening in our model.\nThe aim here is to to achieve Gaussian with zero mean and unit variance.\n\nThe SciKit Learn library provides a class to easily scale our data. We can use the StandardScaler class from the library for this. Now that we know why we need to scale our features, let’s see how to do it. Consider the following dataset of consumer info:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"4c007c","input":"As we can see now, the features are not at all on the same scale. We definitely need to scale them. The StandardScalar method standardizes features (each column) by removing the mean and scaling to unit variance.\n\nThe standard score of a sample x is calculated as:\n\n$z = (x - \\mu) / s$\n\nLet’s look at the code for doing that:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"52b123","input":"This is pretty similar to what we got above using train_test_split. Notice, though, that one or two of the five partitions can give wackier results than the others. If you happened to do only one train/test split on that wackier data, your R2 would be much worse than the cross_val_score results.\n\nAlso note that there are many built in scoring techniques to cross_val_score. We can look at the average Mean Squared Error (MSE) instead of the R2:","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"55d21a","input":"Now, after running this code with the dataset given above, we end up with a nicely scaled set of features as shown below. We can pass this input to a model and get much better results.\n\nNote that StandardScaler is the normalizer that we will mostly use in this class, but there are others. For example, the sk-learn MinMaxScaler standardizes according to this formula:\n\n$z = (x - x_{\\text{min}}) / (x_{\\text{max}}- x_{\\text{min}})$\n\nYou can investigate the various scalers and their pros and cons if you would like.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"7b5a1f","input":"Source: https://www.ritchieng.com/machine-learning-cross-validation/\n\n\nAdvantages of a **train/test split:**\n\n- Runs K times faster than K-fold cross-validation\n- Simpler to examine the detailed results of the testing process\n\nAdvantages of **cross validation:**\n\n- More accurate estimate of out-of-sample accuracy\n- More \"efficient\" use of data (every observation is used for both training and testing)\n\nRecommendations for **cross validation:**\n\n- K can be any number, but K=4 or 5 is common\n\n- Each response class should be represented with equal proportions in each of the K folds\n\n- Scikit-learn's cross_val_score function does this by default","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"9932ac","input":"We can now apply the cross_val_score method:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"a8a442","input":"However, we can read the docs to understand better:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"cc41ff","input":"We find that all of the predictor variables are contained in data and the target variable (housing price) is contained in the target:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"cd17b8","input":"## Training and Testing\n---\n<a class=\"anchor\" id=\"train\"></a>\n\nThe previous lessons were meant to get you comfortable with different types of machine learning algorithms. However, in practice, we would never use our entire dataset to train our model. Instead, we would use a portion of our data, the training set, to train the data, and then we would evaluate the accuracy of our model on the testing portion of our dataset. Since the test portion was not used to train the model, it gives us a more honest indication of how well our model does at predicting new data it hasn't encountered before. There are a few techniques for training and testing. This first one is less computationally intensive.\n\n### Technique 1: Train/Validate/Test\n\nFirst, a couple of vocab words:\n\n**Training Dataset**: The sample of data used to fit the model. The actual dataset that we use to train the model. The model sees and learns from this data.\n\n**Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly.\n\n**Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner).\n\n<img src=\"images/train.png\" width=\"400\">\n\nSide note: there is no hard and fast rule about how to proportion your data. Just know that your model is limited in what it can learn if you limit the data you feed it. However, if your test set is too small, it won’t provide an accurate estimate as to how your model will perform. \n\n### Boston Example\n\nScikit-learn has many data sets built in that you can use. Let's load in a Boston dataset of housing info and housing prices. When you first load the data, it's a bit hard to understand what's going on:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"cfb501","input":"### Technique 2: Cross Validation\n\nCross validation assigns a certain percentage of the dataset to test data, and then does this multiple times. \n\nOne round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on the training set and testing the analysis on the other subset. To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance.\n\nThe upside of this method is it avoids unluckily sampling one unrepresentative set of test data. However, the downside is this method is computationally intensive. This method works great on small to medium-sized datasets. This is absolutely not the kind of thing you’d want to try on a massive dataset. \n\n<img src=\"images/train3.png\" width=\"400\">\n\nNot surprisingly, scikit-learn can help us do this using a method called cross_val_score. Unfortunately, though, cross_val_score does not come with a built in shuffle method. Therefore, will will need to shuffle the data first. Pandas has a built-in command to do this, but we'll need to make the data into a DataFrame first:","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"d675ca","input":"### Homework\n\nDo a 70/30 train/test split and a cv=5 cross validation on your previous car dataset. Be sure to sort and apply the StandardScalar to the training set first.\n\nHow does the error compare to what you previously reported using the whole dataset?","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"d877f5","input":"Then, we would apply our linear regression model. If we wanted to score our model on the testing data, we would then transform that separately:","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"ea1a66","input":"Let's transform the categorical column purchased to a one-hot matrix:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"f01cea","input":"Source: https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data\n\nWe should be careful to not apply scaling to the entire dataset at the beginning, though.\n\nIn the interest of preventing information about the distribution of the test set leaking into your model, you should fit the scaler on your training data only, then standardise both training and test sets with that scaler. If instead you fit the scaler on the full dataset prior to splitting, information about the test set is used to transform the training set, which in turn is passed downstream.\n\nAs an example, knowing the distribution of the whole dataset might influence how you detect and process outliers, as well as how you parameterise your model. Although the data itself is not exposed, information about the distribution of the data is. As a result, your test set performance is not a true estimate of performance on unseen data. \n\nConsider our very small consumer example above, and now suppose we wanted to use our input data to predict the price of the item:","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"fa28b6","input":"Don't be freaked out if the test $R^2$ that you happen to get above is negative. This example was totally crappy and made up. The best possible $R^2$ score is 1.0. A constant model that always predicts the expected value of y, disregarding the input features, would get an $R^2$ score of 0.0. An $R^2$ can be negative if the model is arbitrarily worse than just guessing the expected value of y.","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"ff5308","input":"### Featuring Scaling and Train/Test Splits","pos":25,"type":"cell"}
{"id":0,"time":1614811344677,"type":"user"}
{"last_load":1614833696434,"type":"file"}