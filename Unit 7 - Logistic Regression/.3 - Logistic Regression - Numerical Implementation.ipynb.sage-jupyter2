{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":110,"id":"613c9d","input":"from itertools import product\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":111,"id":"fd4dbd","input":"X = df.drop(columns = [\"health status\"])\ny = df[\"health status\"]\n\nmodel = LogisticRegression(solver=\"lbfgs\")\nmodel.fit(X, y)\nprint(model.score(X,y))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.85\n"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":112,"id":"bb61db","input":"X = df.drop(columns = [\"health status\"])\nQ = X\nQ['dummy'] = 0\ny = df[\"health status\"]\nh = .02  # meshsize\nx_min, x_max = Q['weight'].min() - .5, Q['weight'].max() + .5 \ny_min, y_max = Q['dummy'].min() - .5, Q['dummy'].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nmodel.fit(X.iloc[:,:2], y)\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()]) # ravel() flattens the data\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(Q['weight'], Q['dummy'], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.title('Decision boundaries with 1 attribute')\nplt.xlabel('weight')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.yticks(());","output":{"0":{"data":{"image/png":"a99e6e871f8c847239d4341814285041bf1b9363","text/plain":"<Figure size 288x216 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":114,"id":"d31df0","input":"print(model.classes_)\n\nmodel.fit(X, y)\nconfusion_matrix(y, model.predict(X))","output":{"0":{"name":"stdout","output_type":"stream","text":"[0 1]\n"},"1":{"data":{"text/plain":"array([[ 3,  2],\n       [ 1, 14]])"},"exec_count":114,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":115,"id":"cbca56","input":"# Generate a confusion matrix plot: \n\ndef plot_confusion_matrix(cm, classes=[0, 1], title='some confusion matrix',\n                          normalize=False,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                horizontalalignment=\"center\", size=20,\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    \nplot_confusion_matrix(confusion_matrix(y, model.predict(X)))","output":{"0":{"data":{"image/png":"99fd800b143f10deef532367d4e43a94fabc6e42","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":116,"id":"52ac8d","input":"print(model.intercept_)\nprint(model.coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"[4.3727885]\n[[-0.03673668  0.        ]]\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":119,"id":"da38be","input":"X = df.drop(columns = [\"health status\"])\ny = df[\"health status\"]\n\nmodel = LogisticRegression(solver=\"lbfgs\")\nmodel.fit(X, y)\n\nprint(model.predict([[118]]))\nprint(model.predict([[120]]))","output":{"0":{"name":"stdout","output_type":"stream","text":"[1]\n[0]\n"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":123,"id":"cc9ebc","input":"model = LogisticRegression(solver=\"lbfgs\")\n\nscores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n\n# scores output is negative, which is because Scikit-learn uses negative mean squared error so that scores always improve with higher values \nprint(-scores)\nprint('Average MSE: ', np.mean(-scores))","output":{"0":{"name":"stdout","output_type":"stream","text":"[0.25 0.25 0.   0.25 0.25]\nAverage MSE:  0.2\n"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":160,"id":"ba0cb8","input":"iris = load_iris()\n\nX = pd.DataFrame(iris.data, columns = iris.feature_names)\nX.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2"},"exec_count":160,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":161,"id":"4a41a2","input":"y = iris.target\ny","output":{"0":{"data":{"text/plain":"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"},"exec_count":161,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":162,"id":"bc0e5b","input":"model = LogisticRegression(multi_class = \"auto\", solver = 'lbfgs', max_iter=1000)\n\nmodel.fit(X, y)\n\nprint(model.score(X,y))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.9733333333333334\n"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":163,"id":"d9d4de","input":"print(model.intercept_)\nprint(model.coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"[  9.85483916   2.23332662 -12.08816578]\n[[-0.42409864  0.96688364 -2.51735629 -1.07931427]\n [ 0.53500171 -0.32104132 -0.20651985 -0.94434646]\n [-0.11090307 -0.64584232  2.72387614  2.02366073]]\n"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":164,"id":"b65cc4","input":"print(X.iloc[0])\nprint('\\n predicton:', model.predict([X.iloc[0]]))","output":{"0":{"name":"stdout","output_type":"stream","text":"sepal length (cm)    5.1\nsepal width (cm)     3.5\npetal length (cm)    1.4\npetal width (cm)     0.2\nName: 0, dtype: float64\n\n predicton: [0]\n"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":165,"id":"2a550b","input":"def prob_function(intercept, coeff, X):\n    term = np.exp(intercept+X[0]*coeff[0]+X[1]*coeff[1]+X[2]*coeff[2]+X[3]*coeff[3])\n    return term/(1+term)\n\nprint(prob_function(model.intercept_[0], model.coef_[0], X.iloc[0]))\nprint(prob_function(model.intercept_[1], model.coef_[1], X.iloc[0]))\nprint(prob_function(model.intercept_[2], model.coef_[2], X.iloc[0]))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.9993486860020572\n0.9664370584435653\n2.2633409366700106e-05\n"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":166,"id":"914190","input":"# Plot the decision boundary in 2-d \n# For that, we will assign a color to each point in the \n# mesh [x_min, x_max] x [y_min, y_max].\n\nQ = X.values\nh = .02  # meshsize\nx_min, x_max = Q[:, 0].min() - .5, Q[:, 0].max() + .5 \ny_min, y_max = Q[:, 1].min() - .5, Q[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nmodel.fit(X.iloc[:,:2], y)\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()]) # ravel() flattens the data\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(Q[:, 0], Q[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\nplt.title('Decision boundaries with 2 attributes')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())","output":{"0":{"data":{"text/plain":"(1.5, 4.900000000000003)"},"exec_count":166,"output_type":"execute_result"},"1":{"data":{"image/png":"791c665826954ca75f9366a3d0d479138062dcf1","text/plain":"<Figure size 288x216 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":167,"id":"a8948b","input":"model.fit(X, y)\nconfusion_matrix(y, model.predict(X))","output":{"0":{"data":{"text/plain":"array([[50,  0,  0],\n       [ 0, 47,  3],\n       [ 0,  1, 49]])"},"exec_count":167,"output_type":"execute_result"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":169,"id":"6b15b8","input":"# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n\nmodel = LogisticRegression(multi_class = \"auto\", solver = 'lbfgs', max_iter=1000)\n\nmodel.fit(X_train, y_train)\n\nacc_train = model.score(X_train, y_train)\nacc_test = model.score(X_test, y_test)\nprint(f\"train_score={acc_train}\")\nprint(f\"test_score={acc_test}\")\n\nplot_confusion_matrix(confusion_matrix(y_train, model.predict(X_train)))","output":{"0":{"name":"stdout","output_type":"stream","text":"train_score=0.9714285714285714\ntest_score=0.9333333333333333\n"},"1":{"data":{"image/png":"8399e93ff7cb350fdda5ed332d690f5cc0fd4ede","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":170,"id":"845ddb","input":"digits = load_digits()\n#print(digits.DESCR)\n#print(load_digits.__doc__)","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":171,"id":"81df57","input":"print(digits.data[0])","output":{"0":{"name":"stdout","output_type":"stream","text":"[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"}},"pos":42,"type":"cell"}
{"cell_type":"code","exec_count":172,"id":"3a01ba","input":"plt.gray()\nplt.matshow(digits.images[0])","output":{"0":{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x1a1cc25ac8>"},"exec_count":172,"output_type":"execute_result"},"1":{"data":{"text/plain":"<Figure size 432x288 with 0 Axes>"},"output_type":"display_data"},"2":{"data":{"image/png":"27d899ecf587284a661359e216e3f0dd5fb1bdec","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":173,"id":"90e28f","input":"for i in range(5):\n    image = digits.images[i]\n    digitname = digits.target[i]\n    plt.matshow(image)\n    plt.title(\"Digit %s\" % digitname)","output":{"0":{"data":{"image/png":"3808034e690c70df2ac5c40115198e35d2e66ea8","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"image/png":"9a0c5ef96f6efe13e6d07ca210aa86bd903321e3","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"2":{"data":{"image/png":"a9aefa3c6f57473aadce3b033c9566c901cb9b11","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"3":{"data":{"image/png":"25c052025c7474f1346cc72b62482d0bec545ad5","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"4":{"data":{"image/png":"b3b96bf9f75ea30af88b5112a9b61065d0664a14","text/plain":"<Figure size 288x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":46,"type":"cell"}
{"cell_type":"code","exec_count":202,"id":"eecff6","input":"X, y = digits.data, digits.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=90)\n\n\nmodel = LogisticRegression(multi_class = \"auto\", solver = 'lbfgs', max_iter=10000)\nmodel.fit(X_train, y_train)\n\nprint('train accuracy ', model.score(X_train, y_train))\nprint('test accuracy', model.score(X_test,y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"train accuracy  1.0\ntest accuracy 0.9611111111111111\n"}},"pos":48,"type":"cell"}
{"cell_type":"code","exec_count":204,"id":"859243","input":"confusion_matrix(y, model.predict(X))","output":{"0":{"data":{"text/plain":"array([[176,   0,   0,   0,   0,   1,   0,   1,   0,   0],\n       [  0, 180,   1,   1,   0,   0,   0,   0,   0,   0],\n       [  0,   0, 175,   2,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0, 183,   0,   0,   0,   0,   0,   0],\n       [  0,   1,   0,   0, 178,   0,   0,   1,   1,   0],\n       [  0,   0,   0,   0,   0, 180,   1,   0,   0,   1],\n       [  0,   1,   0,   0,   0,   0, 180,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   1,   0, 178,   0,   0],\n       [  0,   1,   0,   0,   0,   1,   1,   0, 170,   1],\n       [  0,   0,   0,   1,   0,   2,   0,   0,   1, 176]])"},"exec_count":204,"output_type":"execute_result"}},"pos":50,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"209001","input":"df = pd.read_csv(\"data/dogweights.csv\")\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weight</th>\n      <th>health status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>52</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   weight  health status\n0      50              1\n1      50              1\n2      52              1\n3      50              1\n4      48              0"},"exec_count":3,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"0802b7","input":"There are 1797 images in the dataset. Since each image is 8 pixels wide by 8 pixels high, there are 64 attributes for each image. Each attribute takes on an integer value from 0 through 16, indicating black to white. Using these 64 numerical attributes, we'd like to predict the category (i.e., is the digit 0, 1, 2, ..., or 9)? Let's view one of the images' numerical data:","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"102158","input":"Notice that in each testing set of 4 dogs, there was one dog that was misclassified except in the third iteration where all four dogs were classified correctly.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"119779","input":"Let's view the digits 0 through 4:","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"1f61c3","input":"What are the actual model coefficients?","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"236139","input":"We can see this cutoff boundary by predicting a dog weighing 118 pounds and 120 pounds:","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"2437eb","input":"If we plug this iris' properties into the three regression models above, we see that the largest probability is the first one, the iris setosa. Therefore, this data point gets classified as an iris setosa:","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"2e3800","input":"This means that our model is given by:\n\n$P(\\text{Iris Setosa}) = \\frac{e^{9.8549-0.424(\\text{s length})+0.966(\\text{s width})-2.517(\\text{p length})-1.079(\\text{p width})}}{1+e^{9.8549-0.424(\\text{s length})+0.966(\\text{s width})-2.517(\\text{p length})-1.079(\\text{p width})}}$\n\n\n$P(\\text{Iris Versicolour}) = \\frac{e^{2.233+0.535(\\text{s length})-0.32(\\text{s width})-0.206(\\text{p length})-0.94(\\text{p width})}}{1+e^{2.233+0.535(\\text{s length})-0.32(\\text{s width})-0.206(\\text{p length})-0.94(\\text{p width})}}$\n\n\n$P(\\text{Iris Virginica}) = \\frac{e^{-12.088-0.11(\\text{s length})-0.645(\\text{s width})+2.723(\\text{p length})+2.023(\\text{p width})}}{1+e^{-12.088-0.11(\\text{s length})-0.645(\\text{s width})+2.723(\\text{p length})+2.023(\\text{p width})}}$\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"526815","input":"The target variable is the categorical type of iris, where 0, 1, and 2 correspond to Iris Setosa, Iris Versicolour, and Iris Virginica, respectively.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"56df91","input":"We can see from the confusion matrix below that out of all the 1's, for example, one was incorrectly labeled as a 2 and one was incorrectly labeled as a 3. Out of the 8's, one each were mislabeled as a 1, 5, 6, 9.","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"5e4683","input":"Let's see how this works in practice. The first iris in our training set has these properties and our model predicts it be Class 0 (the Iris Setosa).","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"642116","input":"What is the actual model?","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"717522","input":"To view the matrix more clearly, we can use the following code (again, don't focus too much on the actual code):","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"73d079","input":"Let's visualize which ones may have gotten classified incorrectly. Don't focus on the code too much, just the picture:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"93871f","input":"Run the following cell to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"a58960","input":"Let's create our logistic regression model and calculate its score (which returns the mean accuracy on the given test data and labels). We see that out of 150 irises, it classifies 96% correctly, meaning only 6 irises got classified incorrectly:","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"a89266","input":"We can't visualize in 2D exactly which 4 got classified incorrectly, because there are four explanatory variables. However, we can make a good guess of which might be those 4 if we visualize the decision boundary between just sepal width and height. Don't pay too much attention to the code, just view the picture:","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"abe640","input":"Of course, we have violated the cardinal rule of machine learning which is that we didn't first break things up into a test/train split and only measure our success by the testing set accuracy. Let's do that now:","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"b3197a","input":"Of course, we've violated one of the biggest rules in all of machine learning which is to evaluate our accuracy on the test data not the training set. So let's first do a test/train split and then view the confusion matrix of the testing data:","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"c7b318","input":"### The Iris Example\nWe'll do a Multinomial Logistic Regression now, in which we use the famous iris dataset to classify irises by their measurements as Iris Setosa, Iris Versicolour, Iris Virginica. Let's read the dataset in:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"c98e64","input":"And its corresponding image:","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"cb2531","input":"Let's use Logistic Regression to predict the digits:","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"d186b4","input":"To see exactly which ones are misclassified, we can create a confusion matrix. We see that two unhealthy dogs were classfied incorrectly as healthy and one healthy dog was classified incorrectly as not healthy:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"d3ba10","input":"To see exactly which ones were mislabeled, we can view a confusion matrix. We can see that three Iris Versicolours were incorrectly mislabeled as Iris Virginica and one Iris Virginica was incorrectly mislabeled as a Iris Versicolours.","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"d41eee","input":"Let's create our logistic regression model and calculate its score (which returns the mean accuracy on the given test data and labels). We see that out of 20 dogs, it classifies 85% correctly, meaning only 3 dogs got classified incorrectly:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"e52aeb","input":"### Logistic Regression - Digit example\nAnother famous dataset used as an example for classifications is the digit dataset. Let's load it in the data. If you'd like to read the descriptions of what the data is, you can uncomment the two print statements below:","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"edbaa5","input":"Let's return to the dog example from yesterday:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"fd174b","input":"This means that our model is given by:\n\n$P(\\text{dog is healthy}) = \\frac{e^{4.373-0.0367(\\text{weight})}}{1+e^{4.373-0.0367(\\text{weight})}}$\n\nWhat is the decision boundary separating from healthy and not healthy? Approximately 119 pounds, since \n\n$P(\\text{dog is healthy}) = \\frac{e^{4.373-0.0367(119)}}{1+e^{4.373-0.0367(199)}} \\approx 50\\%$","pos":14,"type":"cell"}
{"id":0,"time":1608092602785,"type":"user"}
{"last_load":1608092603268,"type":"file"}