{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-58fea263-c270-4484-a775-f9c03bca0326.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"eed561","input":"","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"b53f2f","input":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n%matplotlib inline","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"ba6ce5","input":"k_max = k_scores[k_scores.accuracy==max(k_scores.accuracy)]\nk_max","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n    </tr>\n    <tr>\n      <th>k</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>0.84152</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    accuracy\nk           \n18   0.84152"},"exec_count":10,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"a447f2","input":"ax = k_scores.plot()\nax.set(xlabel='Number of Neighbors', ylabel='Accuracy');","output":{"0":{"data":{"image/png":"8cd809de0d9c88272b4a4160f52634e4d1c4863e","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":261,"width":392},"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"d2a8b8","input":"for k in k_max.index.values:\n    # get the model\n\n    model = KNeighborsClassifier(n_neighbors=k)\n\n    # Have to fit on training data again as \n    # cross-validation does not return fitted model\n\n    model = model.fit(X_train, y_train)\n\n    print(model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8157894736842105\n"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"70bf35","input":"from sklearn.model_selection import GridSearchCV\n\n# create a parameter grid: map the parameter names to the values that should be searched\n# Grid search uses all the parameters\n\nparam_grid = {'n_neighbors': range(1, 20)}\n\nmodel = GridSearchCV(KNeighborsClassifier(), \n                    param_grid, \n                    cv=3, \n                    scoring='accuracy')\n\nmodel = model.fit(X_train, y_train)\n\nprint(model.best_params_, model.best_estimator_)","output":{"0":{"name":"stdout","output_type":"stream","text":"{'n_neighbors': 18} KNeighborsClassifier(n_neighbors=18)\n"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"05e17b","input":"model = model.best_estimator_\n\nprint(model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8157894736842105\n"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"848cf6","input":"df.columns","output":{"0":{"data":{"text/plain":"Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n      dtype='object')"},"exec_count":18,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"0ffd16","input":"model.predict([[71, 1, 2, 134, 244, 1, 1, 159, 1, 9, 2, 2, 6]])","output":{"0":{"data":{"text/plain":"array([1])"},"exec_count":19,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"91e0eb","input":"df = pd.read_csv('data/heart.csv')\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n0   63    1   3       145   233    1        0      150      0      2.3      0   \n1   37    1   2       130   250    0        1      187      0      3.5      0   \n2   41    0   1       130   204    0        0      172      0      1.4      2   \n3   56    1   1       120   236    0        1      178      0      0.8      2   \n4   57    0   0       120   354    0        1      163      1      0.6      2   \n\n   ca  thal  target  \n0   0     1       1  \n1   0     2       1  \n2   0     2       1  \n3   0     2       1  \n4   0     2       1  "},"exec_count":2,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"82b1bf","input":"model.predict_proba([[71, 1, 2, 134, 244, 1, 1, 159, 1, 9, 2, 2, 6]])","output":{"0":{"data":{"text/plain":"array([[0.38888889, 0.61111111]])"},"exec_count":20,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"ff954c","input":"X = df.drop(columns = ['target'])\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=0)","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"50376a","input":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"590fd1","input":"\nmodel = KNeighborsClassifier(n_neighbors=1)\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.7894736842105263\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"bf3aa0","input":"model = KNeighborsClassifier(n_neighbors = 5)\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_test, y_test))\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8157894736842105\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"a56a87","input":"for i in range(1,20):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(X_train, y_train)\n\n    print(i, model.score(X_test, y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"1 0.7894736842105263\n2 0.7763157894736842\n3 0.8421052631578947\n4 0.8421052631578947\n5 0.8157894736842105\n6 0.8552631578947368\n7 0.868421052631579\n8 0.868421052631579\n9 0.8552631578947368\n10 0.8552631578947368\n11 0.8421052631578947\n12 0.8421052631578947\n13 0.8289473684210527\n14 0.8289473684210527\n15 0.8289473684210527\n16 0.8157894736842105\n17 0.8289473684210527\n18 0.8157894736842105\n19 0.8157894736842105\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"0515d0","input":"model = KNeighborsClassifier(n_neighbors=5)\nscores = cross_val_score(model, X_train, y_train, \n                         cv=3, scoring='accuracy')\n\nprint(scores)\nprint(scores.mean())","output":{"0":{"name":"stdout","output_type":"stream","text":"[0.76315789 0.84210526 0.82666667]\n0.8106432748538012\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"f2b164","input":"k_scores = []\n\nfor k in range(1, 20):\n\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy')\n    \n    k_scores.append((k, scores.mean()))\n    \nk_scores = pd.DataFrame(k_scores, columns=['k', 'accuracy']).set_index('k')\nk_scores.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n    </tr>\n    <tr>\n      <th>k</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.709415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.771053</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.771053</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.793099</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.810643</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   accuracy\nk          \n1  0.709415\n2  0.771053\n3  0.771053\n4  0.793099\n5  0.810643"},"exec_count":9,"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"markdown","id":"0c08b4","input":"### k-Nearest Neighbors\n\nRun the following cell to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"11ac97","input":"Let's do a test/train:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"462ab9","input":"What about k=5?","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"4c5e5a","input":"Let's run our kNN algorithm with k=1:","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"8e6067","input":"Sources: \n\nhttps://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/\n\nhttps://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7\n\nhttps://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n\nhttps://towardsdatascience.com/implement-k-nearest-neighbors-classification-algorithm-c99be8f14052\n\nhttps://www.geeksforgeeks.org/k-nearest-neighbours/\n\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\n<img src=\"images/k1.png\" width=\"400\">\n\nAs an example, consider the following table of data points containing two features:\n\n<img src=\"images/k2.png\" width=\"400\">\n\nNow, given another set of data points (also called testing data), allocate these points a group by analyzing the training set. Note that the unclassified points are marked as ‘White’.\n\n<img src=\"images/k3.png\" width=\"400\">\n\nIf we plot these points on a graph, we may be able to locate some clusters or groups. Now, given an unclassified point, we can assign it to a group by observing what group its nearest neighbors belong to. This means a point close to a cluster of points classified as ‘Red’ has a higher probability of getting classified as ‘Red’.\n\nIntuitively, we can see that the first point (2.5, 7) should be classified as ‘Green’ and the second point (5.5, 4.5) should be classified as ‘Red’.\n\n***k-Nearest Neighbor algorithm**\n\nLet m be the number of training data samples. Let p be an unknown point.\n\n1. Store the training samples in an array of data points.\n\n2. Calculate the distance between each of the training data points and the point p.\n\n3. Take the k-smallest distances obtained. Each of these distances corresponds to an already classified data point.\n\n4. Return the majority label among among these k points.\n\nK can be kept as an odd number so that we can calculate a clear majority in the case where only two groups are possible (e.g. Red/Blue). \n\n## How do we choose the factor of k?\n\nNotice the boundary for these various k values:\n\n<img src=\"images/k5.png\" width=\"600\">\n\nIf you watch carefully, you can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority.  The training error rate and the validation error rate are two parameters we need to access on different K-value. Following is the curve for the training error rate with varying value of K :\n\n<img src=\"images/k6.png\" width=\"600\">\n\nAs you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:\n\n<img src=\"images/k7.png\" width=\"600\">\n\nThis makes the story more clear. At K=1, we we're overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions.\n\n### Scaling the Data\nFor the kNN algorithm, it is essential that we preprocess the data by scaling it. kNN is a distance-based algorithm and all such distance based algorithms are affected by the scale of the variables. For example, consider the issues inherent in calculating the distance between the points in the graph below, where the $x_1$ values are on a much smaller scale than the $x_2$ values:\n\n<img src=\"images/k10.png\" width=\"500\">\n\nConsider a concrete example in which your data has an age variable which tells about the age of a person in years and an income variable which tells the monthly income of the person in rupees:\n\n<img src=\"images/k8.png\" width=\"300\">\n\nHere the Age of the person ranges from 25 to 40 whereas the income variable ranges from 50,000 to 110,000. The Euclidean distance between observation 1 and 2 will be given as:\n\n$Euclidean Distance = [(100000–80000)^2 + (30–25)^2]^(1/2)$\n\nwhich will come out to be around 20000.000625. \n\nIt can be noted here that the high magnitude of income affected the distance between the two points. This will impact the performance of all distance based model as it will give higher weightage to variables which have higher magnitude (income in this case).\n\n\nWe do not want our algorithm to be affected by the magnitude of these variables. The algorithm should not be biased towards variables with higher magnitude. To overcome this problem, we can bring down all the variables to the same scale. One of the most common technique to do so is normalization where we calculate the mean and standard deviation of the variable. Then for each observation, we subtract the mean and then divide by the standard deviation of that variable:\n\n$x_{\\text{norm}} = \\frac{x - u}{s}$.\n\n\n\nApart from normalization, there are other methods too to bring down all the variables to the same scale. For example: Min-Max Scaling. Here the scaling is done using the following formula:\n\n$x_{\\text{norm}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$.\n\nFor now, we will be focusing on the first type. You can try min-max scaling as well. Let’s see how normalization can bring down these variables to same scale and hence improve the performance of these distance based algorithms. If we normalize the above data, it will look like:\n\n<img src=\"images/k9.png\" width=\"300\">\n\nLet’s again calculate the Euclidean distance between observation 1 and 2:\n$Euclidean Distance = [(0.608+0.260)^2 + (-0.447+1.192)^2]^(1/2)$\nThis time the distance is around 1.1438. We can clearly see that the distance is not biased towards the income variable. It is now giving similar weightage to both the variables. Hence, it is always advisable to bring all the features to the same scale for applying distance based algorithms like KNN or K-Means (which we'll get to next).\n\n\n### Heart Example\n\nWe are going to use a cleaned, reduced dataset of the Cleveland data located here:\n\n\nhttps://archive.ics.uci.edu/ml/datasets/Heart+Disease\n\nOur goal will be to predict the presence of heart disease in a patient or not.\n\n\nThe dataset contains following features:\n- age — age in years \n- sex — (1 = male; 0 = female) \n- cp — chest pain type \n- trestbps — resting blood pressure (in mm Hg on admission to the hospital) \n- chol — serum cholestoral in mg/dl \n- fbs — (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n- restecg — resting electrocardiographic results \n- thalach — maximum heart rate achieved \n- exang — exercise induced angina (1 = yes; 0 = no) \n- oldpeak — ST depression induced by exercise relative to rest \n- slope — the slope of the peak exercise ST segment \n- ca — number of major vessels (0–3) colored by flourosopy \n- thal — 3 = normal; 6 = fixed defect; 7 = reversable defect \n- target — have disease or not (1=yes, 0=no)\n\nLet's load the dataset in:","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"9085ec","input":"It appears that k=7 may be best. However, we've got a problem:\n\n**Problem:** using a single train-test split means that errors from the original test set are used to select the correct value of `n_neighbors`. This is not OK! Performing any part of the modeling process other than evaluation of the final error metric on the test set is overfitting.\n\n**Solution:** create either an additional split of the training set (train_small and validation) or use cross-validation on the training set, which is really the same thing as a train_small/validation split.\n\n## Cross-validation\n\nHere is what cross-validation on the training set looks like for a single value of `n_neighbors`.","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"93d0db","input":"In our case, we may have more than one contender to check. We can measure the final error value on each of these contenders on the test set.","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"9a5f7c","input":"Using the best estimator, we can now calculate the error of the test set:","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"ab4f1b","input":"What about k=1 to k=20?","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"c772b1","input":"We can find the maximum accuracy of the validation set:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"d566b9","input":"**Problem:** can this process be simplified and adapted to search over multiple parameter options?\n\n**Solution:** use grid search with cross-validation.\n\n## Parameter tuning using `GridSearchCV`","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"db7de9","input":"Let's scale the data according to the formula $z = \\frac{x - u}{s}$.\n\nAlso, [here is a good article](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe) about why we use .fit_tranform on the training data but use .transform on the test data:\n\nhttps://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"e114ca","input":"This can be repeated for multiple values of k and the results used to choose the best value of n_neighbors.","pos":16,"type":"cell"}
{"id":0,"time":1618939135109,"type":"user"}
{"last_load":1618939133949,"type":"file"}