{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-7be42454-3410-4948-9549-81d57a72d355.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"d7feab","input":"","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"4b2fcc","input":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\n%matplotlib inline","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"5dd04e","input":"# Load the data in\ndf = pd.read_csv('https://stats.idre.ucla.edu/wp-content/uploads/2016/02/p054.txt', sep=\"\\t\")\ndf.columns = df.columns.str.strip()\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43</td>\n      <td>51</td>\n      <td>30</td>\n      <td>39</td>\n      <td>61</td>\n      <td>92</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63</td>\n      <td>64</td>\n      <td>51</td>\n      <td>54</td>\n      <td>63</td>\n      <td>73</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>71</td>\n      <td>70</td>\n      <td>68</td>\n      <td>69</td>\n      <td>76</td>\n      <td>86</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>61</td>\n      <td>63</td>\n      <td>45</td>\n      <td>47</td>\n      <td>54</td>\n      <td>84</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>81</td>\n      <td>78</td>\n      <td>56</td>\n      <td>66</td>\n      <td>71</td>\n      <td>83</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    Y  X1  X2  X3  X4  X5  X6\n0  43  51  30  39  61  92  45\n1  63  64  51  54  63  73  47\n2  71  70  68  69  76  86  48\n3  61  63  45  47  54  84  35\n4  81  78  56  66  71  83  47"},"exec_count":2,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"42f65f","input":"df.corr()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y</th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Y</th>\n      <td>1.000000</td>\n      <td>0.825418</td>\n      <td>0.426117</td>\n      <td>0.623678</td>\n      <td>0.590139</td>\n      <td>0.156439</td>\n      <td>0.155086</td>\n    </tr>\n    <tr>\n      <th>X1</th>\n      <td>0.825418</td>\n      <td>1.000000</td>\n      <td>0.558288</td>\n      <td>0.596736</td>\n      <td>0.669197</td>\n      <td>0.187714</td>\n      <td>0.224580</td>\n    </tr>\n    <tr>\n      <th>X2</th>\n      <td>0.426117</td>\n      <td>0.558288</td>\n      <td>1.000000</td>\n      <td>0.493331</td>\n      <td>0.445478</td>\n      <td>0.147233</td>\n      <td>0.343293</td>\n    </tr>\n    <tr>\n      <th>X3</th>\n      <td>0.623678</td>\n      <td>0.596736</td>\n      <td>0.493331</td>\n      <td>1.000000</td>\n      <td>0.640314</td>\n      <td>0.115965</td>\n      <td>0.531620</td>\n    </tr>\n    <tr>\n      <th>X4</th>\n      <td>0.590139</td>\n      <td>0.669197</td>\n      <td>0.445478</td>\n      <td>0.640314</td>\n      <td>1.000000</td>\n      <td>0.376883</td>\n      <td>0.574186</td>\n    </tr>\n    <tr>\n      <th>X5</th>\n      <td>0.156439</td>\n      <td>0.187714</td>\n      <td>0.147233</td>\n      <td>0.115965</td>\n      <td>0.376883</td>\n      <td>1.000000</td>\n      <td>0.283343</td>\n    </tr>\n    <tr>\n      <th>X6</th>\n      <td>0.155086</td>\n      <td>0.224580</td>\n      <td>0.343293</td>\n      <td>0.531620</td>\n      <td>0.574186</td>\n      <td>0.283343</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"           Y        X1        X2        X3        X4        X5        X6\nY   1.000000  0.825418  0.426117  0.623678  0.590139  0.156439  0.155086\nX1  0.825418  1.000000  0.558288  0.596736  0.669197  0.187714  0.224580\nX2  0.426117  0.558288  1.000000  0.493331  0.445478  0.147233  0.343293\nX3  0.623678  0.596736  0.493331  1.000000  0.640314  0.115965  0.531620\nX4  0.590139  0.669197  0.445478  0.640314  1.000000  0.376883  0.574186\nX5  0.156439  0.187714  0.147233  0.115965  0.376883  1.000000  0.283343\nX6  0.155086  0.224580  0.343293  0.531620  0.574186  0.283343  1.000000"},"exec_count":3,"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"e534cc","input":"sns.pairplot(df, height = 1.2, aspect=1.5)","output":{"0":{"data":{"text/plain":"<seaborn.axisgrid.PairGrid at 0x7fbfbc6b4760>"},"exec_count":4,"output_type":"execute_result"},"1":{"data":{"image/png":"3b7c70df940dcf474a57d2da86eaabf778c053b9","text/plain":"<Figure size 907.2x604.8 with 56 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":596,"width":893},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"49a1eb","input":"# Create an empty model\nmodel = LinearRegression()\n# Choose the predictor variables, here all but the first which is the response variable\n# This model is analogous to the Y ~ X1 + X2 + X3 + X4 + X5 + X6 model\nX = df.iloc[:, 1:]\n# Choose the response variable(s)\ny = df.iloc[:, 0]\n# Fit the model to the full dataset\nmodel.fit(X, y)\n# Print out the R^2 for the model against the full dataset\nprint(f\"R^2: {model.score(X,y)}\")\nprint(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"R^2: 0.7326019925311491\nAdjusted R^2: 0.6628459905827533\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"ccaee4","input":"df.corr()['Y'].sort_values(ascending=False)","output":{"0":{"data":{"text/plain":"Y     1.000000\nX1    0.825418\nX3    0.623678\nX4    0.590139\nX2    0.426117\nX5    0.156439\nX6    0.155086\nName: Y, dtype: float64"},"exec_count":5,"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"bfd1b5","input":"df = df.drop(columns = ['X5', 'X6'])\n\n# Create an empty model\nmodel = LinearRegression()\n# Choose the predictor variables, here all but the first which is the response variable\n# This model is analogous to the Y ~ X1 + X2 + X3 + X4 model\nX = df.iloc[:, 1:]\n# Choose the response variable(s)\ny = df.iloc[:, 0]\n# Fit the model to the full dataset\nmodel.fit(X, y)\n# Print out the R^2 for the model against the full dataset\nprint(f\"R^2: {model.score(X,y)}\")\nprint(f\"Adjusted R^2: {1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)}\")","output":{"0":{"name":"stdout","output_type":"stream","text":"R^2: 0.7152237146514394\nAdjusted R^2: 0.6696595089956696\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"8269be","input":"# print out intercept\nprint(model.intercept_)\n# print out other coefficients\nprint(model.coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"11.833542828302207\n[ 0.69114872 -0.10288557  0.24633061 -0.02551155]\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"f74f7c","input":"model.predict([[1,1,1,1]])","output":{"0":{"data":{"text/plain":"array([12.64262504])"},"exec_count":8,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"markdown","id":"2f8bf0","input":"Let's run a linear regression model:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"3d6ea5","input":"Run the following code to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"54c89f","input":"This means that our model is given by:\n\n$\\hat{y} = 11.833 + 0.691x_1 - 0.103 x_2 + 0.246 x_3 - 0.026 x_4$","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"5faad5","input":"We will use this [simple survey data](http://www.ats.ucla.edu/stat/examples/chp/p054.txt) to demonstrate a few basic features of ***seaborn*** and how it might be used for regression.\n\nWe will use this [simple survey data](https://stats.idre.ucla.edu/wp-content/uploads/2016/02/p054.txt) to demonstrate a few basic features of ***seaborn*** and how it might be used for regression.\n\nThe dataset is simply the results of a survey where the question responses are all numeric.  This leads to 6 numeric independent variable (predictor) fields and 1 numeric dependent variable (response) field.  The predictors are labeled ***X<sub>i</sub>*** and the response is labeled ***Y***.\n\nLet's load the dataset in using ***pandas*** and take a look at it.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"6211e4","input":"Remember that we also need to make sure that there is a linear relationship between the feature and the target variable, or else we might need to perform a transformation. Let's look at the shape of the scatterplots here:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"6e1ffc","input":"Notice in the sorted list above that X5 and X6 had very low correlations with price (0.156439 and 0.155086). Let's see what happens to our adjusted R^2 if we drop those features and rerun our linear model. We'll find that our adjusted R^2 actually increases slightly!","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"6f3a89","input":"So we now have a linear model, but what exactly is it? Let's print out the coefficients:","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"84a852","input":"What would we predict y to be if $x_1=x_2=x_3=x_4=1$? Approximately 12.6.","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"a66d6e","input":"We see that the data has 30 responses with 7 fields (6 independent, 1 dependent) each. Let's use pandas to check out the correlations between the different variables.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b5e680","input":"We notice that some of the variables (such as X1 and X3) are highly correlated.  This is something we might want to take into consideration later.  When 2 predictor variables are highly correlated this is called [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) and it is something we want to watch out for as it can destabilize our model.  In the extreme case, when 2 predictors are perfectly correlated then there is absolutely nothing gained by making both variables part of our regression.\n\nAnother thing we notice is that some variables are highly correlated with our target variable. We see that X1 and Y are highly correlated (r=0.825) whereas X6 and Y are less so (0.155). If we can only choose a few features to add to our model, we will probably prefer to choose those with the highest correlation. \n\nWe can view the highest correlations by sorting them:","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"cee612","input":"### Adjusted R-Squared\nRecall that $R^2$is the square of the correlation coefficient and represents the estimated percentage of the variance in our target variable ***Y*** that can be explained by our regression model. The only drawback of $R^2$ is that if new predictors (X) are added to our model, $R^2$ only increases or remains constant but it never decreases. We can not judge that by increasing complexity of our model, are we making it more accurate?\n\nThat is why we use “Adjusted R-Square”.\n\nThe Adjusted R-Square is the modified form of R-Square that has been adjusted for the number of predictors in the model. The adjusted R-Square only increases if the new term improves the model accuracy. ***Adjusted R<sup>2</sup>*** also penalizes for things such as large coefficients and extra variables to try and limit ***overfitting*** so it is often a better measure of model efficacy.  Here is the formula, where p is the number of predictors and N is the number of data points:\n\n$R^2  \\text{adjusted}=1-\\frac{(1-R^2)(N-1)}{N-p-1}$","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"e406be","input":"### Feature Selection\nIn our previous case, there was only one independent variable. If there is more than one, we might use multiple linear regression.","pos":2,"type":"cell"}
{"id":0,"time":1615933706263,"type":"user"}
{"last_load":1615933705505,"type":"file"}