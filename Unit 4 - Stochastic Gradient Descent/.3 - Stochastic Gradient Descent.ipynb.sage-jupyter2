{"attachments":{},"cell_type":"markdown","id":"62af94","input":"Source:\nhttps://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1\n\nRecall our standard Gradient Descent (aka batch) algorithm:\n![image.png](images/sgd1.png)\nIn the above algorithm says, to perform the GD, we need to calculate the gradient of the cost function J. And to calculate the gradient of the cost function, we need to sum (yellow circle!) the cost of each sample. If we have 3 million samples, we have to loop through 3 million times or use the dot product.\n\nAnother downside of standard gradient descent is you have no guarantee that you will converge to the absolute minimum.\n\nAn algorithm that will be faster (and has a higher likelihood of converging to the true minimum) is stochastic gradient descent, in which we introduce some randomness:\n![image.png](images/sgd2.png)\nBasically, in SGD, we are using the cost gradient of 1 example at each iteration, instead of using the sum of the cost gradient of ALL examples.\n\nA few things to note:\n\na) In SGD, before for-looping, you need to randomly shuffle the training examples.\n\nb) In SGD, because it’s using only one example at a time, its path to the minima is noisier (more random) than that of the batch gradient. But it’s ok as we are indifferent to the path, as long as it gives us the minimum AND the shorter training time. Here’s a picture to view the difference:\n![image.png](images/sgd3.png)\nOr in 3 D:![image.png](images/sgd4.png)\n\nc) Mini-batch gradient descent uses n data points (instead of 1 sample in SGD) at each iteration.\n\nSome pandas notes to help:\n1.\tTo shuffle the dataframe and reset the index:\n\n```\ndf = df.sample(frac=1).reset_index(drop=True)\n```\n\n2.\tTo randomly select one row from an entire dataframe, use:\n\n```\ndf.sample() \n```\n\nOr, to randomly select an integer between 0 and n, inclusive, put “import random” in your list of important packages and then type: \n```\nrandom.randint(0,n)\n```\n\n### Exercise 1:\nGet the SGD algorithm working. To test it, an input of sgd(X,Y,.01,0.00001) gave me out an output of b:86.84622507560407, m:-1.9109285548469945, (although yours will be different due to the randomization).","pos":0,"type":"cell"}
{"backend_state":"init","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-d1b1b71d-5e6c-4842-b637-af7f1dcb4099.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"f30a18","input":"","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"caf5f6","input":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\nimport random\n\ndf = pd.read_csv('data/airlines.csv')\n\nX = df['Mishandled baggage (per 1000 passengers)']\nY = df['Percentage On Time Arrivals']","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":36,"id":"a362f3","input":"# talked to frances about this\ndef sgd_alt(X, Y, alpha, tol):\n    theta0 = 0\n    theta1 = 0\n    tempt0 = 2\n    tempt1 = 2\n    # making an alternative dataframe to shuffle\n    d = pd.DataFrame({'mishandled bags': X, 'on time arrivals': Y})\n    while True:  # assuming i will eventually reach a place of really small change\n        d = d.sample(frac=1).reset_index(drop=True)\n        X, Y = d['mishandled bags'], d['on time arrivals']\n        for _ in range(len(X)):\n            tempt0, tempt1 = theta0, theta1\n            partial0 = ((theta1*X[_]+theta0)-Y[_])\n            partial1 = ((theta1*X[_]+theta0)-Y[_])*X[_]\n            theta0 = theta0 - alpha*partial0\n            theta1 = theta1 - alpha*partial1\n            if ((tempt0-theta0)**2 + (tempt1-theta1)**2)**0.5 < tol:\n                return theta0, theta1\n\n\nsgd_alt(X, Y, 0.01, 0.00001)","output":{"0":{"data":{"text/plain":"(86.96535373867778, -2.100287856146256)"},"exec_count":36,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"934841","input":"d = random.randint(0, len(X)-1)","pos":2,"type":"cell"}
{"cell_type":"code","exec_count":48,"id":"ecc8b2","input":"#insert\ndef gd_graph(X, Y, alpha, tol):\n    theta0 = 0\n    theta1 = 0\n    tempt0 = 2\n    tempt1 = 2\n    t0, t1 = [], []\n    j = 0\n    while ((tempt0-theta0)**2 + (tempt1-theta1)**2)**0.5 > tol:\n        j += 1\n        tempt0, tempt1 = theta0, theta1\n        partial0 = sum((theta1*X+theta0)-Y)/len(X)  # partial 0\n        partial1 = sum(((theta1*X+theta0)-Y)*X)/len(X)\n        theta0 = theta0 - alpha*partial0\n        theta1 = theta1 - alpha*partial1\n        if not j%100:\n            t0.append(theta0)\n            t1.append(theta1)\n    return t0, t1\n\nx, y = gd_grapher(X, Y, 0.01, 0.00001)\nplt.title('GD 2D progression')\nplt.xlabel('Theta 0')\nplt.ylabel('Theta 1')\nplt.plot(x, y, '.')","output":{"0":{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f8f8ee78310>]"},"exec_count":48,"output_type":"execute_result"},"1":{"data":{"image/png":"1ce24396bae548c2d1a71259455b05ac4a5554e7","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":48,"metadata":{"image/png":{"height":277,"width":388},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":49,"id":"9568f6","input":"def sgd_graph(X, Y, alpha, tol):\n    theta0 = 0\n    theta1 = 0\n    tempt0 = 2\n    tempt1 = 2\n    t0, t1 = [], []\n    j = 0\n    while ((tempt0-theta0)**2 + (tempt1-theta1)**2)**0.5 > tol:\n        j += 1\n        d = random.randint(0, len(X)-1)\n        tempt0, tempt1 = theta0, theta1\n        partial0 = ((theta1*X[d]+theta0)-Y[d])\n        partial1 = ((theta1*X[d]+theta0)-Y[d])*X[d]\n        theta0 = theta0 - alpha*partial0\n        theta1 = theta1 - alpha*partial1\n        if not j%100:\n            t0.append(theta0)\n            t1.append(theta1)\n    return t0, t1\n\nx, y = sgd_grapher(X, Y , 0.01 ,0.00001)\nplt.title('SGD 2D progression')\nplt.xlabel('Theta 0')\nplt.ylabel('Theta 1')\nplt.plot(x, y, '.')","output":{"0":{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f8f8ee559d0>]"},"exec_count":49,"output_type":"execute_result"},"1":{"data":{"image/png":"72189da31c25e23846e73f0c69745ac29606e624","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":49,"metadata":{"image/png":{"height":277,"width":384},"needs_background":"light"},"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"7c3283","input":"#insert\ndf = df.sample(frac=1).reset_index(drop=True)\n\ndef sgd(X, Y, alpha, tol):\n    theta0 = 0\n    theta1 = 0\n    tempt0 = 2\n    tempt1 = 2\n    while ((tempt0-theta0)**2 + (tempt1-theta1)**2)**0.5 > tol:\n        d = random.randint(0, len(X)-1)\n        tempt0, tempt1 = theta0, theta1\n        partial0 = ((theta1*X[d]+theta0)-Y[d])\n        partial1 = ((theta1*X[d]+theta0)-Y[d])*X[d]\n        theta0 = theta0 - alpha*partial0\n        theta1 = theta1 - alpha*partial1\n    return theta0, theta1\n\n\nsgd(X, Y, 0.01, 0.00001)","output":{"0":{"data":{"text/plain":"(87.20667095674945, -2.1702665637903773)"},"exec_count":5,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"619732","input":"\n### Exercise 2: \nOnce you have the algorithm working, create a 2D (alpha0, alpha1) plot of both the GD and SGD algorithms. You will see the SGD algorithm take a more meandering path. To speed up the plotting, you will probably only want to plot every 100th point. (You can do this by creating a separate j counting variable and only plot the point if j % 100 == 0.","pos":5,"type":"cell"}
{"id":0,"time":1614285730607,"type":"user"}
{"last_load":1614285729409,"type":"file"}