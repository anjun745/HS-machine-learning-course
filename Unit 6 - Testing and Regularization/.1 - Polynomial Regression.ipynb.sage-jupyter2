{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":1,"id":"4a26c1","input":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"2823ad","input":"X = np.arange(6).reshape(3, 2)\nX","output":{"0":{"data":{"text/plain":"array([[0, 1],\n       [2, 3],\n       [4, 5]])"},"exec_count":10,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"1e08e9","input":"poly = PolynomialFeatures(2)\npoly.fit_transform(X)","output":{"0":{"data":{"text/plain":"array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])"},"exec_count":11,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"38d2b0","input":"model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n\n#fit the model and use it to predict y\nmodel.fit(x, y)\ny_poly_pred = model.predict(x)\nprint(model.score(x,y))\n\n#plot stuff\n#sort the values of x before line plot or else you get zigzags\nplt.scatter(x,y)\nsorted_zip = sorted(zip(x,y_poly_pred))\nx_plot, y_poly_pred = zip(*sorted_zip)\nplt.plot(x_plot, y_poly_pred, color='r')\nplt.xlabel('x')\nplt.ylabel('y')","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8537647164420812\n"},"1":{"data":{"text/plain":"Text(0, 0.5, 'y')"},"exec_count":12,"output_type":"execute_result"},"2":{"data":{"image/png":"7dac7b7ee92f5c281936599195a9c7508bfe4d6d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"335061","input":"model","output":{"0":{"data":{"text/plain":"Pipeline(memory=None,\n     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False))])"},"exec_count":13,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"aabc19","input":"model.steps[1]","output":{"0":{"data":{"text/plain":"('linearregression',\n LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n          normalize=False))"},"exec_count":14,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"41fc41","input":"model.steps[1][1]","output":{"0":{"data":{"text/plain":"LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False)"},"exec_count":15,"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"0c181c","input":"print(model.steps[1][1].intercept_)\nprint(model.steps[1][1].coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"-6.119739594096467\n[ 0.          8.48492679 -1.62853134]\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"3f1417","input":"model = make_pipeline(PolynomialFeatures(2), \n                      StandardScaler(),\n                      LinearRegression())\n\n#fit the model and use it to predict y\nmodel.fit(x, y)\ny_poly_pred = model.predict(x)\nprint(model.score(x,y))\n\n#plot stuff\n#sort the values of x before line plot or else you get zigzags\nplt.scatter(x,y)\nsorted_zip = sorted(zip(x,y_poly_pred))\nx_plot, y_poly_pred = zip(*sorted_zip)\nplt.plot(x_plot, y_poly_pred, color='r')\nplt.xlabel('x')\nplt.ylabel('y')","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8537647164420812\n"},"1":{"data":{"text/plain":"Text(0, 0.5, 'y')"},"exec_count":17,"output_type":"execute_result"},"2":{"data":{"image/png":"7dac7b7ee92f5c281936599195a9c7508bfe4d6d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"111098","input":"print(model)","output":{"0":{"name":"stdout","output_type":"stream","text":"Pipeline(memory=None,\n     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n         normalize=False))])\n"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"5ea566","input":"print(model.steps[1][1].fit_transform(x))","output":{"0":{"name":"stdout","output_type":"stream","text":"[[-1.40524809]\n [ 0.19898942]\n [-0.48154749]\n [-1.96611671]\n [-1.52699325]\n [ 1.81915286]\n [-0.44784936]\n [ 0.8476904 ]\n [ 0.79106922]\n [ 0.18670819]\n [ 0.5002348 ]\n [-1.04088075]\n [-0.2254846 ]\n [ 0.52654509]\n [ 0.14758163]\n [ 0.27718776]\n [-1.08770076]\n [ 0.91097215]\n [ 0.30142564]\n [ 1.67426385]]\n"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"7c39b5","input":"np.random.seed(0)\nx = 2 - 3 * np.random.normal(0, 1, 20)\ny = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\nplt.plot(x,y, '.');","output":{"0":{"data":{"image/png":"715fba98d3b0213cf91e34e98ad0d356ea4c4f45","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"6f2f0a","input":"# We start by seeding the random number generator so that everyone will have the same \"random\" results\nnp.random.seed(9)\n\n# Function that returns the sin(2*pi*x)\ndef f(x):\n    return np.sin(2 * np.pi * x)\n\n# This returns 100 evenly spaced numbers from 0 to 1\nX_plot = np.linspace(0, 1, 100)\n\n#applying model.predict(X) needs the X to be a matrix so the line below adds another dimension\nX = X_plot[:, np.newaxis]\n\n# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\ny = f(X_plot) + np.random.normal(0,.3,100)\n\n\n# Plot the training data against what we know to be the ground truth sin function\nfig,ax = plt.subplots(1,1)\nax.plot(X_plot, f(X_plot), label='ground truth', color='green')\nax.scatter(X_plot, y, label='data')\nax.set_ylabel('y')\nax.set_xlabel('x')\nax.legend()","output":{"0":{"data":{"text/plain":"<matplotlib.legend.Legend at 0x1a238fc668>"},"exec_count":20,"output_type":"execute_result"},"1":{"data":{"image/png":"56a13e8f7388ee03e774be25e5809aa3d4824ccf","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"140752","input":"# Plot the results of a pipeline against ground truth and actual data\ndef plot_approximation(model, ax, x_plot, y, label=None):\n    \"\"\"Plot the approximation of ``model`` on axis ``ax``. \"\"\"\n    ax.plot(x_plot, f(x_plot), 'g', label=\"ground truth\")\n    ax.scatter(x_plot, y, s=10)\n    x = x_plot[:, np.newaxis]\n    ax.plot(x_plot, model.predict(x), color='red', label=label)\n    ax.set_ylim((-2, 2))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='upper right')","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"e79f60","input":"# Set up the plot\nfig,ax = plt.subplots(1,1)\n\n# Set the degree of our polynomial and fit the model\ndegree = 3\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\nmodel.fit(X, y)\n\nplot_approximation(model, ax, X_plot, y, label='degree=%d' % degree)","output":{"0":{"data":{"image/png":"ab74f8f092eca5fb8fca46249f7605feee7b3f8b","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"e1df8a","input":"# Set up the plot\nfig,ax = plt.subplots(1,1)\n\n# Set the degree of our polynomial and fit the model\ndegree = 2\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\nmodel.fit(X, y)\n\nplot_approximation(model, ax, X_plot, y, label='degree=%d' % degree)","output":{"0":{"data":{"image/png":"1e43583f7e04a97ac8282d001d803ede218ec99d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":46,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"77eeb9","input":"# Set up the plot\nfig,ax = plt.subplots(1,1)\n\n# Set the degree of our polynomial and fit the model\ndegree = 27\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\nmodel.fit(X, y)\n\nplot_approximation(model, ax, X_plot, y, label='degree=%d' % degree)","output":{"0":{"data":{"image/png":"232d7dd0fd49551db5fd3006e85eef57f5eae902","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":48,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"81d73a","input":"#slit the data into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)\n\n#set up four subplots\nfig, axes = plt.subplots(2, 2, figsize=(8, 5))\n\n#fit different polynomials and plot approximations\nfor ax, degree in zip(axes.ravel(), [0, 1, 3, 9]):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    print(\"degree: \", degree, \"    R^2:\", model.score(X_test, y_test))\n    plot_approximation(model, ax, X_plot, y, label='degree=%d' % degree)\n    ","output":{"0":{"name":"stdout","output_type":"stream","text":"degree:  0     R^2: -0.0012117357594156886\ndegree:  1     R^2: 0.3888798353915932\ndegree:  3     R^2: 0.797568546442723\ndegree:  9     R^2: 0.5048513992003683\n"},"1":{"data":{"image/png":"ba4526bce1f52d8df554383be58caee5f87793c0","text/plain":"<Figure size 576x360 with 4 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":51,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"7c5da0","input":"degrees = 10\n\ntrain_error = []\ntest_error = []\n\nfor degree in range(degrees):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X_train, y_train)\n    train_error.append(mean_squared_error(y_train, model.predict(X_train)))\n    test_error.append(mean_squared_error(y_test, model.predict(X_test)))\n\n# Plot the training and test errors against degree\nplt.figure(figsize=(8,6))\nplt.plot(np.arange(degrees), train_error, color='green', label='train')\nplt.plot(np.arange(degrees), test_error, color='red', label='test')\nplt.ylim((0.0, 1e0))\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')","output":{"0":{"data":{"text/plain":"<matplotlib.legend.Legend at 0x1a23de4710>"},"exec_count":26,"output_type":"execute_result"},"1":{"data":{"image/png":"709ff63183dc6090e8618d2509a969479b59dd1a","text/plain":"<Figure size 576x432 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":53,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"bc1b37","input":"np.random.seed(0)\nx = 2 - 3 * np.random.normal(0, 1, 20)\ny = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n\n# transforming the data to include another axis because .fit needs x to be a matrix\nx = x[:, np.newaxis]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_pred = model.predict(x)\n\nplt.scatter(x, y, s=10)\nplt.plot(x, y_pred, color='r')","output":{"0":{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x1a23516940>]"},"exec_count":3,"output_type":"execute_result"},"1":{"data":{"image/png":"9f620c723b08c4ed6a40971db852f5d20bc0b777","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"1e75ec","input":"print(\"R^2:\", model.score(x,y))","output":{"0":{"name":"stdout","output_type":"stream","text":"R^2: 0.6386750054827147\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"6410d2","input":"#get polynomial features\npolynomial_features= PolynomialFeatures(degree=2)\nx_poly = polynomial_features.fit_transform(x)\n\n#apply linear model\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"e771e9","input":"print(model.score(x_poly,y))","output":{"0":{"name":"stdout","output_type":"stream","text":"0.8537647164420812\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"9f5cd7","input":"poly = PolynomialFeatures(2)\nx_poly = poly.fit_transform(x)\nprint('original value', x[0])\nprint('transformed value', x_poly[0])","output":{"0":{"name":"stdout","output_type":"stream","text":"original value [-3.29215704]\ntransformed value [ 1.         -3.29215704 10.83829796]\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"80d282","input":"poly = PolynomialFeatures(2)\nx_poly = poly.fit_transform(x)\nprint(x_poly)","output":{"0":{"name":"stdout","output_type":"stream","text":"[[ 1.         -3.29215704 10.83829796]\n [ 1.          0.79952837  0.63924562]\n [ 1.         -0.93621395  0.87649656]\n [ 1.         -4.7226796  22.30370258]\n [ 1.         -3.60267397 12.97925974]\n [ 1.          4.93183364 24.32298305]\n [ 1.         -0.85026525  0.722951  ]\n [ 1.          2.45407162  6.02246754]\n [ 1.          2.30965656  5.3345134 ]\n [ 1.          0.76820449  0.59013814]\n [ 1.          1.56786929  2.4582141 ]\n [ 1.         -2.36282052  5.58292081]\n [ 1.         -0.28311318  0.08015307]\n [ 1.          1.63497495  2.67314309]\n [ 1.          0.6684103   0.44677233]\n [ 1.          0.99897702  0.99795508]\n [ 1.         -2.48223722  6.16150161]\n [ 1.          2.61547479  6.84070838]\n [ 1.          1.0607969   1.12529005]\n [ 1.          4.56228722 20.81446466]]\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":83,"id":"918b82","input":"df = pd.DataFrame(data={'X':X_plot, 'y':y})\nplt.plot(df['X'], df['y'], '.')","output":{"0":{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x1a28ad86a0>]"},"exec_count":83,"output_type":"execute_result"},"1":{"data":{"image/png":"a11eeeb01d8e99de5153775f34fdc490adf3b0c2","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":56,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"efae4b","input":"plt.scatter(x,y)\n\n#sort the values of x before line plot or else you get zigzags\nsorted_zip = sorted(zip(x,y_poly_pred))\nx_plot, y_poly_pred = zip(*sorted_zip)\nplt.plot(x_plot, y_poly_pred, color='r')\nplt.xlabel('x')\nplt.ylabel('y')","output":{"0":{"data":{"text/plain":"Text(0, 0.5, 'y')"},"exec_count":9,"output_type":"execute_result"},"1":{"data":{"image/png":"7dac7b7ee92f5c281936599195a9c7508bfe4d6d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":17,"type":"cell"}
{"cell_type":"code","id":"a622a6","input":"#insert work here","pos":57,"type":"cell"}
{"cell_type":"markdown","id":"07fd25","input":"Alright, well if the higher degree the better, why not plot degree 27?","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"0897c0","input":"### Homework\n\nLet's read in this mystery dataset. Find an optimal polynomial for fitting the data. Write the equation of the polynomial.","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"13d1d6","input":"Now that we have our data and know the ground truth, let's try fitting a 3rd degree polynomial to our training data and see how it looks.  3rd degree makes sense for this interval because the sin function has 2 turning points over the interval [0,1] and a 3rd degree polynomial will have 2 (or less) turning points.\n\nWe first define a function `plot_approximation` that takes a pipeline of steps from make_pipeline and some plotting info and will plot the results of the sklearn pipeline on the specified plot with the ground truth and data in the background.","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"162462","input":"What is PolynomialFeatures actually doing? Generating a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if x=3 is an input value in the original data set, then poly.fit_transform(x) transforms this value into the array [1 3 9], corresponding to $\\theta_0+\\theta_1(3)+\\theta_2(3^2)$. As another example, compare the first x-value in our data set with the first transformed value:","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"24e23b","input":"We can see the three steps of our pipeline here:","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"43b271","input":"### Fitting an nth-degree polynomial\nLet us do another example. We'll generate points on a sine curve:","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"4a139c","input":"To overcome under-fitting, we need to increase the complexity of the model.\nTo generate a higher order equation we can add powers of the original features as new features. The linear model, $Y=\\theta_0+\\theta_1 x$ can be transformed to:\n\n$Y=\\theta_0+\\theta_1 x+\\theta_2 x^2$.\n\n**This is still considered to be linear model as the coefficients/weights $\\theta_i$ associated with the features are still linear. $x^2$ is only a feature. However, the curve that we are fitting is quadratic in nature.**\n\nTo convert the original features into their higher order terms we will use the PolynomialFeatures class provided by scikit-learn. Next, we train the model using Linear Regression.","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"4ba17f","input":"You ignore the first 0 coefficient contained in the coef list to find that the model is:\n\n$y_{\\text{predicted}} = -12.75602135 + 4.85482296 x + -0.46076114 x^2$","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"5054d3","input":"Source: https://towardsdatascience.com/polynomial-regression-bbe8b9d97491\n### Polynomial Regression\nTo understand the need for polynomial regression, let’s generate some random dataset first.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"5a8257","input":"If you wanted to access the y-intercept and coefficients contained in the linear regression model, you would first need to access the second tuple contained in steps:","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"71e88b","input":"### Pipelines\n\nFor this dataset, we needed to first obtain polynomial features and then apply a linear regression model. When several steps are needed, it is good practice to create a pipeline:","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"773a48","input":"Now let's generate our pipeline for a 3rd degree polynomial and try it out in our plotting function.  Note that the steps are:\n- Use PolynomialFeatures(3) to create a generator of 3rd degree polynomials\n- Feed this generator to make_pipeline along with a LinearRegression object to tell it to string together these operations when given a new set of input predictor variables.  This results in a new model object that has the same `fit()`, `score()`, `predict()`, etc functions\n- Call `fit()` on our new object to fit a 3rd degree polynomial regression\n- Send the result to our plotting function to view the results","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"8b8406","input":"Similarly, the entire dataset of transformed input is:","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"8d2bd9","input":"Import the following packages:","pos":0,"slide":"slide","type":"cell"}
{"cell_type":"markdown","id":"8e7766","input":"Fitting a **linear** regression model on the transformed features gives the below plot.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"998823","input":"Our R^2 goes up considerably:","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"9a484b","input":"### Another polynomial regression example\n\nThe above model only contained a single predictor variable, x. What if there were two predictor variables, $a$ and $b$? Then the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. As an example, consider the input:","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"9eb832","input":"If we wanted to get the scaled feature data, we would need to get it from the second step (the StandardScaler step):","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"a2708d","input":"We can see that the straight line is unable to capture the patterns in the data. This is an example of underfitting. Computing the R²-score of the linear line gives:","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"a6e71b","input":"Then the polynomial features are given by:","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"b36eb1","input":"Applying a linear model:","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"b4cd2d","input":"(Source: https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/)\n\n\nHigher degree polynomials will fit the training data better, but the testing data worse. This problem is called as over-fitting. We also say that an overfit the model has high variance and low bias. Similarly, we have another problem called underfitting, it occurs when our model neither fits the training data nor generalizes on the new data. Our model is underfit when we have high bias and low variance. We can view the various situations below:\n\n![title](images/reg1.png)\n\n![title](images/reg2.png)\n\n**Bias** refers to the error due to the model’s simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in under-fitting.\n\n**Variance** refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in over-fitting the data.\n\nWhat does that bias and variance actually mean? Let us understand this by an example of archery targets.\n\n<img src=\"images/reg4.png\" width=\"400\">\n\nLet’s say we have model which is very accurate, therefore the error of our model will be low, meaning a low bias and low variance as shown in first figure. All the data points fit within the bulls-eye. Similarly we can say that if the variance increases, the spread of our data point increases which results in less accurate prediction. And as the bias increases the error between our predicted value and the observed values increases.\n\nNow how this bias and variance is balanced to have a perfect model? Take a look at the image below and try to understand.\n\n<img src=\"images/reg3.png\" width=\"400\">","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"bb705d","input":"You could then print the intercept and coefficients:","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"cf8914","input":"You would then access the second item of the tuple (the first is the label \"linear regression\" and the second is the actual model):","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"d175dc","input":"Now, view a plot of the error as a function of polynomial degree for degrees 1-9:","pos":52,"type":"cell"}
{"cell_type":"markdown","id":"d38901","input":"Let's plot a fit of degree 2 to verify that it gets worse:","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"d5e641","input":"Let's understand a bit more how you can access specifics of the pipeline. First, print the model and notice that there are two items contained in the list steps. The first is a tuple containing the PolynomialFeatures method called \"polynomialfeatures\" and the second is a tuple containing the LinearRegression model called \"linear regression\":","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"d5f324","input":"Let's view how a higher degree polynomial will fit the training data better but the testing data worse. First, view the fits on the training data:","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"e28129","input":"**What do you notice?**\n\nThe higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n\nThis is a visual demonstration of ***overfitting***: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error).\n\nAnother way of thinking about this is that the train and test errors should not be too different. If your training error is much smaller than your test error than you might be overfitting.\n\nIn the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE (mean squared error).","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"e3af34","input":"Actually, remember that we should also employ feature scaling using StandardScaler. Then, our pipeline will contain three steps:","pos":33,"type":"cell"}
{"id":0,"time":1614704130975,"type":"user"}
{"last_load":1614704132604,"type":"file"}