{"backend_state":"running","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-af2c0096-71da-40b6-9f51-82a15b657085.json","kernel":"python3","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1617773383499,"exec_count":1,"id":"8263d1","input":"from itertools import product\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_digits","kernel":"python3","pos":1,"start":1617773380398,"state":"done","type":"cell"}
{"cell_type":"code","end":1617773386839,"exec_count":2,"id":"98843f","input":"df = pd.read_csv(\"data/dogweights.csv\")\ndf.head()","kernel":"python3","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weight</th>\n      <th>health status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>52</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   weight  health status\n0      50              1\n1      50              1\n2      52              1\n3      50              1\n4      48              0"},"exec_count":2}},"pos":6,"start":1617773386825,"state":"done","type":"cell"}
{"cell_type":"code","end":1617773389628,"exec_count":3,"id":"6195c0","input":"X = df.drop(columns = [\"health status\"])\ny = df[\"health status\"]\n\nmodel = LogisticRegression(solver=\"lbfgs\")\nmodel.fit(X, y)\nprint(model.score(X,y))","kernel":"python3","output":{"0":{"name":"stdout","text":"0.85\n"}},"pos":8,"start":1617773389611,"state":"done","type":"cell"}
{"cell_type":"code","end":1617773392461,"exec_count":4,"id":"ade5b1","input":"print(model.intercept_)\nprint(model.coef_)","kernel":"python3","output":{"0":{"name":"stdout","text":"[4.3727885]\n[[-0.03673668]]\n"}},"pos":10,"start":1617773392451,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":5,"id":"6e9f4d","input":"#insert HW\n\n\n\ndef h(x, theta0, theta1):\n    return (1/(1+e**(-theta0-theta1*x)))\n\n\ndef gradient_descent(X, Y, alpha=5e-5, num_steps=100000):\n    theta0 = 0\n    theta1 = 0\n    tempt0 = 2\n    tempt1 = 2\n    for i in range(num_steps):\n        tempt0, tempt1 = theta0, theta1\n        partial0 = sum((theta1*X+theta0)-Y)/len(X)  # partial 0\n        partial1 = sum(((theta1*X+theta0)-Y)*X)/len(X)\n        theta0 = theta0 - alpha*partial0\n        theta1 = theta1 - alpha*partial1\n    return theta0, theta1","kernel":"python3","pos":12,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":8,"id":"13d208","input":"def h(scores):\n    return 1 / (1 + np.exp(-scores))\n\ndef logistic_regression(features, target, alpha, num_steps):\n    intercept = np.ones((features.shape[0], 1))\n    features = np.hstack((intercept, features))\n        \n    thetas = np.zeros(features.shape[1])\n    \n    for step in range(num_steps):\n        scores = np.dot(features, thetas)\n        predictions = h(scores)\n        \n        # Update weights with gradient\n        output_error_signal = predictions - target\n        gradient = np.dot(features.T, output_error_signal)\n        thetas -= alpha * gradient\n\n        if step % 1000 == 0:\n            print(step, thetas)\n            \n    print(thetas)\n        \n    return thetas\n\nX = df[\"weight\"]\ny = df[\"health status\"]\nX = np.array(X)[:, np.newaxis]\n\nlogistic_regression(X, y, alpha = 5e-5, num_steps = 100000)","output":{"0":{"name":"stdout","output_type":"stream","text":"0 [0.00025 0.01325]\n1000 [0.10660995 0.00616572]\n2000 [0.20960972 0.00504867]\n3000 [0.30929341 0.00397253]\n4000 [0.40577339 0.00293557]\n5000 [0.49916157 0.00193607]\n6000 [0.58956871 0.00097238]\n7000 [6.77103866e-01 4.29297435e-05]\n8000 [ 0.76187387 -0.00085383]\n9000 [ 0.84398299 -0.00171935]\n10000 [ 0.92353264 -0.00255503]\n11000 [ 1.00062115 -0.00336223]\n12000 [ 1.07534363 -0.00414221]\n13000 [ 1.14779184 -0.00489619]\n14000 [ 1.2180542  -0.00562533]\n15000 [ 1.28621571 -0.00633072]\n16000 [ 1.35235799 -0.00701342]\n17000 [ 1.41655937 -0.00767442]\n18000 [ 1.47889486 -0.00831465]\n19000 [ 1.53943632 -0.00893502]\n20000 [ 1.59825249 -0.00953636]\n21000 [ 1.6554091  -0.01011949]\n22000 [ 1.71096901 -0.01068517]\n23000 [ 1.76499227 -0.01123412]\n24000 [ 1.81753625 -0.01176702]\n25000 [ 1.86865578 -0.01228453]\n26000 [ 1.91840319 -0.01278727]\n27000 [ 1.96682851 -0.01327582]\n28000 [ 2.0139795  -0.01375074]\n29000 [ 2.05990178 -0.01421256]\n30000 [ 2.10463896 -0.01466179]\n31000 [ 2.1482327  -0.01509889]\n32000 [ 2.19072281 -0.01552434]\n33000 [ 2.23214734 -0.01593856]\n34000 [ 2.27254269 -0.01634195]\n35000 [ 2.31194367 -0.01673493]\n36000 [ 2.35038357 -0.01711785]\n37000 [ 2.38789426 -0.01749107]\n38000 [ 2.42450625 -0.01785494]\n39000 [ 2.46024874 -0.01820978]\n40000 [ 2.49514972 -0.0185559 ]\n41000 [ 2.529236   -0.01889359]\n42000 [ 2.56253328 -0.01922314]\n43000 [ 2.59506621 -0.01954481]\n44000 [ 2.62685842 -0.01985886]\n45000 [ 2.65793261 -0.02016555]\n46000 [ 2.68831054 -0.0204651 ]\n47000 [ 2.71801313 -0.02075775]\n48000 [ 2.74706046 -0.0210437 ]\n49000 [ 2.77547184 -0.02132317]\n50000 [ 2.80326583 -0.02159635]\n51000 [ 2.83046026 -0.02186344]\n52000 [ 2.85707231 -0.02212462]\n53000 [ 2.8831185  -0.02238007]\n54000 [ 2.90861473 -0.02262995]\n55000 [ 2.93357631 -0.02287442]\n56000 [ 2.958018   -0.02311365]\n57000 [ 2.98195402 -0.02334779]\n58000 [ 3.00539807 -0.02357697]\n59000 [ 3.02836338 -0.02380133]\n60000 [ 3.0508627  -0.02402102]\n61000 [ 3.07290832 -0.02423615]\n62000 [ 3.09451213 -0.02444686]\n63000 [ 3.11568559 -0.02465325]\n64000 [ 3.13643978 -0.02485546]\n65000 [ 3.15678542 -0.02505359]\n66000 [ 3.17673283 -0.02524774]\n67000 [ 3.19629203 -0.02543802]\n68000 [ 3.21547268 -0.02562453]\n69000 [ 3.23428415 -0.02580736]\n70000 [ 3.25273548 -0.02598662]\n71000 [ 3.27083545 -0.02616238]\n72000 [ 3.28859254 -0.02633475]\n73000 [ 3.30601496 -0.02650379]\n74000 [ 3.32311068 -0.0266696 ]\n75000 [ 3.33988742 -0.02683225]\n76000 [ 3.35635265 -0.02699182]\n77000 [ 3.37251363 -0.02714838]\n78000 [ 3.38837738 -0.027302  ]\n79000 [ 3.40395072 -0.02745276]\n80000 [ 3.41924028 -0.02760072]\n81000 [ 3.43425248 -0.02774595]\n82000 [ 3.44899355 -0.0278885 ]\n83000 [ 3.46346954 -0.02802845]\n84000 [ 3.47768635 -0.02816584]\n85000 [ 3.49164967 -0.02830075]\n86000 [ 3.50536507 -0.02843322]\n87000 [ 3.51883793 -0.0285633 ]\n88000 [ 3.5320735  -0.02869106]\n89000 [ 3.54507688 -0.02881655]\n90000 [ 3.55785303 -0.0289398 ]\n91000 [ 3.57040677 -0.02906088]\n92000 [ 3.5827428  -0.02917982]\n93000 [ 3.59486567 -0.02929668]\n94000 [ 3.60677985 -0.0294115 ]\n95000 [ 3.61848965 -0.02952432]\n96000 [ 3.62999929 -0.02963518]\n97000 [ 3.64131287 -0.02974413]\n98000 [ 3.6524344  -0.02985121]\n99000 [ 3.66336776 -0.02995644]\n[ 3.6741061  -0.03005978]\n"},"1":{"data":{"text/plain":"array([ 3.6741061 , -0.03005978])"},"exec_count":8,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"c1521c","input":"X = df.drop(columns = [\"health status\"])\ny = df[\"health status\"]\n\nmodel = LogisticRegression(solver=\"lbfgs\")\nmodel.fit(X, y)\nprint(model.intercept_)\nprint(model.coef_)","output":{"0":{"name":"stdout","output_type":"stream","text":"[4.3727885]\n[[-0.03673668]]\n"}},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"03f8df","input":"### An improvement\nNote: Our algorithm is slow in part because it does not utilizie the built-in numpy data structures that speed things up. Note how much faster this algorithm runs by utilizing numpy arrays and the dot product of vectors:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"08ab35","input":"We can view the $\\theta_0$ and $\\theta_1$ by typing:","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"0af8bc","input":"### A big improvement. \n\nNote that the the code above may have taken much quicker than the one before it but it's still MUCH slower and less accurate than the built-in sci-kit learn method:","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"2ff41a","input":"## Supervised Learning: Logistic Regression\n\n---\n<a class=\"anchor\" id=\"log\"></a>\n\nSources: \n\nhttps://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n\n\nhttps://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n\nLogistic Regression is used when the dependent variable (target) is categorical. For example,\n\n-to predict whether an email is spam (1) or (0)\n\n-Whether the tumor is malignant (1) or not (0)\n\nThere are several types of logistic regressions:\n\n1.Binary Logistic Regression\n\nThe categorical response has only two 2 possible outcomes. Example: Spam or Not\n\n2.Multinomial Logistic Regression\n\nThree or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n\n3.Ordinal Logistic Regression\n\nThree or more categories with ordering. Example: Movie rating from 1 to 5\n\nWe'll stick with a basic Binary Logistic Regression example for now. Suppose that you would like to predict from the number of positive cancerous lymph nodes, whether a patient will survive or not five years from now:\n\n<img src=\"images/log1.png\" width=\"400\">\n\n\nOr maybe taking into account two explanatory variables, age and nodes, whether a patient will survive or not:\n\n<img src=\"images/log2.png\" width=\"300\">\n\n\nWe could use a linear regression, and label everyone above the line y = 1/2 as deceased and everyone below the line as survived. However, we would misclassify people:\n\n<img src=\"images/log3.png\" width=\"400\">\n\nA better solution would be to find a \"curvier\" function that has a steeper ascent and a range of only (0,1). The Sigmoid function, also known as the logistic function, will do:\n\n\n<img src=\"images/log4.png\" width=\"400\">\n\nWhen using linear regression, recall that our hypothesis function was $h(\\theta_0,\\theta_1) = \\theta_0+\\theta_1x$. \n\nUsing logistic regression, our function will be: $h(\\theta) = \\frac{1}{1+e^{-(\\theta_0+\\theta_1x)}}$.\n\nIf this function returns a value greater than 0.5 based on an input x (the number of positive nodes), then we will label the patient as deceased. Otherwise, we'll label them as survived.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"4e4ee7","input":"Run the following cell to import the required packages:","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"84fcd5","input":"### Homework 1 - Calculus Derivation\n\nUsing multivariable calculus and properties of logarithms, show that linear regression and logistic regression have nearly the same gradient descent formulas. If you need help, consult this source:\n\n\nhttps://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression?newreg=28a1a02102d9489caad408b4335adfc2\n\n\n### Homework 2 - Logistic Regression Gradient Descent\n\nEdit your previous gradient descent algorithm to now find the optimal parameters for a logistic regression with one explanatory variable.\n\nNote that the two main things that you will have to edit are a.) the $h(\\theta)$ function and b.) take the division by m (the number of points) out of your $\\theta_j$ update.\n\nTo check your work, if you type ```gradient_descent(X,y, alpha= 5e-5, num_steps = 100000)``` with an initial guess of $\\theta_0 = 0$ and $\\theta_1 = 0$, then it should take a long time to run but it will end up giving you out approximately $\\theta_0 = 3.67, \\theta_1 = -0.03$, which isn't quite there yet but relatively close to the sci-kit learn values above.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"9b03dc","input":"By the way - you will see the logistic regression equation written in several equivalent forms:\n\n\n$\\log(\\frac{h(\\theta)}{1-h(\\theta)}) = \\theta_0+\\theta_1x$\n\n\nOR:\n\n\n$h(\\theta) = \\frac{1}{1+e^{-(\\theta_0+\\theta_1x)}}$\n\nOR:\n\n$h(\\theta) = \\frac{e^{\\theta_0+\\theta_1x}}{1+e^{\\theta_0+\\theta_1x}}$\n\nNote: if you need more explanation on Logistic Regression, watch these two Andrew Ng videos:\n\nhttps://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function\n\n\nhttps://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent\n","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"9b79ae","input":"We learned about the cost function J(θ) in linear regression. The cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error:\n$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n(h_{\\theta}(x_i)-y^{(i)})^2$\n\nIf we try to use the cost function of the linear regression in Logistic Regression then it would be of no use as it would end up being a non-convex function with many local minimums, in which it would be very difficult to minimize the cost value and find the global minimum.\n\n<img src=\"images/log5.png\" width=\"400\">\n\nInstead, the cost function for Logistic regression will be:\n\n<img src=\"images/log6.png\" width=\"400\">\n\n<img src=\"images/log7.png\" width=\"600\">\n\nThe above two functions can be compressed into a single function:\n\n<img src=\"images/log8.png\" width=\"600\">\n\nNow the question arises, how do we reduce the cost value? Well, this can be done by using Gradient Descent. The main goal of Gradient Descent is to minimize the cost value, i.e., minimize J(θ).\n\nWhen we take the partial derivative of the cost function and plug it into our gradient descent formula, once again we get the familiar algorithm:\n\n<img src=\"images/log9.png\" width=\"600\">\n\nYou’ll note the summation term is the exact same form as the one you get when deriving gradient descent for linear regression. It is different however because in this case the hypothesis is a logistic function, not a linear one. (And as a tiny extra reason, because we no longer see the division by m out front.)\n\n<img src=\"images/log10.png\" width=\"600\">\n\nHow do we get to the same form as the linear regression case??? This is NOT obvious! You will do the derivation for homework.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"b776ae","input":"We'll do the simplest example possible right now, a binary classifier for dogs with just one predictor variable (weight). Let's read the data in, where 1 corresponds to healthy and 0 corresponds to unhealthy:","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"e43725","input":"Let's create our logistic regression model and calculate its score (which returns the mean accuracy on the given test data and labels). We see that out of 20 dogs, it classifies 85% correctly, meaning only 3 dogs got classified incorrectly:","pos":7,"type":"cell"}
{"id":0,"time":1617772387401,"type":"user"}
{"last_load":1617772386511,"type":"file"}